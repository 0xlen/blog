<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://easoncao.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://easoncao.com/" rel="alternate" type="text/html" /><updated>2023-05-19T08:06:11-05:00</updated><id>https://easoncao.com/feed.xml</id><title type="html">Continuous Improvement</title><subtitle>Sharing technical stuffs, AWS, Cloud computing, container, workout, travling and more.</subtitle><author><name>Yang-Xin Cao (Eason Cao)</name></author><entry><title type="html">身處 Amazon 工作 5 年學到的事：在 AWS 擔任 Cloud Support Engineer 是什麼樣的體驗</title><link href="https://easoncao.com/five-years-at-amazon-as-cloud-support-engineer-in-aws/" rel="alternate" type="text/html" title="身處 Amazon 工作 5 年學到的事：在 AWS 擔任 Cloud Support Engineer 是什麼樣的體驗" /><published>2023-04-01T00:00:00-05:00</published><updated>2023-04-01T00:00:00-05:00</updated><id>https://easoncao.com/five-years-at-amazon-as-cloud-support-engineer-in-aws</id><content type="html" xml:base="https://easoncao.com/five-years-at-amazon-as-cloud-support-engineer-in-aws/"><![CDATA[<p>2023 是我在 AWS 擔任 Cloud Support Engineer 的第六年，這份工作對我來說某種程度上是又愛又恨。作為見證 AWS 中文技術支援團隊的一員，五年時間說長不長，說短也不短，卻有非常多學習和體悟。</p>

<figure class="">
  <img src="/assets/images/posts/2023/04/five-years-at-amazon-as-cloud-support-engineer-in-aws/cover.jpg" alt="身處 Amazon 工作 5 年學到的事：在 AWS 擔任 Cloud Support Engineer 是什麼樣的體驗" />
  
    <figcaption>
      身處 Amazon 工作 5 年學到的事：在 AWS 擔任 Cloud Support Engineer 是什麼樣的體驗 (source: <a href="https://unsplash.com/photos/LJ9KY8pIH3E">Unsplash</a>)

    </figcaption>
  
</figure>

<p>一轉眼五年過去，除了見證團隊的成長，也看過各種人來來去去。在 Amazon 待超過五年的員工不正確估算在全球大概佔 PR 90，在同一個團隊待超過五年的人更是稀缺。一滿五年，你就能夠晉身橘色識別證的一員<sup id="fnref:amazon-badge-color" role="doc-noteref"><a href="#fn:amazon-badge-color" class="footnote" rel="footnote">1</a></sup>。作為團隊少數幾個拿橘牌的員工，不免有一些勵志或是辛苦的工作歷程可以分享，可惜忙碌跟快速的工作步調讓很多想法稍縱即逝。尤其是回顧自己過去寫下的內容 (<a href="/ten-thing-I-learned-in-amazon">我在 Amazon 學到的 10 件事</a>)，驚覺這些改變其實無時無刻都在發生，我確實明白自己有些模樣在這五年前已經有所改變，慶幸的是我沒有因為環境而變得事故。</p>

<p>我想或多或少是因為能夠透過回顧這些內容，甚至是因為協助面試的緣故，很多來面試的候選人甚至主動提及曾經讀過我的內容並且因此受到一些啟發 (不論是否最終有進入我們團隊工作)。這些文字跟反饋都讓我時刻刻提醒自己要永保初心，也要求自己用更成熟的方法處理事情。有些文章甚至獲得很多各方高手的迴響，但一直遲遲沒有安排時間再繼續寫更多。因此寫下這篇內容，也算是我個人對自己在用五年後的視角，來總結我對這個團隊喜歡和認為有待加強的地方。</p>

<p>在這裡先提個免責申明，AWS 雖然作為 Amazon 整個集團重要的一個業務，但有許多管理的方式可能也因不同組織和角色而有所差異。畢竟 Amazon 在全球有數十萬名員工分佈在世界各地，工作時遇到一堆奇奇怪怪的部門早已屢見不鮮，用 AWS Support 不能以偏概全所有團隊的樣貌。</p>

<p>此外，這篇內容僅為個人的觀察，不代表任何官方立場，本篇內容更不期待帶有任何批判色彩。在團隊裡身為一個獨立貢獻者 (Individual Contributor)，大部分時間我也是站在第一線解決各式各樣的工程問題，而非討論管理維運議題。因為不是站在管理層的角度用宏觀的面向關注團隊維運，工程師關注的議題跟管理者的議題多少都有些不同，我僅分享我對於身為工程師及親身經歷的相關感受，不意味著任何一方不好。</p>

<p>畢竟我的角度為「工程師」而非「管理者」，因此可能部分觀察的角度也會有所侷限，但會盡可能用客觀的角度寫下我個人的觀察，還請閱讀者自行評判。</p>

<h2 id="認識不同年齡層工作者的機會間接拓展更廣的人際網路">認識不同年齡層工作者的機會間接拓展更廣的人際網路</h2>

<p>我在中文技術團隊觀察一直沒變的事情是，團隊中充滿了各式各樣優秀的同事，許多不乏都是在業界工作數年的工程師並且擁有豐富的經歷。有趣的是，時下的招聘政策也鼓勵許多剛畢業的學生加入 AWS Support 技術團隊，不時為團隊注入心血，使得整個團隊的組成十分多元。常常也有許多不同的新進同事從其他公司帶來不一樣的經驗和想法，甚至從原本使用 AWS 產品的客戶端變成解決問題的角色，都促使團隊中因為不同背景的工作者加入彼此交流而建立強大的人脈網路。</p>

<h2 id="與全世界交流">與全世界交流</h2>

<p>Cloud Support Engineer 是一個全球化的團隊，工程師遍佈世界各地。當你身肩的責任和任務越多，自然越有更多機會與全世界的同事交流和執行專案。我個人曾經主持過跨三個時區的技術講座和訓練，不僅讓我實際應用專案管理的思維、訓練教學技能，更讓我進一步增加自己的能見度，並且實質提升團隊的技術職能，為團隊作出貢獻。</p>

<p>能進一步打開自己的視野並與全世界交流，是我個人最喜歡的部分。尤其身處 Dublin，多得是不同語言和背景的工程師，充滿了不同國家、語言、背景交流的機會，除了專注在技術上的交流，工作之餘也可以聊聊不同背景之間的特色和學習彼此的文化。</p>

<p>但如果人為在台灣的技術團隊，實際上可能因為時區的緣故，部分交流多少會有一些受限。但與世界各地的同事交流仍只是使用通訊軟體這種觸手可及的事情，壞處就是你可能要額外安排其他時間 (比如你必須要早起、晚上回訊息或參加會議)。</p>

<h2 id="增強抗壓能力">增強抗壓能力</h2>

<p>我認為這份工作與一般軟體開發最大不同的是，由於面對的是客戶的生產環境，Cloud Support Engineer 不時會有面對高壓的情境，例如功能故障、應用程式不工作的事件、系統崩潰等等。有時候客戶甚至會不斷的催促你盡快給予資訊，在這種情況下，你需要學會保持冷靜和集中注意力，同時，不能因客戶的情緒引領你往不正確的調查方向。</p>

<p>我還記得第一次面對客戶環境故障時，我還得請求資深工程師一同指導解決客戶所遭遇的技術問題。但在從中觀摩學習後，並且檢討學習排查問題的方法不斷練習，慢慢將這種壓力視為對客戶的理解，學會用冷靜的態度處理棘手的狀況，正確的釐清和排除。可能有些人認為是缺點 (例如：客戶很難搞)，但實際在走過一遭後，一旦你跨過那個不舒服的過程，將成為無懈可擊且帶得走的軟實力，就看你是否願意視這些挑戰為成長的機會。</p>

<p>這樣的經驗和訓練不自覺在生活中產生一些幫助，尤其是面對系統故障或是一些非預期的狀況發生時，第一直覺反應是釐清問題並且如何找出解決問題的方式，而不是受情緒或環境影響不斷焦慮。</p>

<h2 id="磨練跨團隊的溝通技巧">磨練跨團隊的溝通技巧</h2>

<p>Cloud Support Engineer 與軟體開發不同的是，這是一個大量需要溝通的工作角色，並且將技術問題用淺顯易懂的方式與客戶分享相關的調查結果、提及可執行且理解的步驟供客戶採納 (不論是書信、Chat 或是電話形式)。由於面向的客戶端存在各式各樣的角色，你除了要學會用開發者能理解的方式解釋問題和解決方案，不免遇到客戶端主管級別的角色想釐清問題的相應狀況；同時，AWS 也會有不同面向的客戶端角色一同協助客戶的問題，身為技術工程師也讓我多了很多必須要學會與這些不同角色溝通的技能。</p>

<p>某種程度上學會站在他人角度、同理他人。在產品開發團隊前面，我會需要了解客戶遭遇的問題是什麼、如何複製、點出產品當前的問題、建議如何修正。除了要讓產品團隊理解當前問題要排查的方向，更需要對產品整體的核心運作有一定的認識，才能使用開發團隊所能理解的語言有效的將問題修正；在客戶前面，我需要了解客戶所遭遇的問題、客戶的痛點，並且提出可參考的實務建議，以幫助他們在業務上透過產品的功能和方法，滿足他們業務上所期待的目標，甚至有時候需要引導客戶改正問題以爲他們帶來長遠的效益 (因為有時候客戶有自己的想法並且多急於解決當下的短期問題)。</p>

<h2 id="提升書信寫作能力">提升書信寫作能力</h2>

<p>除了 Amazon 組織文化本身就具備寫文件的精神外，技術支持工程師會有大量的時間會投入在將複雜的問題調查報告轉譯成客戶或是產品團隊能理解的語言，並且寫出能夠讓客戶理解的書信內容。</p>

<figure class="">
  <img src="/assets/images/posts/2023/04/five-years-at-amazon-as-cloud-support-engineer-in-aws/support-case-example.png" alt="一個 AWS Support Center 的通訊範例 (非技術相關)" />
  
    <figcaption>
      一個 AWS Support Center 的通訊範例 (非技術相關)

    </figcaption>
  
</figure>

<h2 id="各種曝光和成長的機會">各種曝光和成長的機會</h2>

<p>AWS Support 除了是一個跨國組成的團隊外，AWS 本身就提供了一個能夠讓你成長和曝光的平台。</p>

<p>身處中文 DevOps/Container 領域的技術團隊，我特別喜歡的一點是週遭的同事都非常支持且互相幫忙，並且在自己的職涯規劃上都很積極，不會只侷限在日常協助客戶解決單一 Support Case 的問題上。</p>

<p>即使每天日常解決多少個 Support Case (Ticket) 很重要，但更多得是其他面向的工作幫助你成長不同面向的技能。由於 AWS Support 密切的與不同產業的客戶合作，一個顯著的例子是透過客戶端面向的教育訓練機會幫助你成長，為不同規模的企業客戶分享有關 AWS 產品的使用建議和最佳實踐。</p>

<p>此外，為了協助更多客戶解決技術問題，內部不時充斥各種專案和計畫。不論是藉由影片、技術文章或是教育訓練，團隊成員們都會透過不同的方式提升自己的技能。除了使中文的客戶受益，更多時候貢獻己力讓全球的客戶受益，並且在全球打開能見度。例如，以下都是我或是同事們貢獻的各種內容：</p>

<p><strong>AWS Knowledge Center</strong></p>
<ul>
  <li><a href="https://repost.aws/knowledge-center/ecs-unable-to-assume-role">How do I troubleshoot the error “ECS was unable to assume the role” when running the Amazon ECS tasks?</a> - Joyce Kuo</li>
  <li><a href="https://repost.aws/knowledge-center/ecs-task-container-health-check-failures">How do I troubleshoot the container health check failures for Amazon ECS tasks?</a> - Shih-ting Yuan</li>
</ul>

<!-- Courtesy of embedresponsively.com //-->

<div class="responsive-video-container">
    <iframe src="https://www.youtube-nocookie.com/embed/BxEFqTv-icU" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen=""></iframe>
  </div>

<!-- Courtesy of embedresponsively.com //-->

<div class="responsive-video-container">
    <iframe src="https://www.youtube-nocookie.com/embed/JANKxp728dg" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen=""></iframe>
  </div>

<!-- Courtesy of embedresponsively.com //-->

<div class="responsive-video-container">
    <iframe src="https://www.youtube-nocookie.com/embed/sHup_xwwZx0" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen=""></iframe>
  </div>

<!-- Courtesy of embedresponsively.com //-->

<div class="responsive-video-container">
    <iframe src="https://www.youtube-nocookie.com/embed/X3wPRrVeSqo" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen=""></iframe>
  </div>

<p>更甚者，我厲害的同事們多是路見不平拿 Pull Request 來填，甚至會提交對應的修補程式，或開發對應的工具讓更多客戶從中受益，例如：</p>

<ul>
  <li><a href="https://github.com/aws/aws-cdk/pull/21185">(aws-cdk) fix(eks): cannot disable cluster logging once it has been enabled #21185</a> - Kuo-Le Mei</li>
  <li><a href="https://github.com/awsdocs/aws-cloudformation-user-guide/pull/1225">AWS::RDS::DBInstance replacement update notice #1225</a> - Yu-Chi Chen</li>
  <li><a href="https://github.com/guessi/cloudtrail-cli">cloudtrail-cli</a> - Kuo-Le Mei</li>
</ul>

<h2 id="彈性的職涯規劃路徑">彈性的職涯規劃路徑</h2>

<p>作為開啟 AWS 職涯的敲門磚，Cloud Support Engineer 著實是一個充滿學習廣度機會的工作，這也是我個人覺得這份工作十分有趣的地方，例如：</p>

<ul>
  <li>處理客戶案例所累積對於 AWS 服務的技術知識和高可用架構設計思維、在客戶端大量的提供教育訓練或最佳實踐建議，轉職成為解決方案架構師 (Solution Architect)</li>
  <li>針對特定客戶所遭遇的問題提供協助而逐步轉職成為技術經理 (Technical Account Manager)</li>
  <li>協助客戶執行教育訓練或是訓練內部工程師，培養成為講者的能力 (Trainer)</li>
  <li>設計內部工具和相關專案計畫累積系統設計相關的開發技能 (Software Development Engineer)</li>
  <li>大量的書信寫作訓練和開源文件貢獻機會，轉職成為技術寫手 (Technical Writer)</li>
  <li>整合內部資源執行跨區域或是全球的計畫，逐步成為主管 (Operation Manager) 或是資深工程師</li>
</ul>

<h2 id="還算可接受的-work-life-balance">還算可接受的 Work-life balance</h2>

<p>雖然每個人對於 Work-life balance 的定義不同 (就我的觀點，這是一種相對感受)，但比起很多公司的 IT Support 或是工程師職位，「相對來說」，AWS Cloud Support Engineer 可能不會是一個非常輕鬆的工作。</p>

<p>以目前團隊的工作型態，為了提供客戶 24 x 7 x 365 天不間斷的支持，除了意味著國定假日或是週末會是你的工作時間外，團隊成員彼此之間通常工作時間會有部分不重疊的輪班制度。</p>

<p>但之所以我認為這還算可接受，主要有以下幾點觀察：</p>

<ul>
  <li>
    <p>(1) 在過去，在台灣的中文團隊值班時間需覆蓋整整 16 小時，直到晚上 11 點左右才轉移至北美時區 (這意味著當時有部分同事需要工作到晚上 11 點)。由於值晚班這件事情對很多人來說並不是一個很健康的工作型態，管理層也在台灣團隊成立不久後，不斷地尋求可能的解決方案。隨著歐洲團隊的建立，這樣的現象也趨於改善，使得台灣能從值晚班的噩夢中解放，將工作時間往前推移 (能下班的時間越來越早)。</p>
  </li>
  <li>
    <p>(2) 即使 Cloud Support Engineer 同樣會有 Oncall 的機制，團隊的 Oncall 會以工作時間為主。由於是全球化的團隊，工作時間結束後的 Oncall 班次將會由其他時區輪值。</p>
  </li>
</ul>

<p>大部分情況下，新進人員在 Work-life balance 這件事情上面通常能有很好的控制。但隨著想做的事情越多，可能在你身上肩負的責任也越多，使得工作與生活上不見得能夠充分平衡。例如：我觀察到資深的工程師，有時候也得必須配合美洲時區在晚上時間 (21:00) 之後開會。<strong>但「相對來說」，比起傳聞有些 Amazon 的開發團隊需要半夜 Oncall 起床處理問題，確實種程度上是還可接受的 Work-life balance。</strong></p>

<p>雖然難以置信，但在提供客戶 24 小時不間斷服務的背後，仍有 AWS 開發團隊需要負擔全天候的 Oncall 工作，不得不得在半夜時間起床。尤其你身為 AWS 技術支持工程師並具備一定資歷後，多少會接觸到客戶生產環境故障的問題，多的是把北美時區的開發者叫起床的機會。</p>

<p>作為曾經在半夜被叫醒的工作者，我個人十分認同半夜起床值 Oncall 是一個很不健康的工作型態。雖然團隊對於工作型態的設計不是我很喜歡的一點，但在方面團隊整體確實有在以緩慢的節奏進步。隨著加入的人才越多，整個團隊的工作安排會趨於理想值。</p>

<h2 id="客戶無法區分清楚真正的問題緊急程度客戶預期和實際環境的衝突導致工作時間碎片化">客戶無法區分清楚真正的問題緊急程度：客戶預期和實際環境的衝突導致工作時間碎片化</h2>

<p>日常工作時間的碎片化是我個人很不喜歡的一點。要討論這點之前，需要先理解整體環境的影響佔這個問題的關鍵性因素。取決於市場型態不同，客戶的行為也會有所不同；對於中文的客戶，由於客戶習慣即時通訊方法和立刻響應速度，這往往造就了客戶使用技術支持服務時，產生很多非預期的行為。</p>

<p>即使在文件和產品頁面中明確定義了嚴重性的不同 <sup id="fnref:case-sev-product-page" role="doc-noteref"><a href="#fn:case-sev-product-page" class="footnote" rel="footnote">2</a></sup> <sup id="fnref:case-sev-doc" role="doc-noteref"><a href="#fn:case-sev-doc" class="footnote" rel="footnote">3</a></sup>，客戶習慣仍傾向選擇最少的時間來開啟案例 (能選多短就選多短)，而非真正的問題嚴重性影響 (例如：客戶有一個專案趕著下週上線所以選擇最短的響應時間 15 分鐘，而非系統正在當機)：</p>

<figure class="">
  <img src="/assets/images/posts/2023/04/five-years-at-amazon-as-cloud-support-engineer-in-aws/case-sev-product-page.png" alt="在產品頁面明確定義的案例嚴重性" />
  
    <figcaption>
      在產品頁面明確定義的案例嚴重性

    </figcaption>
  
</figure>

<figure class="">
  <img src="/assets/images/posts/2023/04/five-years-at-amazon-as-cloud-support-engineer-in-aws/case-sev-doc.png" alt="在文件頁面明確定義的案例嚴重性" />
  
    <figcaption>
      在文件頁面明確定義的案例嚴重性

    </figcaption>
  
</figure>

<p>即使明白客戶常常不正確的選擇案例嚴重性，AWS Support 仍提供客戶最大的決定權。然而，這樣的現象某種程度上確實也導致工程資源被濫用。這就好比家喻戶曉的伊索寓言「狼來了」中描述的故事，當假警報一多，除了使得團隊無法正確區分真正受到生產環境影響的故障，更嚴重的是由於工程師都被一堆非故障影響的問題佔用，使得團隊工程師無法很好平衡不同問題之間的嚴重性，即時協助真正有環境受損影響的故障。</p>

<p>這個現象所帶來的影響更使得中文技術團隊必須大量的應付客戶這種短而快的回覆，而往往喪失能夠專注在技術問題上的時間。我看過許多新進人員因此無所適從，迫使被拉去處理大量需要短時間回應的案例，而無法真正的在單一案例中投入太多充分的時間進行調查：一下忙 A 案例，一下被抽去做 B 案例，或是手邊正在忙碌的事情、正在開的會不得不中斷去協助客戶，導致工作時間的碎片化。如果間接犧牲的是客戶長遠的服務品質，我相信整個市場型態有很多有待改進的空間。</p>

<p>相對來說，日文客戶就慣用「一般指導」、「系統效能不佳/系統受損」等這類真正反應其事實狀況的案例嚴重性，進行問題指導和事件後的相關問題調查。這樣的市場型態讓我從日文團隊同事中間接受益，從他們每個人身上提供給客戶的回覆我學習到非常多。我常常可以觀察到他們提供給客戶的訊息都十分詳細且完整，在回覆前不僅做了非常多詳細的測試，更提出各種可參考的方案或是 PoC，甚至在內部已經討論過一圈 (但缺點是客戶需要有足夠的耐心)。</p>

<p>當然這並不是要比較哪種客戶比較好，畢竟，從親身體驗過印度客戶的型態，也覺得支持上充滿挑戰，就明白這並不是單一語系客戶的問題，每種客戶都有各自的特性。而是從實際經驗中，讓我對文化和市場差異有更深的體悟。</p>

<p>同時，團隊也在針對這項問題做了很多不同面向的嘗試，以試圖優化工程師在工作上的痛點，讓客戶學習如何更好且正確的使用 AWS 技術資源，以幫助他們解決真正重要的問題。</p>

<h2 id="快速迭代的流程有時讓人無所適從">快速迭代的流程有時讓人無所適從</h2>

<p>由於團隊快速成長且存在多時區的問題，為了適應不同問題的情境，流程的修訂和快速迭代有時往往讓人跟不上。例如：有時候今天建議的工作流程 A，可能明天會變成 A-1 版本，再過幾週可能變成 A-10。由於流程常因客戶需求和問題不斷迭代，有時候不見得能夠即時的在各區域中套用，或是特定區域根本是獨立的系統，有自己的一套系統運作。</p>

<p>除了「朝令夕改」大概是最適合用來形容團隊一些流程上的現況，很多時候團隊會不斷嘗試導入一些新的方法或是流程。我自己有時候都會覺得無所適從。必須要不時回去翻翻內部的文件，或是回到一些原則性的討論，以了解是否有任何理解錯誤。</p>

<p>雖然有時候感到混亂，但這種迭代的過程在團隊中會一直不斷的進行，你得學會適應快節奏和不斷變化的文化。</p>

<h2 id="學會如何成為一名更好的客戶">學會如何成為一名更好的客戶</h2>

<p>最後，<strong>「學會如何成為一名更好的客戶」</strong>絕對是我任職客戶面向技術支持工程師角色這幾年收穫最大的體悟，絕對可以列為最重要的一點。由於日常工作中實際就是在做類似客戶服務性質的工作，工作中不免看盡客戶百態，了解不同客戶的型態，並且了解到什麼是好的、什麼是不好的。</p>

<p>身為一名技術支持工程師，隨著協助客戶經驗的累積，漸漸地學會如何站在客戶服務人員的角度思考，並且同理身為客戶服務角色的辛苦。</p>

<p>當你了解到客戶並未尊重專業時，那份挫折感絕對是深深擊潰你對於技術的自信。甚至很多時候即使你的建議正確，並且成功解決客戶問題後，客戶可能就只把你當做一名 AWS 的後台人員，也不見得會獲得客戶的肯定。</p>

<p>在開始工作的頭幾年我很不能理解，往往覺得在工作上付出了努力但仍得不到任何客戶的反饋；但轉頭一看，其實還是有非常多的客戶展現充分的專業並且在工作上有很好的合作體驗，更加學會心平氣和的理解不同性質的客戶。<strong>也因此學習到如何成為一名「好客戶」，更懂得如何在日常的生活中，向不同產業、工作的客戶服務人員展示尊重</strong>，可以是餐廳服務生、是銀行電話客戶服務人員，或是任何一種客戶面向的工作者。在我所處的團隊中，團隊成員也都不吝分享自己的經驗，並且分享如何針對不同客戶提供適當的處理方式，藉由這些經驗成長。</p>

<p>自從成為客戶服務角色後，能更同理不同行業類別的客服工作，更加重視第一線的服務人員背後所付出的辛勞。並且在未獲得預期的服務水平時，能提出實質的建議、想法而不是純抱怨。有趣的是，這樣做往往讓我獲得更為滿意的結果 (不是更快的退費效率或是獲得更多的補償)，一同促使產品和服務進步。</p>

<h2 id="總結">總結</h2>

<p>在這篇內容中，深入探討了在 Amazon Web Services (AWS) 擔任 Cloud Support Engineer 的工作體驗，並分享了個人在過去五年中學到的寶貴技能和體驗。以上幾點完全屬於個人的觀察，不代表任何官方立場。</p>

<p>如果你正對 AWS Cloud Support Engineer 職位感興趣，希望以上的內容對您有所幫助，也可以透過參考其他系列文章以幫助你了解更多資訊。</p>

<h2 id="看更多系列文章">看更多系列文章</h2>

<ul>
  <li><a href="/ten-thing-I-learned-in-amazon">我在 Amazon 學到的 10 件事</a></li>
  <li><a href="/how-am-I-get-into-amazon-before-graduate">我是如何在還沒畢業就錄取並進入到 Amazon 工作</a></li>
  <li><a href="/how-i-pass-aws-all-five-certificate-within-one-year">我是如何在一年內通過 AWS 五大核心認證</a></li>
  <li><a href="/what-is-cloud-support-engineer-doing-in-amazon">Amazon Cloud Support Engineer 到底是在做什麼 (Amazon Web Services / AWS)</a></li>
  <li><a href="/aws-cloud-support-engineer-faq">關於 Cloud Support Engineer 職位的常見問題 (Amazon Web Services / AWS) - AWS Cloud Support Engineer FAQs</a></li>
  <li><a href="/aws-cloud-support-engineer-interview-tips-and-required-skills">在 AWS Cloud Support Engineer 面試中脫穎而出：必備的技術能力和重要特質</a></li>
  <li><a href="/five-years-at-amazon-as-cloud-support-engineer-in-aws">身處 Amazon 工作 5 年學到的事：在 AWS 擔任 Cloud Support Engineer 是什麼樣的體驗</a></li>
</ul>

<h2 id="references">References</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:amazon-badge-color" role="doc-endnote">
      <p><a href="https://www.aboutamazon.eu/news/working-at-amazon/discover-whats-behind-the-amazon-id-badges">Discover what’s behind the Amazon ID badges</a> <a href="#fnref:amazon-badge-color" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:case-sev-product-page" role="doc-endnote">
      <p><a href="https://aws.amazon.com/tw/premiumsupport/plans">比較 AWS Support 計畫</a> <a href="#fnref:case-sev-product-page" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:case-sev-doc" role="doc-endnote">
      <p><a href="https://docs.aws.amazon.com/zh_tw/awssupport/latest/user/case-management.html#choosing-severity">建立支援案例和案例管理 - 選擇嚴重性</a> <a href="#fnref:case-sev-doc" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Yang-Xin Cao (Eason Cao)</name></author><category term="amazon" /><category term="aws" /><category term="amazon web services" /><category term="work" /><category term="Cloud Support" /><category term="Cloud Support Engineer" /><summary type="html"><![CDATA[如果您對 AWS 技術支援和客戶服務感興趣，且想了解更多有關 AWS Cloud Support Engineer 的訊息。這篇文章深入探討了在 Amazon Web Services (AWS) 擔任 Cloud Support Engineer 的工作體驗，並分享了作者在過去五年中學到的寶貴技能和豐富經驗。透過這篇文章，您可以了解到 AWS Cloud Support Engineer 的日常工作內容、挑戰和回報，並探究一些工作秘密。]]></summary></entry><entry><title type="html">在 AWS Cloud Support Engineer 面試中脫穎而出：必備的技術能力和重要特質</title><link href="https://easoncao.com/aws-cloud-support-engineer-interview-tips-and-required-skills/" rel="alternate" type="text/html" title="在 AWS Cloud Support Engineer 面試中脫穎而出：必備的技術能力和重要特質" /><published>2023-03-26T00:00:00-05:00</published><updated>2023-03-26T00:00:00-05:00</updated><id>https://easoncao.com/aws-cloud-support-engineer-interview-tips-and-required-skills</id><content type="html" xml:base="https://easoncao.com/aws-cloud-support-engineer-interview-tips-and-required-skills/"><![CDATA[<p>如果你正準備面試 AWS Cloud Support Engineer 的職位，在這篇文章中，我將與你分享我對於 AWS Cloud Support Engineer 所需的必備技能和特質，幫助你更好地了解這份職位且思考自己目前所具備的能力是與該團隊所交集，讓你在面試中脫穎而出。</p>

<figure class="">
  <img src="/assets/images/posts/2023/03/aws-cloud-support-engineer-interview-tips-and-required-skills/cover.jpg" alt="在 AWS Cloud Support Engineer 面試中脫穎而出：必備的技術能力和重要特質" />
  
    <figcaption>
      在 AWS Cloud Support Engineer 面試中脫穎而出：必備的技術能力和重要特質 (source: <a href="https://unsplash.com/photos/7aakZdIl4vg">Unsplash</a>)

    </figcaption>
  
</figure>

<p>詳細內容可以參考以下我的團隊針對 AWS Cloud Support Engineer (雲端工程師) 職涯分享會中所提到的具體細節，裡面也包含了部分面試流程和一些小秘訣幫助你可以更好地掌握我們團隊所看重的能力：</p>

<!-- Courtesy of embedresponsively.com //-->

<div class="responsive-video-container">
    <iframe src="https://www.youtube-nocookie.com/embed/Q0qkKhxAo04" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen=""></iframe>
  </div>

<p><strong>請注意本篇內容純屬個人觀點</strong>，會分享這些內容的目的是希望有興趣投遞 AWS Cloud Support Engineer 能夠了解具體應該提升哪些必備技能，也同時分享一些日常為客戶解決技術問題時非常重要的能力。<strong>寫這篇內容的目的不是要幫助大家 Crash 考題，也不代表任何官方的參考指南。</strong></p>

<p>即使你面試時展現把考題背熟的能力，進來團隊終究會在實際面對客戶問題時怕得要死，因為客戶問題往往都是沒有被定義清楚但又希望你能給予解答。如果只會背誦這些內容，仍然無法實際為客戶解決任何問題。</p>

<p>另外團隊召聘所看重的核心技能也可能隨時間變化，面試也不是一般考試，重要的是了解你能為團隊貢獻什麼樣的技能，看的是不同維度的全面評估。因此以下內容屬於我個人在團隊的經驗和處理客戶案例認為非常重要的必備技能，僅供參考。</p>

<p>如果你還不是很清楚 AWS Cloud Support Engineer 在做什麼，我十分推薦你可以參考我在 AWS 職涯系列的相關文章，以幫助你逐步建立對於這個職位的認識：</p>

<ul>
  <li><a href="/ten-thing-I-learned-in-amazon">我在 Amazon 學到的 10 件事</a></li>
  <li><a href="/how-am-I-get-into-amazon-before-graduate">我是如何在還沒畢業就錄取並進入到 Amazon 工作</a></li>
  <li><a href="/how-i-pass-aws-all-five-certificate-within-one-year">我是如何在一年內通過 AWS 五大核心認證</a></li>
  <li><a href="/what-is-cloud-support-engineer-doing-in-amazon">Amazon Cloud Support Engineer 到底是在做什麼 (Amazon Web Services / AWS)</a></li>
  <li><a href="/aws-cloud-support-engineer-faq">關於 Cloud Support Engineer 職位的常見問題 (Amazon Web Services / AWS) - AWS Cloud Support Engineer FAQs</a></li>
  <li><a href="/aws-cloud-support-engineer-interview-tips-and-required-skills">在 AWS Cloud Support Engineer 面試中脫穎而出：必備的技術能力和重要特質</a></li>
  <li><a href="/five-years-at-amazon-as-cloud-support-engineer-in-aws">身處 Amazon 工作 5 年學到的事：在 AWS 擔任 Cloud Support Engineer 是什麼樣的體驗</a></li>
</ul>

<h2 id="基本技術能力-fundamental-technical-skill">基本技術能力 (Fundamental Technical Skill)</h2>

<p>很多應徵者把這份工作當成一般設定環境的 IT Helpdesk 或是只是單純的客戶服務職位，以為遵循 Runbook 就能解決大部分工作上的問題。但實際 AWS Support 做的工作與一般公司的 IT Support 會與想像中有蠻大的差異，即使工作上以 Ticket 形式與客戶互動，但角色仍偏向顧問服務性質，直接被拉進客戶會議直接一個人打十個討論問題更是你都可能會遇到的情境，我會建議在應徵這份工作前可以有個心理準備。</p>

<p>我常觀察到很多候選人即使在 IT 界從業多年，對於很多基本的知識都有很大的落差 (例如：我聽過有人說用 <code class="language-plaintext highlighter-rouge">ping</code> 可以測網站的 Port 80 看網站是不是掛了)。這種現象在只專注做開發相關工作的工程師身上尤為明顯 (嚴格上來說很多軟體工程師職位都是在「實作」，面對的很多產品規格都已經在現有的封裝函式庫或是公開解決方案的 API 上定義能夠直接套用，所以可能也沒太多機會思考這種這麼核心底層的問題)。</p>

<p>不論是開發人員或是系統維運人員，可能在高階應用和實作上能夠滿足「使用者」身份角度的需求，所以也不需要對於基礎知識有太深入的了解，使得當東西壞掉或問題情境複雜時便束手無策 (也是感謝這些人才讓我有飯吃)。</p>

<p>但 AWS Cloud Support Engineer 就像是醫生這項職業，醫生必須根據病人描述的徵狀跟現有資訊提出正確的診斷步驟，用正確的診斷工具 (例如：聽診器、X 光)，最後開出正確的藥來緩解病人的症狀；工程師在協助 Troubleshooting 問題時也需要依照自己對於客戶提出問題的背景，知道要收集什麼資訊進行分析、用什麼樣正確的工具。</p>

<p><strong>甚至有時候客戶給的資訊還是錯的，這就十分仰賴對於基礎知識至上層的全面了解，提供正確的排查方向，否則就只是在亂查一通。</strong></p>

<h3 id="網路概論-networking">網路概論 (Networking)</h3>

<p>網路概論基本包含：</p>

<ul>
  <li>基本的 TCP/IP 協議運作</li>
  <li>OSI 網路七層的基本架構 (由於 AWS Cloud Support Engineer 主要負責的產品就是 “Web Services”，所以 L4-L7 的討論不少，延伸知識例如常見的 Route、SSL、HTTP 等基本認識)</li>
  <li>IPv4 的組成 (學習 AWS VPC 時對於 Subnet 的基本認識尤為重要)</li>
  <li>DNS 的運作及除錯</li>
  <li>HTTP/SSL/TLS</li>
  <li>VPN (如果招聘團隊不著重處理網路相關的產品，則不會是討論的重心)</li>
  <li>熟悉不同協議中對應的網路除錯工具和實際排查案例</li>
</ul>

<p>DNS 算是基本中再基本到不行的必備知識，我自己個人倒是遇過很多連基本 DNS 協議都不太熟的候選人 (當然也有些從事 IT 工作的客戶也不是那麼熟)，最常見的問題就是 DNS 查詢的具體流程、DNS 協議的組成和問題除錯。</p>

<p>遇到問題除錯的場景或是系統故障就將矛頭直接指向應用程式或是服務端，壓根沒有想到實際造成問題的其實是 DNS 不正確設定或是一些非預期行為導致。</p>

<p>關於網路概論，有太多的免費資源可以參考，甚至有一些很不錯的參考資源可以具體幫助你了解。可能光以下的連結要全盤了解就讀不完了，我這裡就不一一列舉：</p>

<ul>
  <li><a href="https://www.youtube.com/watch?v=e2xLV7pCOLI">What is DNS? - Introduction to Domain Name System</a></li>
  <li><a href="https://hpbn.co/">High network performance</a></li>
</ul>

<p>請注意問題的深度仍取決招聘團隊所看重的技術能力，如果是專注網路相關 AWS 服務的團隊，則可能在網路的部分就會更加深入；但對於其他專業團隊來說，由於更多的重心在於協助特定領域的 AWS 產品，則具備基本網路問題排查能力即可滿足協助客戶的情境。</p>

<h3 id="作業系統-operating-system">作業系統 (Operating System)</h3>

<p>由於我個人不太熟 Windows，為避免誤人子弟，這邊就僅列舉我認為非常實用的 Linux 資源，以及基礎到不能再基礎的檔案系統章節 (其實把鳥哥所有章節認真讀完並且實作，可能就足以面對 60-80% 有關 Linux 維運的情境)：</p>

<p>Linux (file system/operation/administration knowledges)</p>
<ul>
  <li><a href="https://linux.vbird.org/linux_basic/centos5/0210filepermission-centos5.php">Linux 的檔案權限與目錄配置</a></li>
  <li><a href="https://linux.vbird.org/linux_basic/centos5/0220filemanager-centos5.php">Linux 檔案與目錄管理</a></li>
  <li><a href="https://linux.vbird.org/linux_basic/centos5/0230filesystem-centos5.php">Linux 磁碟與檔案系統管理</a></li>
</ul>

<p>如果你完全並未具備這方面的經驗，搭建一個 Web Service (HTTP) 涵蓋網路概論至作業系統基本維運操作過程中所必備的知識都是必須的。</p>

<h2 id="重要特質">重要特質</h2>

<p>以下列舉幾個我認為應徵 AWS Cloud Support Engineer 所需具備的幾項重要特質：</p>

<h3 id="客戶服務技能和溝通能力">客戶服務技能和溝通能力</h3>

<p>AWS Cloud Support Engineer 的主要工作是為客戶提供協助，並且常常會需要將複雜的技術問題拆解成能理解的步驟。讓客戶甚至是其他團隊 (例如開發團隊) 能夠清楚地知道如何排查問題、修正哪些錯誤。<strong>需要能夠清晰、明確地傳達信息，解釋問題和解決方案。</strong></p>

<p>與一般軟體開發工作不同的是，AWS Cloud Support Engineer 由於需要經手系統故障排查的情境，不免會接受客戶環境上的壓力，例如有時候客戶在系統故障影響到營收的同時，充滿緊張和壓力的情境下只希望能盡快把問題解決 (<del>急急急</del>)，原本這些專業的 IT 人員也會瞬間變得很不理智。</p>

<p>試想下你剛進問題現場才短短的 5 分鐘，進行 Live troubleshooting 的同時試著釐清問題檢查每一個項目，並且引導客戶執行正確的步驟確認 (因為有時候客戶給的資訊是錯的)。但有時候客戶就是會覺得你在浪費他的時間，具備保持冷靜和耐心特質的重要性在這種情境下就特別顯著。我個人自己聽過的就有：</p>
<ul>
  <li><em>你到底會不會</em></li>
  <li><em>這沒必要看，我現在只想要他恢復</em></li>
  <li><em>你看這沒用啊，你行不行啊，我檢查過了這沒有問題</em></li>
  <li><em>跟你溝通沒有意義，請找 ECS 專家來跟我溝通</em></li>
</ul>

<p>客戶有情緒但你不能帶著情緒協助，因為這樣就是大家都一起 Panic (<del>大家一起急急急</del>)。我在這份工作確實也學習到很多溝通軟技巧，如果再拉一堆不相干的人進來，我的經驗是這通常只會把問題攪得更亂，對問題調查沒有太大的幫助。</p>

<p>你可以想想自己過去是否有類似的經驗，談論如何與其他人有效地溝通，如何解決複雜的技術問題以及如何處理緊急情況。</p>

<h3 id="解決問題的能力">解決問題的能力</h3>

<p>AWS Cloud Support Engineer 需要有效、精確地識別和解決問題。基本的邏輯和良好的分析能力是必須的，並能夠迅速掌握問題的本質。同時，你需要能夠綜合多個方面的信息，從而定位出問題的癥結點。</p>

<p>簡單來說就是排查問題的過程邏輯要對、能夠正確分析問題、使用正確的方法和工具，知道當問題發生時要如何排查、為什麼用這些工具、為何查 A 不是查 B。比如網站連不上為什麼是用 ping 而不是其他工具、用 ping 返回的結果代表什麼、正確獲取結果後調查的方向是什麼？。</p>

<p>而不是收集到一堆無用的資訊胡亂瞎猜，將問題弄得更發散 (反面案例即是前面提到使用 <code class="language-plaintext highlighter-rouge">ping</code> 檢查 Port 80 能不能正常連通)。</p>

<h3 id="主動學習問題研究能力和自我提升">主動學習、問題研究能力和自我提升</h3>

<p>AWS 產品不斷推陳出新，基本上已經學不完，這項工作不得不跟隨客戶的快速腳步不斷地持續的學習和自我提升。</p>

<p>由於這份工作的角色也從原本用戶端 (使用者) 變成解決問題的角色，在你的專業領域中也必須擁有深入研究問題的能力，當客戶拋出未知且未定義清楚的問題，你通常才能具體的給予明確的排查方向。</p>

<h2 id="專業技術能力-professional-technical-competency">專業技術能力 (Professional Technical Competency)</h2>

<p>由於各個專業領域都有各自側重的項目，例如，專注 Database 專業的工程師跟專注 Linux 領域專業的工程師對於 Linux 知識的要求定義可能有所不同。可能 Database 專業的工程師具體了解 Linux 的基本原理、知道一些基本的指令和明白檔案系統、檔案權限管理、基本問題排查即可；但 Linux 專業的工程師可能就要非常了解 Linux process 運作、知道如何使用 Linux 的工具更加了解系統效能、知道 kernel dump 怎麼解讀、troubleshooting 等等知識。<sup id="fnref:aws-support-faq" role="doc-noteref"><a href="#fn:aws-support-faq" class="footnote" rel="footnote">1</a></sup></p>

<p>每個專業領域會有基礎需要知道的基本知識，但團隊的技能樹也都是隨著客戶需求在變化，解決的問題也是日益更新。以下分享一些我認為可能對於所有專業團隊來說都十分有幫助的學習資源：</p>

<h3 id="aws-相關的知識">AWS 相關的知識</h3>

<p>AWS Cloud Support Engineer 的工作是支援 AWS 的客戶，你需要熟悉 AWS 的服務和產品，並能夠協助客戶解決他們遇到的問題。同時，你需要知道如何設定和管理 AWS 環境，以及如何進行故障排除。因此如果你對 AWS 技術有著深刻的了解，這會對於你在入職之後非常有幫助。</p>

<ul>
  <li><a href="https://www.aws.training/">AWS Training</a>: 包含了各式免費 AWS 訓練的資源</li>
  <li><a href="https://aws.amazon.com/education/awseducate">AWS Educate</a>: AWS Educate 是給予大專院校學生或是任何有意展開雲端旅程的任何人，免費學習和進行 AWS 服務使用試驗的計畫，裡面有一些 Training 通常也直接包含了 Cloud Support Associate 角色的相關訓練材料，能幫助你更加了解這份職位的工作內容和必備技能</li>
</ul>

<p>其他專業技術領域則參考招聘簡介中所提到的對應專業技能，不同技術團隊所重視的技能樹基於專注的產品多少都有些不同，可以透過產品頁面大致了解相關的細節：</p>

<ul>
  <li>[All AWS Products]https://aws.amazon.com/products/</li>
</ul>

<h3 id="container--devops--deployment-領域">Container / DevOps / Deployment 領域</h3>

<p>對於整個 AWS Support，我可能還稱不上非常了解，但如果是 DevOps 和容器技術相關領域的團隊，個人對於該領域小有心得還能分享點東西。</p>

<p>我所在的團隊大部分涵蓋 AWS 服務包含以下：</p>

<ul>
  <li>Elastic Container Service (ECS)</li>
  <li>Elastic Kubernetes Service (EKS)</li>
  <li>AWS Fargate</li>
  <li>Amazon Elastic Container Registry (ECR)</li>
  <li>CloudFormation</li>
  <li>AWS Batch</li>
  <li>AWS AppMesh</li>
  <li>AppRunner</li>
  <li>CodeCommit</li>
  <li>CodeBuild</li>
  <li>CodePipeline</li>
  <li>CodeDeploy</li>
  <li>CodeStar</li>
  <li>Cloud Map</li>
  <li>CodeArtifact</li>
  <li>Cloud Development Kit (CDK)</li>
  <li>Amazon Managed Service for Prometheus (AMP)</li>
  <li>Elastic Beanstalk</li>
  <li>AWS OpsWorks</li>
  <li>Artifact</li>
  <li>X Ray</li>
  <li>DevOps Guru</li>
  <li>CodeGuru</li>
</ul>

<p>目前我的團隊一個人可能可以支持到快 40 個不同的 AWS Service，上述服務基本上都是客戶如果拋出問題來我都會有機會協助。</p>

<p>有鑒於我們團隊也在積極尋求合適的人才，以下是我們團隊十分看重的技術經驗和能力，部分附上一些你可以參考的學習資源：</p>

<p>Linux</p>
<ul>
  <li>Container and Virtualization features</li>
  <li>Container Networking</li>
  <li>Performance analysis (I/O, process state, CPU, memory)</li>
</ul>

<p>Kubernetes / Docker</p>
<ul>
  <li><a href="https://www.cncf.io/certification/cka/">CERTIFIED KUBERNETES ADMINISTRATOR (CKA)</a></li>
  <li><a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">Kubernetes The Hard Way</a></li>
  <li><a href="https://learnk8s.io/">learnk8s</a></li>
  <li><a href="https://docs.docker.com/">Docker official document</a></li>
  <li><a href="https://kubernetes.io/">Kubernetes official document</a></li>
</ul>

<p>CI/CD</p>
<ul>
  <li>部署策略</li>
  <li>基本的版本控制方法</li>
  <li>Infrastructure as Code (IaC)</li>
</ul>

<h2 id="總結">總結</h2>

<p>在這篇內容中，簡述了有關 AWS Cloud Support Engineer 必備的技術能力、特質和相關可以參考的學習材料。如果你是正在考慮加入 AWS Cloud Support Engineer 團隊，希望這些內容能夠更加幫助你建立更多認識。</p>

<p>這篇內容也更像是我自己對於 AWS Cloud Support Engineer 技術職位所具備的長遠學習路徑有個基本的指南，並且幫助你思考如何在你的問題中使用具體的案例正確的展現這些能力。</p>

<h2 id="看更多系列文章">看更多系列文章</h2>

<ul>
  <li><a href="/ten-thing-I-learned-in-amazon">我在 Amazon 學到的 10 件事</a></li>
  <li><a href="/how-am-I-get-into-amazon-before-graduate">我是如何在還沒畢業就錄取並進入到 Amazon 工作</a></li>
  <li><a href="/how-i-pass-aws-all-five-certificate-within-one-year">我是如何在一年內通過 AWS 五大核心認證</a></li>
  <li><a href="/what-is-cloud-support-engineer-doing-in-amazon">Amazon Cloud Support Engineer 到底是在做什麼 (Amazon Web Services / AWS)</a></li>
  <li><a href="/aws-cloud-support-engineer-faq">關於 Cloud Support Engineer 職位的常見問題 (Amazon Web Services / AWS) - AWS Cloud Support Engineer FAQs</a></li>
  <li><a href="/aws-cloud-support-engineer-interview-tips-and-required-skills">在 AWS Cloud Support Engineer 面試中脫穎而出：必備的技術能力和重要特質</a></li>
  <li><a href="/five-years-at-amazon-as-cloud-support-engineer-in-aws">身處 Amazon 工作 5 年學到的事：在 AWS 擔任 Cloud Support Engineer 是什麼樣的體驗</a></li>
</ul>

<h2 id="references">References</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:aws-support-faq" role="doc-endnote">
      <p><a href="/aws-cloud-support-engineer-faq/#q-%E4%BB%A5-big-datanetworkingdatabase--xyz-%E8%AB%8B%E5%95%8F%E9%9D%A2%E8%A9%A6%E9%81%8E%E7%A8%8B%E4%B8%AD%E6%9C%83%E5%95%8F%E5%88%B0-linux-os-%E6%88%96%E6%98%AF-networking-%E7%9B%B8%E9%97%9C%E7%9A%84%E5%95%8F%E9%A1%8C%E5%97%8E-%E9%82%84%E6%98%AF%E5%81%8F%E5%90%91-aws-%E7%94%A2%E5%93%81-abc-%E5%91%A2%E6%9C%83%E5%95%8F%E5%88%B0%E5%A4%9A%E6%B7%B1%E5%85%A5%E5%91%A2">關於 Cloud Support Engineer 職位的常見問題 - 請問面試過程中會問到多深入呢？</a> <a href="#fnref:aws-support-faq" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Yang-Xin Cao (Eason Cao)</name></author><category term="amazon" /><category term="aws" /><category term="amazon web services" /><category term="work" /><category term="Cloud Support" /><category term="Cloud Support Engineer" /><summary type="html"><![CDATA[如果你正準備面試 AWS Cloud Support Engineer 的職位，在這篇文章中，我將與你分享我對於 AWS Cloud Support Engineer 所需的必備技能和特質，幫助你更好地了解這份職位且思考自己目前所具備的能力是與該團隊所交集，讓你在面試中脫穎而出。]]></summary></entry><entry><title type="html">[AWS] 那些沒寫在文件上的事 - AWS Load Balancer Controller v1 升級至 v2</title><link href="https://easoncao.com/eks-aws-load-balancer-controller-v1-to-v2-migration-behavior/" rel="alternate" type="text/html" title="[AWS] 那些沒寫在文件上的事 - AWS Load Balancer Controller v1 升級至 v2" /><published>2023-02-05T00:00:00-06:00</published><updated>2023-02-05T00:00:00-06:00</updated><id>https://easoncao.com/eks-aws-load-balancer-controller-v1-to-v2-migration-behavior</id><content type="html" xml:base="https://easoncao.com/eks-aws-load-balancer-controller-v1-to-v2-migration-behavior/"><![CDATA[<p>隨著 Amazon EKS 對於 Kubernetes 1.22 的推出，1.21 也預計於 February 15, 2023 在 Amazon EKS 終止支援 <sup id="fnref:eks-kubernetes-release-calendar" role="doc-noteref"><a href="#fn:eks-kubernetes-release-calendar" class="footnote" rel="footnote">1</a></sup>，對於 AWS Load Balancer Controller 仍然使用舊版本的用戶來說，勢必面臨需要升級 AWS Load Balancer Controller 至 2.4.1 以上版本的需要。若在現有環境中仍持續運行 AWS Load Balancer Controller v1 (原 ALB Ingress Controller)，此時此刻將可能來到遷移業務需求的高峰。由於 AWS Load Balancer Controller v2 版本帶來了許多的改進，並且與 v1 版本有很大的差異，絕對不是套用一個 YAML 文件就能搞定的事情。</p>

<p>在 AWS Load Balancer Controller 文件中提及了具體的幾個注意事項<sup id="fnref:lb-migrate-v1-to-v2" role="doc-noteref"><a href="#fn:lb-migrate-v1-to-v2" class="footnote" rel="footnote">2</a></sup>，然而，有些情境文件上不見得會全部捕捉 (或是不巧剛好被我遇到)，以下列舉在 AWS Load Balancer Controller 遷移到 v2 版本之前，我個人在協助用戶執行遷移關注到的幾個有趣的問題。</p>

<h2 id="那些沒寫在文件上的事">那些沒寫在文件上的事</h2>

<h3 id="elb-資源命名規則改變帶來可能的停機行為">ELB 資源命名規則改變帶來可能的停機行為</h3>

<p>也許有的人發現 v1 版本的控制器升級到 v2 版本控制器的過程，會由 v2 控制器產生一個新的 ELB 資源，並且將相關資源部署到新產生的資源中完成遷移，舊有 v1 建立的 ELB 資源就不會繼續使用。使得完成遷移的過程如果有依賴舊有 ELB 資源的位置 (e.g. <code class="language-plaintext highlighter-rouge">fe584233-echoserver-echose-XXXX-XXXXXXX.ap-northeast-1.elb.amazonaws.com</code>) 或是有依賴服務關聯，就要記得去改對應的 DNS 紀錄。在文件上也確實具體提到了這項行為：</p>

<blockquote>
  <p>The AWS LoadBalancer resource created for your Ingress will be preserved. If migrating from&lt;v1.1.3, a new AWS LoadBalancer resource will be created and the old AWS LoadBalancer will remain in the account. However, the old AWS LoadBalancer will not be used for the ingress resource.</p>
</blockquote>

<p>對於很多用戶來說，這可能不是什麼大問題；但對於已經有許多系統依賴單一 ELB 資源的用戶來說，前期規劃安裝也沒想太多就直接給他上了。導致這樣的升級過程對這部分用戶來說，就像跑了廁所蹲了馬桶，卻仍然還便秘一樣令人不愉快。</p>

<p>於是就有仍然在運行 v1.0.1 版本的客戶們很天才地提了我甚至都沒思考過的升級路徑：</p>

<blockquote>
  <p>既然文件上是說 <code class="language-plaintext highlighter-rouge">&lt;v1.1.3</code>，那是不是我先升級到大於這個版本之後 (例如：<code class="language-plaintext highlighter-rouge">v1.1.9</code>)，再升級到 v2 版本就可以保留原有的 ELB 資源了呢？</p>

  <p>升級路徑為 v1.0.1 -&gt; v1.1.9 -&gt; v2</p>
</blockquote>

<p>邏輯上好像沒有什麼謬誤，但在看過 AWS Load Balancer Controlelr 的原始代碼後，很遺憾，只能說這種想法真的是太美好了。</p>

<h4 id="實際測試行為">實際測試行為</h4>

<p>根據 AWS Load Balancer Controller v2 版本對應產生的 ELB 資源名稱都存在 <code class="language-plaintext highlighter-rouge">k8s</code> 保留字串的不正確觀察，我大可以預料到只要是舊版本的 v1 控制器遷移很可能都無法複用舊有的 ELB 資源。</p>

<p>但針對上述的行為我們可以大膽假設，仍需要仔細驗證一下相關的行為。為求真相，實際在我的環境簡單複製測試後，直接地瓦解了上述的論證。</p>

<h5 id="step-1部署-alb-ingress-controller-v101">Step 1：部署 ALB Ingress Controller v1.0.1</h5>

<p>首先，我在我的環境運行了 <code class="language-plaintext highlighter-rouge">v1.0.1</code> 的控制器，歷經一番考古和 kubectl convert 轉換 (舊有的 API 宣告)，將舊版 v1.0.1 控制器成功安裝到 Kubernetes 1.20 版本中運行，並且部署一個簡單的範例應用：</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Running controller v1.0.1</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.0.1/docs/examples/rbac-role.yaml
<span class="nv">$ </span>wget https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.0.1/docs/examples/alb-ingress-controller.yaml
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> alb-ingress-controller.yaml
<span class="nv">$ </span>kubectl logs <span class="nt">-n</span> kube-system <span class="si">$(</span>kubectl get po <span class="nt">-n</span> kube-system | egrep <span class="nt">-o</span> alb-ingress[a-zA-Z0-9-]+<span class="si">)</span>

W0130 12:45:29.518393       1 client_config.go:552] Neither <span class="nt">--kubeconfig</span> nor <span class="nt">--master</span> was specified.  Using the inClusterConfig.  This might not work.
<span class="nt">-------------------------------------------------------------------------------</span>
AWS ALB Ingress controller
  Release:    v1.0.1
  Build:      git-ebac62dd
  Repository: https://github.com/kubernetes-sigs/aws-alb-ingress-controller.git
<span class="nt">-------------------------------------------------------------------------------</span>

<span class="c"># Running a sample application</span>
<span class="nv">$ </span>kubectl describe ing <span class="nt">-n</span> echoserver echoserver
Address:          fe584233-echoserver-echose-XXXX-XXXXXXX.ap-northeast-1.elb.amazonaws.com
...
</code></pre></div></div>

<h5 id="step-2升級至-alb-ingress-controller-v119">Step 2：升級至 ALB Ingress Controller v1.1.9</h5>

<p>並且直接更新到 <code class="language-plaintext highlighter-rouge">v1.1.9</code> 版本，即使 ALB Ingress Controller 存在刷新操作，仍然針對原有的 Ingress 物件保留了原本的部署關聯 (<code class="language-plaintext highlighter-rouge">fe584233-echoserver-echose-XXXX-XXXXXXX.ap-northeast-1.elb.amazonaws.com</code>)：</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Upgrade and deploy to v1.1.9</span>

<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.9/docs/examples/rbac-role.yaml

<span class="c"># Use kubectl and update the image to "docker.io/amazon/aws-alb-ingress-controller:v1.1.9"</span>
<span class="nv">$ </span>kubectl logs <span class="nt">-n</span> kube-system <span class="si">$(</span>kubectl get po <span class="nt">-n</span> kube-system | egrep <span class="nt">-o</span> <span class="s2">"alb-ingress[a-zA-Z0-9-]+"</span><span class="si">)</span>

W0130 13:05:04.770613       1 client_config.go:549] Neither <span class="nt">--kubeconfig</span> nor <span class="nt">--master</span> was specified.  Using the inClusterConfig.  This might not work.
<span class="nt">-------------------------------------------------------------------------------</span>
AWS ALB Ingress controller
  Release:    v1.1.9
  Build:      6c19d2fb
  Repository: https://github.com/kubernetes-sigs/aws-alb-ingress-controller.git
<span class="nt">-------------------------------------------------------------------------------</span>

<span class="c"># ELB name doesn't change</span>
<span class="nv">$ </span>kubectl describe ing <span class="nt">-n</span> echoserver echoserver
Address:          fe584233-echoserver-echose-XXXX-XXXXXXX.ap-northeast-1.elb.amazonaws.com
</code></pre></div></div>

<h5 id="step-3升級至-aws-load-balancer-controller-v2">Step 3：升級至 AWS Load Balancer Controller v2</h5>

<p>在預備升級 v2 的過程 ELB 資源都持續存在，且 Ingress 也同樣關聯舊有的 ELB 資源；然而，一旦 v2 版本一部署下去，新的 ELB 資源立即被建立，並且使用不同的命名 (<code class="language-plaintext highlighter-rouge">k8s-echoserv-echoserv-XXXXXXXX-XXXXXXX</code>) 運作，並且關聯的 Ingress 和 Kubernetes Service 均遷移使用新的 ELB 資源：</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Update the controller to v2</span>
<span class="c"># The old ALB Ingress controller has been uninstalled at this moment, and can see the ingress object is still preserved</span>

<span class="nv">$ </span>kubectl describe ing <span class="nt">-n</span> echoserver echoserver
Address:          fe584233-echoserver-echose-XXXX-XXXXXXX.ap-northeast-1.elb.amazonaws.com
....

<span class="nv">$ </span>helm <span class="nb">install </span>aws-load-balancer-controller eks/aws-load-balancer-controller <span class="se">\</span>
  <span class="nt">-n</span> kube-system <span class="se">\</span>
  <span class="nt">--set</span> <span class="nv">clusterName</span><span class="o">=</span>eks <span class="se">\</span>
  <span class="nt">--set</span> serviceAccount.create<span class="o">=</span><span class="nb">false</span> <span class="se">\</span>
  <span class="nt">--set</span> serviceAccount.name <span class="o">=</span>aws-load-balancer-controller

<span class="c"># Once v2 controller has been installed, the controller will update the ELB name</span>

<span class="nv">$ </span>kubectl describe ing <span class="nt">-n</span> echoserver echoserver
Address:          k8s-echoserv-echoserv-XXXXXXXX-XXXXXXX.ap-northeast-1.elb.amazonaws.com
...

Events:
  Type    Reason                  Age   From     Message
  <span class="nt">----</span>    <span class="nt">------</span>                  <span class="nt">----</span>  <span class="nt">----</span>     <span class="nt">-------</span>
  Normal  SuccessfullyReconciled  11s   ingress  Successfully reconciled
</code></pre></div></div>

<p>此時，舊有的 ELB 資源 (<code class="language-plaintext highlighter-rouge">fe584233-echoserver-echose-XXXX-XXXXXXX.ap-northeast-1.elb.amazonaws.com </code>) 仍然存在，只是 AWS Load Balancer Controller 並未直接管理及操作該資源，並且不再執行註冊 Kubernetes Service 至相關 Target Group 資源。</p>

<h4 id="行為分析">行為分析</h4>

<p>從上述部署來看，我們可以觀察到 v1 版本的控制器在古時候，使用 namespace + ingress 名稱的方式進行組合命名，這項行為也確實在 <code class="language-plaintext highlighter-rouge">v1.0.1</code> 如此呼叫 (Source: <a href="https://github.com/kubernetes-sigs/aws-load-balancer-controller/blob/v1.0.1/internal/alb/lb/loadbalancer.go#L299-L317">L299-L317</a>)，在 <code class="language-plaintext highlighter-rouge">v1.1.9</code> (Source: <a href="https://github.com/kubernetes-sigs/aws-load-balancer-controller/blob/v1.1.9/internal/alb/lb/loadbalancer.go#L285-L304">L285-L304</a>) 亦同。一個 <code class="language-plaintext highlighter-rouge">NameLB</code> 短短幾行副程式道盡前人的字串處理之美 (Source: <a href="https://github.com/kubernetes-sigs/aws-load-balancer-controller/blob/v1.0.1/internal/alb/generator/name.go#L23-L39">v1.0.1</a>, <a href="https://github.com/kubernetes-sigs/aws-load-balancer-controller/blob/v1.1.9/internal/alb/generator/name.go#L23-L39">v1.1.9</a>)：</p>

<div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">func</span> <span class="p">(</span><span class="n">gen</span> <span class="o">*</span><span class="n">NameGenerator</span><span class="p">)</span> <span class="n">NameLB</span><span class="p">(</span><span class="n">namespace</span> <span class="kt">string</span><span class="p">,</span> <span class="n">ingressName</span> <span class="kt">string</span><span class="p">)</span> <span class="kt">string</span> <span class="p">{</span>
  <span class="n">hasher</span> <span class="o">:=</span> <span class="n">md5</span><span class="o">.</span><span class="n">New</span><span class="p">()</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hasher</span><span class="o">.</span><span class="n">Write</span><span class="p">([]</span><span class="kt">byte</span><span class="p">(</span><span class="n">namespace</span> <span class="o">+</span> <span class="n">ingressName</span><span class="p">))</span>
  <span class="n">hash</span> <span class="o">:=</span> <span class="n">hex</span><span class="o">.</span><span class="n">EncodeToString</span><span class="p">(</span><span class="n">hasher</span><span class="o">.</span><span class="n">Sum</span><span class="p">(</span><span class="no">nil</span><span class="p">))[</span><span class="o">:</span><span class="m">4</span><span class="p">]</span>

  <span class="n">r</span><span class="p">,</span> <span class="n">_</span> <span class="o">:=</span> <span class="n">regexp</span><span class="o">.</span><span class="n">Compile</span><span class="p">(</span><span class="s">"[[:^alnum:]]"</span><span class="p">)</span>
  <span class="n">name</span> <span class="o">:=</span> <span class="n">fmt</span><span class="o">.</span><span class="n">Sprintf</span><span class="p">(</span><span class="s">"%s-%s-%s"</span><span class="p">,</span>
    <span class="n">r</span><span class="o">.</span><span class="n">ReplaceAllString</span><span class="p">(</span><span class="n">gen</span><span class="o">.</span><span class="n">ALBNamePrefix</span><span class="p">,</span> <span class="s">"-"</span><span class="p">),</span>
    <span class="n">r</span><span class="o">.</span><span class="n">ReplaceAllString</span><span class="p">(</span><span class="n">namespace</span><span class="p">,</span> <span class="s">""</span><span class="p">),</span>
    <span class="n">r</span><span class="o">.</span><span class="n">ReplaceAllString</span><span class="p">(</span><span class="n">ingressName</span><span class="p">,</span> <span class="s">""</span><span class="p">),</span>
  <span class="p">)</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">name</span><span class="p">)</span> <span class="o">&gt;</span> <span class="m">26</span> <span class="p">{</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">name</span><span class="p">[</span><span class="o">:</span><span class="m">26</span><span class="p">]</span>
  <span class="p">}</span>
  <span class="n">name</span> <span class="o">=</span> <span class="n">name</span> <span class="o">+</span> <span class="s">"-"</span> <span class="o">+</span> <span class="n">hash</span>
  <span class="k">return</span> <span class="n">name</span>
<span class="p">}</span>
</code></pre></div></div>

<p>然而，在 v2 版本除了功能性的改進，針對 ALB Ingress Controller 也確實做了多項程式上的重構。最顯著的就是上述命名的變更，在 v2 版本的命名中存在了更多元的規範 (Source: <a href="https://github.com/kubernetes-sigs/aws-load-balancer-controller/blob/8d282339857c615b0bec6e3d984063c697e98f70/pkg/ingress/model_build_load_balancer.go#L90-L124">v2.4.4, L90-L124</a>)：</p>

<div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">func</span> <span class="p">(</span><span class="n">t</span> <span class="o">*</span><span class="n">defaultModelBuildTask</span><span class="p">)</span> <span class="n">buildLoadBalancerName</span><span class="p">(</span><span class="n">_</span> <span class="n">context</span><span class="o">.</span><span class="n">Context</span><span class="p">,</span> <span class="n">scheme</span> <span class="n">elbv2model</span><span class="o">.</span><span class="n">LoadBalancerScheme</span><span class="p">)</span> <span class="p">(</span><span class="kt">string</span><span class="p">,</span> <span class="kt">error</span><span class="p">)</span> <span class="p">{</span>
  <span class="o">...</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">explicitNames</span><span class="p">)</span> <span class="o">==</span> <span class="m">1</span> <span class="p">{</span>
    <span class="n">name</span><span class="p">,</span> <span class="n">_</span> <span class="o">:=</span> <span class="n">explicitNames</span><span class="o">.</span><span class="n">PopAny</span><span class="p">()</span>
    <span class="c">// The name of the loadbalancer can only have up to 32 characters</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">name</span><span class="p">)</span> <span class="o">&gt;</span> <span class="m">32</span> <span class="p">{</span>
      <span class="k">return</span> <span class="s">""</span><span class="p">,</span> <span class="n">errors</span><span class="o">.</span><span class="n">New</span><span class="p">(</span><span class="s">"load balancer name cannot be longer than 32 characters"</span><span class="p">)</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">name</span><span class="p">,</span> <span class="no">nil</span>
  <span class="p">}</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">explicitNames</span><span class="p">)</span> <span class="o">&gt;</span> <span class="m">1</span> <span class="p">{</span>
    <span class="k">return</span> <span class="s">""</span><span class="p">,</span> <span class="n">errors</span><span class="o">.</span><span class="n">Errorf</span><span class="p">(</span><span class="s">"conflicting load balancer name: %v"</span><span class="p">,</span> <span class="n">explicitNames</span><span class="p">)</span>
  <span class="p">}</span>
  <span class="n">uuidHash</span> <span class="o">:=</span> <span class="n">sha256</span><span class="o">.</span><span class="n">New</span><span class="p">()</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">uuidHash</span><span class="o">.</span><span class="n">Write</span><span class="p">([]</span><span class="kt">byte</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">clusterName</span><span class="p">))</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">uuidHash</span><span class="o">.</span><span class="n">Write</span><span class="p">([]</span><span class="kt">byte</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">ingGroup</span><span class="o">.</span><span class="n">ID</span><span class="o">.</span><span class="n">String</span><span class="p">()))</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">uuidHash</span><span class="o">.</span><span class="n">Write</span><span class="p">([]</span><span class="kt">byte</span><span class="p">(</span><span class="n">scheme</span><span class="p">))</span>
  <span class="n">uuid</span> <span class="o">:=</span> <span class="n">hex</span><span class="o">.</span><span class="n">EncodeToString</span><span class="p">(</span><span class="n">uuidHash</span><span class="o">.</span><span class="n">Sum</span><span class="p">(</span><span class="no">nil</span><span class="p">))</span>

  <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">ingGroup</span><span class="o">.</span><span class="n">ID</span><span class="o">.</span><span class="n">IsExplicit</span><span class="p">()</span> <span class="p">{</span>
    <span class="n">payload</span> <span class="o">:=</span> <span class="n">invalidLoadBalancerNamePattern</span><span class="o">.</span><span class="n">ReplaceAllString</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">ingGroup</span><span class="o">.</span><span class="n">ID</span><span class="o">.</span><span class="n">Name</span><span class="p">,</span> <span class="s">""</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fmt</span><span class="o">.</span><span class="n">Sprintf</span><span class="p">(</span><span class="s">"k8s-%.17s-%.10s"</span><span class="p">,</span> <span class="n">payload</span><span class="p">,</span> <span class="n">uuid</span><span class="p">),</span> <span class="no">nil</span>
  <span class="p">}</span>

  <span class="n">sanitizedNamespace</span> <span class="o">:=</span> <span class="n">invalidLoadBalancerNamePattern</span><span class="o">.</span><span class="n">ReplaceAllString</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">ingGroup</span><span class="o">.</span><span class="n">ID</span><span class="o">.</span><span class="n">Namespace</span><span class="p">,</span> <span class="s">""</span><span class="p">)</span>
  <span class="n">sanitizedName</span> <span class="o">:=</span> <span class="n">invalidLoadBalancerNamePattern</span><span class="o">.</span><span class="n">ReplaceAllString</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">ingGroup</span><span class="o">.</span><span class="n">ID</span><span class="o">.</span><span class="n">Name</span><span class="p">,</span> <span class="s">""</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">fmt</span><span class="o">.</span><span class="n">Sprintf</span><span class="p">(</span><span class="s">"k8s-%.8s-%.8s-%.10s"</span><span class="p">,</span> <span class="n">sanitizedNamespace</span><span class="p">,</span> <span class="n">sanitizedName</span><span class="p">,</span> <span class="n">uuid</span><span class="p">),</span> <span class="no">nil</span>
<span class="p">}</span>
</code></pre></div></div>

<p>除了對於 ELB 命名的檢查更為嚴謹了，也多了以 Cluster 名稱、Ingress Group 和多個關聯的識別資料進行雜湊產生 UUID，最終透過 <code class="language-plaintext highlighter-rouge">k8s-</code> 前綴組織成了人見 - 人們不見得愛的正則化命名。</p>

<h2 id="總結">總結</h2>

<p>本章用了數百字的篇幅傳遞 v2 版本真的改很多東西，作為總結：</p>

<p>若從 v1 版本遷移至 v2 控制器涉及你目前所遭遇或未來將面臨的情境，則生成新的 ELB 資源都是<strong>很有可能且可以預期的行為</strong>。</p>

<p>在規劃遷移的同時，若尚未對 ELB 資源存取設計另一層存取介面的情境，考慮透過 DNS 紀錄 (CNAME) 方法管理應對 ELB 資源更新變更的存取位置是一種常見的做法，以降低用戶端因上述升級行為所產生的改動；也建議透過規劃相應的停機時間和更新 DNS 對應紀錄的變更紀錄納入考量。</p>

<p>畢竟，有很多事情只是人生暫時過不去的坎，在這樣的機制下，沒有什麼事情是清一下 DNS 快取以及「請稍候重試」提示訊息不能解決的。</p>

<h2 id="related-posts">Related Posts</h2>

<ul>
  <li><a href="/eks-best-practice-load-balancing-1-en">Best practice for load balancing - 1. Let’s start with an example from Kubernetes document</a></li>
  <li><a href="/eks-best-practice-load-balancing-2-en">Best practice for load balancing - 2. imbalanced problem</a></li>
  <li><a href="/eks-best-practice-load-balancing-3-en">Best practice for load balancing - 3. what controller should I use</a></li>
</ul>

<h2 id="references">References</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:eks-kubernetes-release-calendar" role="doc-endnote">
      <p><a href="https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html#kubernetes-release-calendar">Amazon EKS Kubernetes versions - Amazon EKS Kubernetes release calendar</a> <a href="#fnref:eks-kubernetes-release-calendar" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:lb-migrate-v1-to-v2" role="doc-endnote">
      <p><a href="https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/deploy/upgrade/migrate_v1_v2/">AWS Load Balancer Controller - Migrate from v1 to v2</a> <a href="#fnref:lb-migrate-v1-to-v2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Yang-Xin Cao (Eason Cao)</name></author><category term="aws" /><category term="amazon web services" /><category term="amazon" /><category term="ELB" /><category term="ALB" /><category term="Load Balancer" /><category term="Elastic Load Balancer" /><category term="ALB Ingress Controller" /><category term="Kubernetes" /><category term="k8s" /><category term="EKS" /><category term="Elastic Kubernetes Service" /><category term="AWS Load Balancer Controller" /><summary type="html"><![CDATA[This article is sharing the best practice for doing load balancing on Amazon EKS, learn what is advantage and disadvantage of using different controller. We will discuss more detail about what is the problem of using default Kubernetes service deployment as mentioned on official document.]]></summary></entry><entry><title type="html">[AWS][EKS] Zero downtime deployment(RollingUpdate) when using AWS Load Balancer Controller on Amazon EKS</title><link href="https://easoncao.com/zero-downtime-deployment-when-using-alb-ingress-controller-on-amazon-eks-and-prevent-502-error/" rel="alternate" type="text/html" title="[AWS][EKS] Zero downtime deployment(RollingUpdate) when using AWS Load Balancer Controller on Amazon EKS" /><published>2022-06-01T00:00:00-05:00</published><updated>2022-06-01T00:00:00-05:00</updated><id>https://easoncao.com/zero-downtime-deployment-when-using-alb-ingress-controller-on-amazon-eks-and-prevent-502-error</id><content type="html" xml:base="https://easoncao.com/zero-downtime-deployment-when-using-alb-ingress-controller-on-amazon-eks-and-prevent-502-error/"><![CDATA[<p>This article is describing the thing you need to aware when using ALB Ingress Controller (AWS Load Balancer Controller) to do deployment and prevent 502 errors.</p>

<figure class="">
  <img src="/assets/images/posts/2020/10/zero-downtime-deployment-when-using-alb-ingress-controller/deployment-workflow-alb-ingress-controller-4-side-note-of-deregister.png" alt="Overview" />
  
    <figcaption>
      Overview

    </figcaption>
  
</figure>

<h2 id="whats-aws-load-balancer-controller-legacy-alb-ingress-controller">What’s AWS Load Balancer Controller (legacy ALB Ingress Controller)</h2>

<p>Kubernetes doesn’t involve the Application Load Balancer (ALB) deployment in the native implementation for using Kubernetes service object with <code class="language-plaintext highlighter-rouge">type=LoadBalancer</code>. Therefore, if you would like to expose your container service with Application Load Balancer (ALB) on EKS, it is recommended to integrate with AWS Load Balancer Controller (In the past, it was <strong>ALB Ingress Controller</strong> when it firstly initiated by <a href="https://aws.amazon.com/blogs/apn/coreos-and-ticketmaster-collaborate-to-bring-aws-application-load-balancer-support-to-kubernetes/">CoreOS and Ticketmaster</a>). This controller make it possible to manage have load balancers with Kubernetes deployment.</p>

<p>Below is showing an overview diagram that describing the controller workflow:</p>

<figure class="">
  <img src="/assets/images/posts/2020/10/zero-downtime-deployment-when-using-alb-ingress-controller/controller-design.png" alt="How AWS Load Balancer Controller works" />
  
    <figcaption>
      How AWS Load Balancer Controller works - <a href="https://kubernetes-sigs.github.io/aws-load-balancer-controller/guide/controller/how-it-works/">source</a>

    </figcaption>
  
</figure>

<ul>
  <li>(1) The controller watches for ingress events from the API server.</li>
  <li>(2) An ALB (ELBv2) is created in AWS for the new ingress resource. This ALB can be internet-facing or internal.</li>
  <li>(3) Target Groups are created in AWS for each unique Kubernetes service described in the ingress resource.</li>
  <li>(4) Listeners are created for every port detailed in your ingress resource annotations.</li>
  <li>(5) Rules(ELB Listener Rules) are created for each path specified in your ingress resource. This ensures traffic to a specific path is routed to the correct Kubernetes Service.</li>
</ul>

<blockquote>
  <p>Note: AWS ALB Ingress Controller is replaced, while rename it to be “AWS Load Balancer Controller” with several new features coming out. For more detail, please refer the <a href="https://github.com/kubernetes-sigs/aws-alb-ingress-controller">GitHub project - kubernetes-sigs/aws-alb-ingress-controller</a></p>
</blockquote>

<h2 id="how-to-deploy-kubernetes-with-aws-load-balancer-controller">How to deploy Kubernetes with AWS Load Balancer Controller?</h2>

<p>Using Application Load Balancer as example, when running the controller, AWS Load Balancer Controller will be deployed as a Pod running on your worker node while continously monitor/watch your cluster state. Once there have any request for <code class="language-plaintext highlighter-rouge">Ingress</code>  object creation, AWS Load Balancer Controller will help you to manage and create Application Load Balancer resource. Here is a part of example for <code class="language-plaintext highlighter-rouge">v1.1.8</code> deployment manifest:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">app.kubernetes.io/name</span><span class="pi">:</span> <span class="s">alb-ingress-controller</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">alb-ingress-controller</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">kube-system</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">app.kubernetes.io/name</span><span class="pi">:</span> <span class="s">alb-ingress-controller</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app.kubernetes.io/name</span><span class="pi">:</span> <span class="s">alb-ingress-controller</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">containers</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">alb-ingress-controller</span>
          <span class="na">args</span><span class="pi">:</span>
            <span class="c1"># Setting the ingress-class flag below ensures that only ingress resources with the</span>
            <span class="c1"># annotation kubernetes.io/ingress.class: "alb" are respected by the controller. You may</span>
            <span class="c1"># choose any class you'd like for this controller to respect.</span>
            <span class="pi">-</span> <span class="s">--ingress-class=alb</span>

            <span class="c1"># REQUIRED</span>
            <span class="c1"># Name of your cluster. Used when naming resources created</span>
            <span class="c1"># by the ALB Ingress Controller, providing distinction between</span>
            <span class="c1"># clusters.</span>
            <span class="c1"># - --cluster-name=devCluster</span>

            <span class="c1"># AWS VPC ID this ingress controller will use to create AWS resources.</span>
            <span class="c1"># If unspecified, it will be discovered from ec2metadata.</span>
            <span class="c1"># - --aws-vpc-id=vpc-xxxxxx</span>

            <span class="c1"># AWS region this ingress controller will operate in.</span>
            <span class="c1"># If unspecified, it will be discovered from ec2metadata.</span>
            <span class="c1"># List of regions: http://docs.aws.amazon.com/general/latest/gr/rande.html#vpc_region</span>
            <span class="c1"># - --aws-region=us-west-1</span>
          <span class="err"> </span><span class="na">image</span><span class="pi">:</span> <span class="s">docker.io/amazon/aws-alb-ingress-controller:v1.1.8</span>
      <span class="na">serviceAccountName</span><span class="pi">:</span> <span class="s">alb-ingress-controller</span>
</code></pre></div></div>

<p>The deployment basically will run a copy of ALB Ingress Controller (pod/alb-ingress-controller-xxxxxxxx-xxxxx) in <code class="language-plaintext highlighter-rouge">kube-system</code>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NAMESPACE     NAME                                          READY   STATUS    RESTARTS   AGE
kube-system   pod/alb-ingress-controller-5fd8d5d894-8kf7z   1/1     Running   0          28s

NAMESPACE     NAME                                     READY   UP-TO-DATE   AVAILABLE   AGE
kube-system   deployment.apps/alb-ingress-controller   1/1     1            1           3m48s
</code></pre></div></div>

<p>Since v2, the controller added lots of different custom resources and enhancements. But the core deployment still preserve many thing that mentioned in this post. Depending on your environment, the default and suggested installation steps may also involve the configuration of IRSA (IAM Role for Service Account) to grant permission for the AWS Load Balancer Controller Pods in order to operate AWS resources (e.g. ELB), so it is recommended to take a look official documentation to help you quickly understand how to install the controller:</p>

<ul>
  <li><a href="https://docs.aws.amazon.com/eks/latest/userguide/alb-ingress.html">AWS Load Balancer Controller on Amazon EKS</a></li>
</ul>

<p>In addition, the service can be deployed as <code class="language-plaintext highlighter-rouge">Ingress</code> Object. For example, if you tried to deploy the simple 2048 application:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-namespace.yaml
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-deployment.yaml
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-service.yaml
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-ingress.yaml
</code></pre></div></div>

<p>The file <code class="language-plaintext highlighter-rouge">2048-ingress.yaml</code> is mentioning the <code class="language-plaintext highlighter-rouge">annotations</code>, <code class="language-plaintext highlighter-rouge">spec</code> in format that supported by ALB Ingress Controller can recognize (Before Kubernetes 1.18):</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">extensions/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Ingress</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s2">"</span><span class="s">2048-ingress"</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s2">"</span><span class="s">2048-game"</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="na">kubernetes.io/ingress.class</span><span class="pi">:</span> <span class="s">alb</span>
    <span class="na">alb.ingress.kubernetes.io/scheme</span><span class="pi">:</span> <span class="s">internet-facing</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">2048-ingress</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">rules</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">http</span><span class="pi">:</span>
        <span class="na">paths</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">path</span><span class="pi">:</span> <span class="s">/*</span>
            <span class="na">backend</span><span class="pi">:</span>
              <span class="na">serviceName</span><span class="pi">:</span> <span class="s2">"</span><span class="s">service-2048"</span>
              <span class="na">servicePort</span><span class="pi">:</span> <span class="m">80</span>
</code></pre></div></div>

<p>Before the IngressClass resource and ingressClassName field were added in Kubernetes 1.18, Ingress classes were specified with a <code class="language-plaintext highlighter-rouge">kubernetes.io/ingress.class</code> annotation on the Ingress. So right now, you should see the ingress specification will be defined as below if you are using controller version <code class="language-plaintext highlighter-rouge">v2.x</code>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.4.1/docs/examples/2048/2048_full.yaml
</code></pre></div></div>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">networking.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Ingress</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">game-2048</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">ingress-2048</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="na">alb.ingress.kubernetes.io/scheme</span><span class="pi">:</span> <span class="s">internet-facing</span>
    <span class="na">alb.ingress.kubernetes.io/target-type</span><span class="pi">:</span> <span class="s">ip</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">ingressClassName</span><span class="pi">:</span> <span class="s">alb</span>
  <span class="na">rules</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">http</span><span class="pi">:</span>
        <span class="na">paths</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">path</span><span class="pi">:</span> <span class="s">/</span>
          <span class="na">pathType</span><span class="pi">:</span> <span class="s">Prefix</span>
          <span class="na">backend</span><span class="pi">:</span>
            <span class="na">service</span><span class="pi">:</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">service-2048</span>
              <span class="na">port</span><span class="pi">:</span>
                <span class="na">number</span><span class="pi">:</span> <span class="m">80</span>
</code></pre></div></div>

<p>The ingress object will construct ELB Listeners according rules and forward the connection to the corresponding backend(<code class="language-plaintext highlighter-rouge">serviceName</code>), which match the group of service <code class="language-plaintext highlighter-rouge">service-2048</code>, any traffic match the rule <code class="language-plaintext highlighter-rouge">/*</code> will be routed to the group of selected Pods. In this case, Pods are exposed on the worker node based on <code class="language-plaintext highlighter-rouge">type=NodePort</code>:</p>

<p>Here is the definition of this Kubernetes service:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s2">"</span><span class="s">service-2048"</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s2">"</span><span class="s">2048-game"</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="m">80</span>
      <span class="na">targetPort</span><span class="pi">:</span> <span class="m">80</span>
      <span class="na">protocol</span><span class="pi">:</span> <span class="s">TCP</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">NodePort</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s2">"</span><span class="s">2048"</span>
</code></pre></div></div>

<h2 id="so--whats-the-problem">So … what’s the problem?</h2>

<p><strong>Zero downtime deployment</strong> is always a big challenge for DevOps/Operation team when running any kind of business. When you try to apply the controller as a solution to expose your service, it has a couple of things need to take care due to the behavior of Kubernetes, ALB and AWS Load Balancer Controller. To achieve zero downtime, you need to consider many perspectives, some new challenges will also popup when you would like to roll out the new deployment for your Pods with AWS Load Balancer Controller.</p>

<p>Let’s use the 2048 game as example to describe the scenario when you are trying to roll out a new version of your container application. In my environment, I have:</p>

<ul>
  <li>A Kubernetes service <code class="language-plaintext highlighter-rouge">service/service-2048</code> using <code class="language-plaintext highlighter-rouge">NodePort</code> to expose the service</li>
  <li>The deployment also have 5 copy of Pods for 2048 game, which is my backend application waiting for connections forwarding by Application Load Balancer (ALB)</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NAMESPACE     NAME                                          READY   STATUS    RESTARTS   AGE
2048-game     pod/2048-deployment-58fb66554b-2f748          1/1     Running   0          53s
2048-game     pod/2048-deployment-58fb66554b-4hz5q          1/1     Running   0          53s
2048-game     pod/2048-deployment-58fb66554b-jdfps          1/1     Running   0          53s
2048-game     pod/2048-deployment-58fb66554b-rlpqm          1/1     Running   0          53s
2048-game     pod/2048-deployment-58fb66554b-s492n          1/1     Running   0          53s

NAMESPACE     NAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>         AGE
2048-game     service/service-2048   NodePort    10.100.53.119   &lt;none&gt;        80:30337/TCP    52s

NAMESPACE     NAME                                     READY   UP-TO-DATE   AVAILABLE   AGE
2048-game     deployment.apps/2048-deployment          5/5     5            5           53s
</code></pre></div></div>

<p>And for sure, once the controller correctly set up and provision the ELB resource, the full domain of ELB also will be recorded to the <code class="language-plaintext highlighter-rouge">Ingress</code> object:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ kubectl get ingress -n 2048-game
NAME           HOSTS   ADDRESS                                                                      PORTS   AGE
2048-ingress   *       xxxxxxxx-2048game-xxxxxxxx-xxxx-xxxxxxxxx.ap-northeast-1.elb.amazonaws.com   80      11m
</code></pre></div></div>

<p>I can use the DNS name as endpoint to visit my container service:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>curl <span class="nt">-s</span> xxxxxxxx-2048game-xxxxxxxx-xxxx-xxxxxxxxx.ap-northeast-1.elb.amazonaws.com | <span class="nb">head</span>
&lt;<span class="o">!</span>DOCTYPE html&gt;
&lt;html&gt;
&lt;<span class="nb">head</span><span class="o">&gt;</span>
  &lt;meta <span class="nv">charset</span><span class="o">=</span><span class="s2">"utf-8"</span><span class="o">&gt;</span>
  &lt;title&gt;2048&lt;/title&gt;

  &lt;<span class="nb">link </span><span class="nv">href</span><span class="o">=</span><span class="s2">"style/main.css"</span> <span class="nv">rel</span><span class="o">=</span><span class="s2">"stylesheet"</span> <span class="nb">type</span><span class="o">=</span><span class="s2">"text/css"</span><span class="o">&gt;</span>
  &lt;<span class="nb">link </span><span class="nv">rel</span><span class="o">=</span><span class="s2">"shortcut icon"</span> <span class="nv">href</span><span class="o">=</span><span class="s2">"favicon.ico"</span><span class="o">&gt;</span>
  ...
</code></pre></div></div>

<figure class="">
  <img src="/assets/images/posts/2020/10/zero-downtime-deployment-when-using-alb-ingress-controller/alb-ingress-controller-with-2048-game-screenshot.png" alt="2048 Game deployed with controller" />
  
    <figcaption>
      2048 Game deployed with controller

    </figcaption>
  
</figure>

<p>This application can be any kind of critical service you are running. As a administrator, SRE (Site Reliability Engineer), member of operation team or a DevOps engineer, the goal and your duty is: <strong>we always try to ensure the service can run properly without any issue and no interruption</strong> (Sometimes it means good sleep). That’s why people really gets hand dirty and maintain the regular operation usually don’t like to adopt service change, because it generally means <strong>unstable</strong>.</p>

<p>No matter you don’t want to change, with any new business requests, you still can face the challenges like: your developers are saying that <strong>“Oh! we need to upgrade the application”</strong>, <strong>“we are going to roll out a bug fix”</strong>, <strong>“the new feature is going to be online”</strong>, no one can one hundred percent guarantees the service can run properly if any changes applied, because system usually has its limitation and trade-off. Any service downtime can lead anyone of stakeholders(users, operation team or leadership) unhappy.</p>

<p>However, <strong>the question is that can we better to address these problem once we know the limitation and its behavior?</strong> Some people in Taiwan will also consider to put <a href="https://en.wikipedia.org/wiki/Kuai_Kuai_culture">Kuai Kuai</a> on the workstation because they believe it can make service happy, but I am not very obsessed with this method, so in the following section I will try to walk through more realistic logic and phenomena by using the 2048 game as my sample service.</p>

<p>I am going to use a simple loop trick to continously access my service via the endpoint <code class="language-plaintext highlighter-rouge">xxxxxxxx-2048game-xxxxxxxx-xxxx-xxxxxxxxx.ap-northeast-1.elb.amazonaws.com</code> to demonstrate a scenario: This is a popular web service and we always have customer need to access it. (e.g. social media platform, bitcoin trading platform or any else, we basically have zero tolerance for any service downtime as it can impact our revenue.), as below:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="k">while </span><span class="nb">true</span><span class="p">;</span><span class="k">do</span> ./request-my-service.sh<span class="p">;</span> <span class="nb">sleep </span>0.1<span class="p">;</span> <span class="k">done
</span><span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.010038
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.012131
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.005366
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.010119
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.012066
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.005451
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.010006
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.012084
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.005598
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.010086
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.012162
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.005278
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.010326
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.012193
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.005347
...
</code></pre></div></div>

<p>Meanwhile, I am using <code class="language-plaintext highlighter-rouge">RollingUpdate</code> strategy in my Kubernetes deployment strategy with <code class="language-plaintext highlighter-rouge">maxUnavailable=25%</code>, which means, when Kubernetes need to update or patch(Like update the image or environment variables), the maximum number of unavailable Pods cannot exceed over <code class="language-plaintext highlighter-rouge">25%</code> as well as it ensures that at least 75% of the desired number of Pods are up (only replace 1-2 Pods if I have 5 copies at the same time):</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">2048-deployment</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">2048-game</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="s">...</span>
  <span class="s">selector</span><span class="err">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s2">"</span><span class="s">2048"</span>
<span class="err">  </span><span class="s">...</span>
  <span class="s">strategy</span><span class="err">:</span>
    <span class="na">rollingUpdate</span><span class="pi">:</span>
      <span class="na">maxSurge</span><span class="pi">:</span> <span class="s">25%</span>
      <span class="na">maxUnavailable</span><span class="pi">:</span> <span class="s">25%</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">RollingUpdate</span>
</code></pre></div></div>

<h3 id="scenario-rolling-the-new-container-image-to-existing-container-application-with-potential-service-downtime">Scenario: Rolling the new container image to existing container application with potential service downtime</h3>

<p>When rolling the new version of my container application (for example, I update my deployment by replacing the container image with the new image <code class="language-plaintext highlighter-rouge">nginx</code>), it potentially can have a period of time that can return <code class="language-plaintext highlighter-rouge">HTTP Status Code 502</code> error in my few hits:</p>

<figure class="">
  <img src="/assets/images/posts/2020/10/zero-downtime-deployment-when-using-alb-ingress-controller/http-502-error-during-deployment-using-instance-mode.png" alt="The HTTP 502 Error response from ELB (instance mode)" />
  
    <figcaption>
      The HTTP 502 Error response from ELB during the rolling update deployment (instance mode)

    </figcaption>
  
</figure>

<p>If you are specifying the controller to use <code class="language-plaintext highlighter-rouge">instance</code> mode to register targets(Pods) to your ELB Target Group, it will use worker nodes’ instance ID and expose your service in that ELB target group with Kubernetes <code class="language-plaintext highlighter-rouge">NodePort</code>. In this case, the traffic will follow the Kubernetes networking design to do second tier of transmission according to <code class="language-plaintext highlighter-rouge">externalTrafficPolicy</code> defined in the Kubernetes <code class="language-plaintext highlighter-rouge">Service</code> object (No matter using <code class="language-plaintext highlighter-rouge">externalTrafficPolicy=Cluster</code> or <code class="language-plaintext highlighter-rouge">externalTrafficPolicy=Local</code>).</p>

<p>Due to the controller only care about to register Worker Node to the ELB target group, so if the scenario doesn’t involve the worker node replacement, the case basically have miniumun even no downtime(expect that it is rare to have downtime if the Kubernetes can perfectly handle the traffic forwarding); however, this is not how real world operate, few seconds downtime still can happen potentially due to the workflow below:</p>

<p><strong>This is the general workflow when the client reach out to the service endpoint (ELB) and how was traffic goes</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Client ----&gt; ELB ----&gt; Worker Node (iptables) / In this step it might be forwarded to other Worker Node ----&gt; Pod
</code></pre></div></div>

<p>So, in these cases, you can see the downtime:</p>

<ul>
  <li>(1) The client established the connection with ELB, ELB is trying to forward the request to the backend (the Worker Node), but the Worker Node is not ready to serve the Pod.</li>
  <li>(2) Follow the iptables rules, the traffic be forwarded to the Pod just terminated due to RollingUpdate (Or the Pod just got the in-flight reqeust but immediately need to be terminated, the Pod flip to <code class="language-plaintext highlighter-rouge">Terminating</code> state. It haven’t response back yet, caused the ELB doesn’t get the response from Pod.)</li>
  <li>(3) ELB established connection with the Worker Node-1, once the packet enter into the Worker Node-1, it follows the iptables then forward it to the Pod running on Worker Node-2 (jump out the current worker node), however, the Worker Node-2 just got terminated due to auto scaling strategy or any replacement due to upgrade, caused the connection lost.</li>
</ul>

<p>Let’s say if you try to remove the encapsulation layer of the Kubernetes networking design and make thing more easier based on the AWS supported CNI Plugin (Only rely on the ELB to forward the traffic to the Pod directly by using <code class="language-plaintext highlighter-rouge">IP mode</code> with annotation setting <code class="language-plaintext highlighter-rouge">alb.ingress.kubernetes.io/target-type: ip</code> in my Ingress object), you can see the downtime more obvious when Pod doing RollingUpdate. That’s because not only the problem we mentioned the issues in case (1)/(2)/(3), but also there has different topic on the behavior of the controller need to be covered if the question comes to <strong>zero downtime deployment</strong>:</p>

<p>Here is an example by using IP mode (<code class="language-plaintext highlighter-rouge">alb.ingress.kubernetes.io/target-type: ip</code>) as <a href="https://kubernetes-sigs.github.io/aws-load-balancer-controller/guide/ingress/annotations/#target-type">resgistration type</a> to route traffic directly to the Pod IP</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">networking.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Ingress</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">game-2048</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">ingress-2048</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="na">alb.ingress.kubernetes.io/scheme</span><span class="pi">:</span> <span class="s">internet-facing</span>
    <span class="na">alb.ingress.kubernetes.io/target-type</span><span class="pi">:</span> <span class="s">ip</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">ingressClassName</span><span class="pi">:</span> <span class="s">alb</span>
  <span class="na">rules</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">http</span><span class="pi">:</span>
        <span class="na">paths</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">path</span><span class="pi">:</span> <span class="s">/</span>
          <span class="na">pathType</span><span class="pi">:</span> <span class="s">Prefix</span>
          <span class="na">backend</span><span class="pi">:</span>
            <span class="na">service</span><span class="pi">:</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">service-2048</span>
              <span class="na">port</span><span class="pi">:</span>
                <span class="na">number</span><span class="pi">:</span> <span class="m">80</span>
</code></pre></div></div>

<figure class="">
  <img src="/assets/images/posts/2020/10/zero-downtime-deployment-when-using-alb-ingress-controller/alb-target-group-with-pod-in-ip-mode.png" alt="An example when using IP mode in AWS Load BalancerController" />
  
    <figcaption>
      An example when using IP mode in AWS Load Balancer Controller - Can see my Pods all are registering with Pod owns IP address

    </figcaption>
  
</figure>

<p>Again follow the issue we mentioned (1) (2) (3), when doing the rolling update (I was replacing the image again in <code class="language-plaintext highlighter-rouge">IP mode</code>), similar problem can be observed. Potentially, you can have 10-15 seconds even longer downtime can be noticed if you are doing the same lab:</p>

<figure class="">
  <img src="/assets/images/posts/2020/10/zero-downtime-deployment-when-using-alb-ingress-controller/http-502-error-during-deployment-using-ip-mode.png" alt="The HTTP 502 Error response from ELB (IP mode)" />
  
    <figcaption>
      The HTTP 502 Error response from ELB during the rolling update deployment (IP mode)

    </figcaption>
  
</figure>

<p>When Kubernetes is rolling the deployment, in the target group, you will see AWS Load Balancer Controller was issuing old targets draining process(Old Pods) in the meantime</p>

<figure class="">
  <img src="/assets/images/posts/2020/10/zero-downtime-deployment-when-using-alb-ingress-controller/old-target-doing-draining-in-target-group-with-ip-mode.png" alt="Old targets were going to be draining state in target group" />
  
    <figcaption>
      Old targets were going to be draining state in target group

    </figcaption>
  
</figure>

<p>However, you still can see HTTP 502/504 errors exceed 3-10 seconds for a single requset</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.005413
2048
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.009980
502 Bad Gateway
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">502_TotalTime</span><span class="o">=</span>3.076954
2048
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.005700
2048
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.010019
502 Bad Gateway
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">502_TotalTime</span><span class="o">=</span>3.081601
2048
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.005527
502 Bad Gateway
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">502_TotalTime</span><span class="o">=</span>3.070947
502 Bad Gateway
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">502_TotalTime</span><span class="o">=</span>3.187812
504 Gateway Time-out
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">504_TotalTime</span><span class="o">=</span>10.006324
Welcome to nginx!
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.011838
Welcome to nginx!
</code></pre></div></div>

<h3 id="the-issue-and-the-workflow-of-the-aws-load-balancer-controller">The issue and the workflow of the AWS Load Balancer Controller</h3>

<p>Let’s use this scenario as it is a edge problem we need to consider for most use case. The issue generally is bringing out the core topic we want to address and giving a good entry point to dive deep into the workflow between the Kubernetes, AWS Load Balancer Controller and the ELB, which can lead HTTP status code 502/503(5xx) erros during deployment when having Pod termination.</p>

<p>Before diving into it, we need to know <strong>when a pod is being replaced, AWS Load Balancer Controller will register the new pod in the target group and removes the old Pods.</strong> However, at the same time:</p>

<ul>
  <li>For the the new Pods, the target is in <code class="language-plaintext highlighter-rouge">initial</code> state, until it pass the defined health check threshold (ALB health check)</li>
  <li>For the old Pods is remaining as <code class="language-plaintext highlighter-rouge">draining</code> state, until it completes draining action for the in-flight connection, or reaching out the <code class="language-plaintext highlighter-rouge">Deregistration delay</code> defined in the target group.</li>
</ul>

<p>Which result in the service to be unavailable and return HTTP 502.</p>

<p>To better understand that, I made the following diagrams. It might be helpful to you understanding the workflow:</p>

<p>1) In the diagram, I used the following IP addresses to remark and help you recognize new/old Pods. Here is the initial deployment.</p>

<ul>
  <li>Old Pods: Target-1(Private IP: 10.1.1.1), Target-2(Private IP: 10.2.2.2)</li>
  <li>New Pods: Target-3(Private IP: 10.3.3.3), Target-4(Private IP: 10.4.4.4)</li>
</ul>

<figure class="">
  <img src="/assets/images/posts/2020/10/zero-downtime-deployment-when-using-alb-ingress-controller/deployment-workflow-alb-ingress-controller-1-initial.png" alt="Deployment workflow of AWS Load Balancer Controller - 1. the initial deployment" />
  
    <figcaption>
      Deployment workflow of AWS Load Balancer Controller - 1. the initial deployment

    </figcaption>
  
</figure>

<p>2) At this stage, I was doing container image update and start rolling out the new copies of Pods. In the meantime, the controller will make <code class="language-plaintext highlighter-rouge">RegisterTarget</code> API call to ELB on behalf of the Kubernetes.</p>

<figure class="">
  <img src="/assets/images/posts/2020/10/zero-downtime-deployment-when-using-alb-ingress-controller/deployment-workflow-alb-ingress-controller-2-register-target.png" alt="Deployment workflow of AWS Load Balancer Controller - 2. start rolling out the new copies of Pods and AWS Load Balancer Controller is going to issue RegisterTarget API call" />
  
    <figcaption>
      Deployment workflow of AWS Load Balancer Controller - 2. start rolling out the new copies of Pods and AWS Load Balancer Controller is going to issue RegisterTarget API call

    </figcaption>
  
</figure>

<p>3) Meanwhile, the <code class="language-plaintext highlighter-rouge">DeregisterTarget</code> API will be called by AWS Load Balancer Controller and new targets are in <code class="language-plaintext highlighter-rouge">initial</code> state.</p>

<figure class="">
  <img src="/assets/images/posts/2020/10/zero-downtime-deployment-when-using-alb-ingress-controller/deployment-workflow-alb-ingress-controller-3-start-dereigster-old-target.png" alt="Deployment workflow of AWS Load Balancer Controller - 3. AWS Load Balancer Controller start to dereigster old targets on ELB Target Group" />
  
    <figcaption>
      Deployment workflow of AWS Load Balancer Controller - 3. AWS Load Balancer Controller start to dereigster old targets on ELB Target Group

    </figcaption>
  
</figure>

<p>4) At this stage, anything could happen to cause service outage. Because the <code class="language-plaintext highlighter-rouge">DeregisterTarget</code> API call might take some time to process, but, Kubernetes doesn’t have any design to monitor the current state of the ELB Target Group, it only care about rolling the new version of Pods and terminate old one.</p>

<p>In this case, if the Pod got terminated by Kubernetes but <code class="language-plaintext highlighter-rouge">Target-1</code> or <code class="language-plaintext highlighter-rouge">Target-2</code> are still leaving in the ELB Target Group as <code class="language-plaintext highlighter-rouge">Active</code>/<code class="language-plaintext highlighter-rouge">Healthy</code> state (It need to wait few seconds to be <code class="language-plaintext highlighter-rouge">Unhealthy</code> once it reach out to the threshold of ELB HTTP health check), result in the ELB cannot forward the front-end request to the backend correctly.</p>

<figure class="">
  <img src="/assets/images/posts/2020/10/zero-downtime-deployment-when-using-alb-ingress-controller/deployment-workflow-alb-ingress-controller-4-side-note-of-deregister.png" alt="Deployment workflow of AWS Load Balancer Controller - 4. Note: issue cause by inconsistent state between Kubernetes and ELB" />
  
    <figcaption>
      Deployment workflow of AWS Load Balancer Controller - 4. Note: issue cause by inconsistent state between Kubernetes and ELB

    </figcaption>
  
</figure>

<p>5) ELB received the <code class="language-plaintext highlighter-rouge">DeregisterTarget</code> request. So the ELB Target Group will start to perform connection draining(set old targets as <code class="language-plaintext highlighter-rouge">draining</code>), and mark the <code class="language-plaintext highlighter-rouge">Target-1</code>/<code class="language-plaintext highlighter-rouge">Target-2</code> as <code class="language-plaintext highlighter-rouge">draining</code> state, any new connection won’t be routed to these old targets.</p>

<figure class="">
  <img src="/assets/images/posts/2020/10/zero-downtime-deployment-when-using-alb-ingress-controller/deployment-workflow-alb-ingress-controller-5-elb-doing-draining.png" alt="Deployment workflow of AWS Load Balancer Controller - 5. ELB start to perform connection draining for old targets" />
  
    <figcaption>
      Deployment workflow of AWS Load Balancer Controller - 5. ELB start to perform connection draining for old targets

    </figcaption>
  
</figure>

<p>6) However, here brings another issue: if the new targets (<code class="language-plaintext highlighter-rouge">Target-3</code> and <code class="language-plaintext highlighter-rouge">Target-4</code>) are still working on passing the health check of ELB(Currently those are in <code class="language-plaintext highlighter-rouge">Initial</code> state), there has no backend can provide service at this moment, which can cause the ELB only can return HTTTP 5XX status code</p>

<figure class="">
  <img src="/assets/images/posts/2020/10/zero-downtime-deployment-when-using-alb-ingress-controller/deployment-workflow-alb-ingress-controller-6-response-5XX-error.png" alt="Deployment workflow - 6. ELB response HTTP 5XX error due to no healthy targets in can provide service" />
  
    <figcaption>
      Deployment workflow - 6. ELB response HTTP 5XX error due to no healthy targets in can provide service

    </figcaption>
  
</figure>

<p>7) Until the new Pods is in <code class="language-plaintext highlighter-rouge">Running</code> state as well as can react the health check reqeust from ELB through HTTP/HTTPS protocol, the ELB end up mark the targets as <code class="language-plaintext highlighter-rouge">Active/Healthy</code> and the service become available</p>

<figure class="">
  <img src="/assets/images/posts/2020/10/zero-downtime-deployment-when-using-alb-ingress-controller/deployment-workflow-alb-ingress-controller-7-new-pod-in-active.png" alt="Deployment workflow - 7. The service need to wait a period to recover until new targets passed the ELB health check" />
  
    <figcaption>
      Deployment workflow - 7. The service need to wait a period to recover until new targets passed the ELB health check

    </figcaption>
  
</figure>

<h2 id="how-to-resolve-the-issue-and-meet-zero-downtime">How to resolve the issue and meet zero-downtime?</h2>

<h3 id="factor-1-pod-readiness-gates">Factor-1: Pod Readiness Gates</h3>

<p>Since version <a href="https://github.com/kubernetes-sigs/aws-alb-ingress-controller/releases/tag/v1.1.6">v1.1.6</a>, AWS Load Balancer Controller (ALB Ingress Controller) introduced <a href="https://github.com/kubernetes-sigs/aws-alb-ingress-controller/blob/master/docs/guide/ingress/pod-conditions.md">Pod readiness gates</a>. This feature can monitor the rolling deployment state and trigger the deployment pause due to any unexpected issue(such as: getting timeout error for AWS APIs), which guarantees you always have Pods in the Target Group even having issue on calling ELB APIs when doing rolling update.</p>

<h4 id="alb-ingress-controller-1x-legacy">ALB Ingress Controller 1.x (Legacy)</h4>

<p>As mentioned in the previous workflow, obviously, if you would like to prevent the downtime, it is required to use several workarounds to ensure the Pod state consistency between ALB, ALB Ingress Controller and Kubernetes.</p>

<p>In the past, the readiness gate can be configured with legacy (version 1) by using the following pod spec. Here is an example to add a readiness gate with <code class="language-plaintext highlighter-rouge">conditionType: target-health.alb.ingress.k8s.aws/&lt;ingress name&gt;_&lt;service name&gt;_&lt;service port&gt;</code></p>

<p>(As it might be changed afterward, for more detail, please refer to the documentation as mentioned in the AWS Load Balancer Controller project on GitHub):</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">nginx-service</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">clusterIP</span><span class="pi">:</span> <span class="s">None</span>
  <span class="na">ports</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="m">80</span>
    <span class="na">protocol</span><span class="pi">:</span> <span class="s">TCP</span>
    <span class="na">targetPort</span><span class="pi">:</span> <span class="m">80</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">nginx</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">extensions/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Ingress</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">nginx-ingress</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="na">kubernetes.io/ingress.class</span><span class="pi">:</span> <span class="s">alb</span>
    <span class="na">alb.ingress.kubernetes.io/target-type</span><span class="pi">:</span> <span class="s">ip</span>
    <span class="na">alb.ingress.kubernetes.io/scheme</span><span class="pi">:</span> <span class="s">internal</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">rules</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">http</span><span class="pi">:</span>
        <span class="na">paths</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">backend</span><span class="pi">:</span>
              <span class="na">serviceName</span><span class="pi">:</span> <span class="s">nginx-service</span>
              <span class="na">servicePort</span><span class="pi">:</span> <span class="m">80</span>
            <span class="na">path</span><span class="pi">:</span> <span class="s">/*</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">nginx-deployment</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">nginx</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">2</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">nginx</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">readinessGates</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">conditionType</span><span class="pi">:</span> <span class="s">target-health.alb.ingress.k8s.aws/nginx-ingress_nginx-service_80</span>
      <span class="na">containers</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">nginx</span>
        <span class="na">image</span><span class="pi">:</span> <span class="s">nginx</span>
        <span class="na">ports</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">80</span>
</code></pre></div></div>

<h4 id="aws-load-balancer-controller-after-v2x">AWS Load Balancer Controller (After v2.x)</h4>

<p><strong>For now, if you are using controller later than v2, the readiness gate configuration can be automatically injected to the pod spec</strong> by defining the label <code class="language-plaintext highlighter-rouge">elbv2.k8s.aws/pod-readiness-gate-inject: enabled</code> to your Kubernetes namespace.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl create namespace readiness
namespace/readiness created

<span class="nv">$ </span>kubectl label namespace readiness elbv2.k8s.aws/pod-readiness-gate-inject<span class="o">=</span>enabled
namespace/readiness labeled

<span class="nv">$ </span>kubectl describe namespace readiness
Name:         readiness
Labels:       elbv2.k8s.aws/pod-readiness-gate-inject<span class="o">=</span>enabled
Annotations:  &lt;none&gt;
Status:       Active
</code></pre></div></div>

<p>So defining legacy fields <code class="language-plaintext highlighter-rouge">readinessGates</code> and <code class="language-plaintext highlighter-rouge">conditionType</code> are not required if you are using controller later than <code class="language-plaintext highlighter-rouge">v2.0</code>. <strong>If you have a pod spec with legacy readiness gate configuration, ensure you label the namespace and create the Service/Ingress objects before applying the pod/deployment manifest. The controller will remove all legacy readiness-gate configuration and add new ones during pod creation.</strong></p>

<h3 id="factor-2-graceful-shutdown-your-applications">Factor-2: Graceful shutdown your applications</h3>

<p>For existing connections(As mentioned in the workflow-4), the case is involving the gracefully shutdown/termination handling in Kubernetes. Therefore, it is requires to use the method provided by Kubernetes.</p>

<p>You can use <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination">Pod Lifecycle</a> with <code class="language-plaintext highlighter-rouge">preStop</code> hook and make some pause(like using <code class="language-plaintext highlighter-rouge">sleep</code> command) for Pod termination. This trick ensures ALB can have some time to completely remove old targets on Target Group (It is recommended to adjust longer based on your <code class="language-plaintext highlighter-rouge">Deregistration delay</code>):</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="na">lifecycle</span><span class="pi">:</span>
      <span class="na">preStop</span><span class="pi">:</span>
        <span class="na">exec</span><span class="pi">:</span>
          <span class="na">command</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">/bin/sh"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">-c"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">sleep</span><span class="nv"> </span><span class="s">40"</span><span class="pi">]</span>
<span class="err">  </span><span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">70</span>
</code></pre></div></div>

<blockquote>
  <p>Note: If a container has a preStop hook configured, that runs before the container enters the Terminated state. Also, if the preStop hook needs longer to complete than the default grace period allows, you must modify <code class="language-plaintext highlighter-rouge">terminationGracePeriodSeconds</code> to suit this.</p>
</blockquote>

<h3 id="an-example-to-achieve-zero-downtime-when-doing-rolling-update-after-applying-methods-above">An example to achieve zero downtime when doing rolling update after applying methods above</h3>

<p>First apply the label to the namespace so the controller can automatically inject the readiness gate:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Namespace</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">2048-game</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">elbv2.k8s.aws/pod-readiness-gate-inject</span><span class="pi">:</span> <span class="s">enabled</span>
</code></pre></div></div>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s2">"</span><span class="s">2048-deployment"</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s2">"</span><span class="s">2048-game"</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s2">"</span><span class="s">2048"</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">5</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s2">"</span><span class="s">2048"</span>
    <span class="na">spec</span><span class="pi">:</span>

      <span class="c1"># This would be optional if you are using controller after v2.x</span>
      <span class="na">readinessGates</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">conditionType</span><span class="pi">:</span> <span class="s">target-health.alb.ingress.k8s.aws/2048-ingress_service-2048_80</span>

      <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">70</span>
      <span class="na">containers</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">image</span><span class="pi">:</span> <span class="s">alexwhen/docker-2048</span>
        <span class="na">imagePullPolicy</span><span class="pi">:</span> <span class="s">Always</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s2">"</span><span class="s">2048"</span>
        <span class="na">ports</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">80</span>
        <span class="na">lifecycle</span><span class="pi">:</span>
          <span class="na">preStop</span><span class="pi">:</span>
            <span class="na">exec</span><span class="pi">:</span>
              <span class="na">command</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">/bin/sh"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">-c"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">sleep</span><span class="nv"> </span><span class="s">40"</span><span class="pi">]</span>
</code></pre></div></div>

<p>Here is an example after following the practice I was getting a try. The deployment will apply the feature and can see the status of the readiness gates:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl get pods <span class="nt">-n</span> 2048-game <span class="nt">-o</span> wide
NAME                              READY   STATUS    RESTARTS   AGE   IP               NODE                                              NOMINATED NODE   READINESS GATES
2048-deployment-99b6fb474-c97ht   1/1     Running   0          78s   192.168.14.209   XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.compute.internal   &lt;none&gt;           1/1
2048-deployment-99b6fb474-dcxfs   1/1     Running   0          78s   192.168.31.47    XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.compute.internal   &lt;none&gt;           1/1
2048-deployment-99b6fb474-kvhhh   1/1     Running   0          54s   192.168.29.6     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.compute.internal   &lt;none&gt;           1/1
2048-deployment-99b6fb474-vhjbg   1/1     Running   0          54s   192.168.18.161   XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.compute.internal   &lt;none&gt;           1/1
2048-deployment-99b6fb474-xfd5q   1/1     Running   0          78s   192.168.16.183   XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.compute.internal   &lt;none&gt;           1/1
</code></pre></div></div>

<p>Once rolling the new version of the container image, the deployment goes smoothly and prevent the downtime issue as mentioned in previous paragraphs:</p>

<figure class="">
  <img src="/assets/images/posts/2020/10/zero-downtime-deployment-when-using-alb-ingress-controller/zero-downtime-deployment-with-alb-demo.png" alt="Zero downtime with AWS Load Balancer Controller - Can see the targets are gracefully replaced when the Kubernetes is doing rolling update" />
  
    <figcaption>
      Zero downtime with AWS Load Balancer Controller - Can see the targets are gracefully replaced when the Kubernetes is doing rolling update

    </figcaption>
  
</figure>

<p>In my scenario, the Kubernetes need to take at least 40 seconds termination period for single Pod, so the old targets are gradually moved out instead of remove all of them at once within few seconds, until entire target group only exists new targets.</p>

<p>Therefore, you probably also need to notice the <code class="language-plaintext highlighter-rouge">Deregistration delay</code> defined in your ELB Target Group, which can be updated through <a href="https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/guide/ingress/annotations/#target-group-attributes">the annotation</a>:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">alb.ingress.kubernetes.io/target-group-attributes</span><span class="pi">:</span> <span class="s">deregistration_delay.timeout_seconds=30</span>
</code></pre></div></div>

<p>In this case, it is recommended to be less than 40 seconds so ELB can drain your old targets before the Pod completely shutdown.</p>

<p>With the configuration, client can get normal responses from old Pods/existing connection during the deployment:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.012028
2048
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.005383
2048
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.010174
2048
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.012233
Welcome to nginx!
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.007116
2048
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.010090
2048
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.012201
2048
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.005532
2048
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.010107
2048
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.012163
Welcome to nginx!
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.005452
Welcome to nginx!
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.009950
2048
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.012082
Welcome to nginx!
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.005349
2048
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.010142
2048
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.012143
2048
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.005507

...
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.012149
Welcome to nginx!
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.005364
Welcome to nginx!
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.010021
Welcome to nginx!
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.012092
Welcome to nginx!
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.005463
Welcome to nginx!
<span class="nv">HTTPCode</span><span class="o">=</span><span class="nv">200_TotalTime</span><span class="o">=</span>0.010136
Welcome to nginx!
</code></pre></div></div>

<p>This is the practice in case having AWS Load Balancer Controller for doing graceful deployment with <code class="language-plaintext highlighter-rouge">RollingUpdate</code>. However, it is another big topic need to be discussed regarding what type of the application when rolling the update. Because other type of applications need to establish long connection with the ELB or have requirement for considering persistence data need to be stored on the backend. All these things can bring out other issues we need to talk about.</p>

<p>But in summarize, with the deployment strategy above, it is also recommended to design the client/backend application as stateless, implement retry and fault-tolerance. These mothod usually help to reduce the customer complain and provide better user experience for most common use case.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Due to the current design of Kubernetes, it is involving the state inconsistent issue when you are exposing the service with Application Load Balancer. Therefore, in this article, I mentioned the potential issue when doing rolling update in the scenario having container service integrating with the AWS Load Balancer Controller (ALB Ingress Controller).</p>

<p>Even the technology is always in revolution, I am still willing to help people better handle the deployment strategy. I used a couple of hours to draft this content and tried to cover several major issues, metioned things you might need to aware, break down the entire workflow and shared few practical suggestions that can be achieved by using AWS Load Balancer Controller in order to meet the goal when doing zero downtime deployment.</p>

<p>The article was written based on my own experience (Of course many communications back and forth with different customers using AWS), it might not be perfect but I hope it is helpful to you. For sure, if you find any typo or have any suggestions, please feel free and leave comment below.</p>

<h3 id="references">References</h3>

<ul>
  <li><a href="https://docs.aws.amazon.com/eks/latest/userguide/alb-ingress.html">ALB Ingress Controller on Amazon EKS</a></li>
  <li><a href="https://github.com/kubernetes-sigs/aws-alb-ingress-controller/blob/master/docs/guide/ingress/pod-conditions.md">Using pod conditions / pod readiness gates</a></li>
  <li><a href="https://github.com/kubernetes-sigs/aws-alb-ingress-controller/issues/1124">Issue#1124</a></li>
  <li><a href="https://github.com/kubernetes-sigs/aws-alb-ingress-controller/issues/814">Issue#814</a></li>
  <li><a href="https://github.com/kubernetes-sigs/aws-alb-ingress-controller/issues/1064">Issue#1064</a></li>
</ul>]]></content><author><name>Yang-Xin Cao (Eason Cao)</name></author><category term="aws" /><category term="amazon web services" /><category term="EC2" /><category term="Elastic Compute Cloud" /><category term="amazon" /><category term="ELB" /><category term="ALB" /><category term="Load Balancer" /><category term="Elastic Load Balancer" /><category term="ALB Ingress Controller" /><category term="Kubernetes" /><category term="k8s" /><category term="EKS" /><category term="Elastic Kubernetes Service" /><category term="AWS Load Balancer Controller" /><summary type="html"><![CDATA[This article is describing the thing you need to aware when using ALB Ingress Controller (AWS Load Balancer Controller) to do deployment and prevent 502 errors]]></summary></entry><entry><title type="html">在 AWS 上打造 Serverless (無伺服器) 雲端綠界支付金流 (ECPay)</title><link href="https://easoncao.com/serverless-ecpay-aws/" rel="alternate" type="text/html" title="在 AWS 上打造 Serverless (無伺服器) 雲端綠界支付金流 (ECPay)" /><published>2022-05-27T00:00:00-05:00</published><updated>2022-05-27T00:00:00-05:00</updated><id>https://easoncao.com/serverless-ecpay-aws</id><content type="html" xml:base="https://easoncao.com/serverless-ecpay-aws/"><![CDATA[<p>金流系統一直是許多線上服務、電商所必備的功能之一；常見的使用方式往往是透過第三方金流服務提供商對應的模組實現 (例如：OpenCart, WooCommerce)，以串接金流系統實現購物結帳的機制。</p>

<p>即使是設計自己的金流系統串接，想要將這些金流服務串接應用在雲端服務上，對於不熟悉使用雲端技術的用戶來說，仍然需要花一些時間摸索以達到這項目的。</p>

<p>為了實現在 AWS 上串接綠界金流 (ECPay) 提供信用卡付款機制，並且簡化管理和維護流程，以下內容以 Serverless 技術作為背景，簡介在 AWS 上實作的相關細節。</p>

<h2 id="什麼是-serverless為什麼要用-serverless-技術">什麼是 Serverless？為什麼要用 Serverless 技術？</h2>

<p>無伺服器運算 (Serverless) 概念提出了拋棄舊有傳統管理 Server ，在過去，你需要維護及管理運行你應用程式的基礎運算系統；Serverless 提出以平台即服務（PaaS）的運作模式，提供簡單且容易操作的微型的架構，使得你不需要部署、組態或管理伺服器，只需要運用 Serverless 相關的解決方案，將你的程式碼推送至相關平台，運行所需要的伺服器服務皆由雲端平台來提供。</p>

<p>自 2014 年 AWS 推出 Serverless 服務以來，已經儼然成為一項 IT 部署解決方案中熱門的運行架構；學會使用 Serverles，將幫助你更容易且快速地推行不同類型的應用，將你的想法付諸於實際實現。</p>

<p>在過去，如果要運作相關的金流服務，我可能會需要開啟一台虛擬機器 24 小時的提供商業邏輯的運作，並且，可能會因為一些非預期狀況而多許多而外的工作，例如：突然暴增的訂單請求、過高的使用負載等原因影響到業務。往往程式開發出來只是個起點，後面的系統維護工作才是更大的挑戰。</p>

<p>選擇使用 Serverless 架構設計的考量之一，便是考量部署、組態或管理伺服器的長遠維護性，尤其對於金流這種關鍵業務來說，更是至關重要。</p>

<h2 id="怎麼在-aws-上與綠界支付科技串接">怎麼在 AWS 上與綠界支付科技串接</h2>

<p>一般來說，在 AWS 上要建立 Serverless 為基礎架構的應用程式，通常涉及幾種不同的關鍵服務；以這項支付金流系統為例，我採用了以下的 AWS 服務</p>

<h3 id="aws-lambda">AWS Lambda</h3>

<p>是一種無伺服器的運算服務，可讓您執行程式但不必佈建或管理伺服器、建立工作負載感知叢集擴展邏輯、維護事件整合或管理執行階段。使用 Lambda，您可以透過虛擬方式執行任何類型的應用程式或後端服務，全部無需管理。在這篇內容中，我使用了 Lambda Function 以推送訊息至 Amazon SNS 以發佈檔案更新。 Amazon CloudWatch</p>

<h3 id="api-gateway">API Gateway</h3>

<p>為 AWS 提供的託管服務，可以讓開發人員輕鬆地建立、發佈、維護、監控和保護任何規模的 API。API 可作為應用程式的「前門」，以便從後端服務存取資料、商業邏輯或功能。使用 API Gateway 時，您可以建立 RESTful API 等應用程式。API Gateway 支援無伺服器工作負載和 Web 應用程式。API Gateway 可以用以負責處理有關接受和處理多達數十萬個並行 API 呼叫的所有工作，包括流量管理、CORS 支援、授權和存取控制。API Gateway 沒有最低費用或啟動成本。您要為收到的 API 呼叫和資料傳輸量支付費用。</p>

<h3 id="serverless-application-model-sam">Serverless Application Model (SAM)</h3>

<p>為了更容易實現在 AWS 上運作串接綠界金流支付並且以 Serverless 架構運作的目標，我使用了 AWS <a href="https://aws.amazon.com/serverless/sam/">Serverless Application Model</a> (簡稱為 SAM) 為開發流程的重要工具，用以建置 Serverless 應用服務。</p>

<p>Serverless Application Model (SAM) 提供了一系列以簡易描述的方法，提供你用以更容易，在很多情況下，你可能無需非常熟悉不同 AWS 服務的設置，即可透過 SAM 建立無伺服器應用程式。</p>

<p>為了幫助你快速了解 Serverless Application Model (SAM) 的運作機制以及簡介，以下簡短 10 分鐘的影片分享了其運作流程的機制：</p>

<!-- Courtesy of embedresponsively.com //-->

<div class="responsive-video-container">
    <iframe src="https://www.youtube-nocookie.com/embed/HYtyzT-4Mqc" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen=""></iframe>
  </div>

<h3 id="運作流程與架構概覽-architecture-overview">運作流程與架構概覽 (Architecture Overview)</h3>

<figure class="">
  <img src="/assets/images/posts/2022/05/serverless-ecpay-aws/create-payment-flow.png" alt="以綠界支付科技為基礎的 Serverless 雲端金流概覽 (建立訂單)" />
  
    <figcaption>
      以綠界支付科技為基礎的 Serverless 雲端金流概覽 (建立訂單)

    </figcaption>
  
</figure>

<p>上述的使用者流程描述了用戶及各個彼此 AWS 服務之間的運作關係，以建立訂單為例，我們可以藉由綠界支付 (ECPay) 開放的對應 SDK 實際在 AWS 中設計屬於其結帳流程的相關操作，透過 API Gateway 建立一致的對外 API 接口，並且實作建立訂單訊息的 Python 應用程式，並且將其透過 Serverless Application Model 提供的 CLI 工具 (SAM CLI) 將應用部署至 AWS Lambda 上運作。</p>

<p>在這種運作架構下，我們只需要專注設計結帳流程和用戶流程的設計，其餘的服務運作機制，均可以交付由 AWS Serverless 相關的解決方案滿足業務實作需求。</p>

<p>如果你對於具體的實作內容有興趣，可以透過下列的連結獲取更多資訊：</p>

<div class="notice--success">
<h3 id="學習在一天內於-aws-雲端搭建-serverless-架構的金流服務">學習在一天內於 AWS 雲端搭建 Serverless 架構的金流服務</h3>

<p>從 Zero 到 Hero，學習 AWS 入門知識並深入了解、應用 Serverless 相關的服務及架構，同時學會在 AWS 上使用不同的解決方案實踐無伺服器技術 (Serverless)，運行屬於你的雲端金流系統</p>

<p><a href="https://bit.ly/ECPaySAM?utm_source=easoncao.com&amp;utm_medium=courses-page&amp;utm_campaign=course_sale" class="btn btn--inverse btn--x-large">點擊獲取更多資訊</a>
<a href="/courses" class="btn btn--x-large">延伸學習</a></p>
</div>

<h2 id="總結與延伸學習">總結與延伸學習</h2>

<p>本篇內容簡介了以 AWS Serverless 為基礎架構設計金流應用程式的實作流程，以及提及部分在 AWS 上實現串接綠界支付科技的對應機制，並且分享一項參考架構。如果你對於本篇具體的實作內容有興趣，可以利用以下連結獲取完整的內容：</p>

<ul>
  <li><a href="https://bit.ly/ECPaySAM?utm_source=easoncao.com&amp;utm_medium=courses-page&amp;utm_campaign=course_sale">打造 Serverless (無伺服器) 雲端金流</a></li>
</ul>

<p>如果你覺得這樣的內容有幫助，可以在底下按個 Like / 留言讓我知道。</p>]]></content><author><name>Yang-Xin Cao (Eason Cao)</name></author><category term="aws" /><category term="amazon web services" /><category term="Lambda" /><category term="Lambda Function" /><category term="Serverless" /><category term="Serverless Application Model" /><category term="SAM" /><summary type="html"><![CDATA[金流系統一直是許多線上服務、電商所必備的功能之一；常見的使用方式往往是透過第三方金流服務提供商對應的模組實現 (例如：OpenCart, WooCommerce)，以串接金流系統實現購物結帳的機制。為了實現在 AWS 上串接綠界金流 (ECPay) 提供信用卡付款機制，並且簡化管理和維護流程，以下內容以 Serverless 技術作為背景，簡介在 AWS 上實作的相關細節。]]></summary></entry><entry><title type="html">[AWS][EKS] Best practice for load balancing - 3. what controller should I use</title><link href="https://easoncao.com/eks-best-practice-load-balancing-3-en/" rel="alternate" type="text/html" title="[AWS][EKS] Best practice for load balancing - 3. what controller should I use" /><published>2022-04-08T00:00:00-05:00</published><updated>2022-04-08T00:00:00-05:00</updated><id>https://easoncao.com/eks-best-practice-load-balancing-3-en</id><content type="html" xml:base="https://easoncao.com/eks-best-practice-load-balancing-3-en/"><![CDATA[<p>This article is sharing the best practice for doing load balancing on Amazon EKS, learn what is advantage and disadvantage of using different controller. We will discuss what controller should you use.</p>

<h2 id="compare-different-controller-options">Compare different controller options</h2>

<p>Here are some common load balancing solutions that can be applied on Amazon EKS:</p>

<h3 id="kubernetes-in-tree-load-balancer-controller">Kubernetes in-tree load balancer controller</h3>

<p>This is the easiest way to provision your Elastic Load Balancer resource, which could be done by using default Kubernetes service deployment with <code class="language-plaintext highlighter-rouge">type: LoadBalancer</code>. In most case, the in-tree controller can quickly spin up the load balancer for experiment purpose; or, offers production workload.</p>

<p>However, you need to aware the problem as we mentioned in the previous posts <sup id="fnref:eks-best-practice-load-balancing-1" role="doc-noteref"><a href="#fn:eks-best-practice-load-balancing-1" class="footnote" rel="footnote">1</a></sup> <sup id="fnref:eks-best-practice-load-balancing-2" role="doc-noteref"><a href="#fn:eks-best-practice-load-balancing-2" class="footnote" rel="footnote">2</a></sup> because it generally can add a hop for your load balancing behavior on AWS and also can increase the complexity for your traffic.</p>

<p>In addition, you need to aware this method only applies for creating Classic Load Balancer and Network Load Balancer (by using annotation <sup id="fnref:in-tree-aws-nlb-support" role="doc-noteref"><a href="#fn:in-tree-aws-nlb-support" class="footnote" rel="footnote">3</a></sup>).</p>

<h3 id="nginx-ingress-controller">nginx ingress controller</h3>

<p>If you are using nginx Ingress controller in AWS, it will deploy Network load balancer (NLB) to expose the NGINX Ingress controller behind a Service of <code class="language-plaintext highlighter-rouge">type=LoadBalancer</code>. Here is an example for deploying Kubernetes service of nginx Ingress controller 1.1.3:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="na">service.beta.kubernetes.io/aws-load-balancer-backend-protocol</span><span class="pi">:</span> <span class="s">tcp</span>
    <span class="na">service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled</span><span class="pi">:</span> <span class="s2">"</span><span class="s">true"</span>
    <span class="na">service.beta.kubernetes.io/aws-load-balancer-type</span><span class="pi">:</span> <span class="s">nlb</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">app.kubernetes.io/component</span><span class="pi">:</span> <span class="s">controller</span>
    <span class="na">app.kubernetes.io/instance</span><span class="pi">:</span> <span class="s">ingress-nginx</span>
    <span class="na">app.kubernetes.io/name</span><span class="pi">:</span> <span class="s">ingress-nginx</span>
    <span class="na">app.kubernetes.io/part-of</span><span class="pi">:</span> <span class="s">ingress-nginx</span>
    <span class="na">app.kubernetes.io/version</span><span class="pi">:</span> <span class="s">1.1.3</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">ingress-nginx-controller</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">ingress-nginx</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">externalTrafficPolicy</span><span class="pi">:</span> <span class="s">Local</span>
  <span class="na">ports</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">appProtocol</span><span class="pi">:</span> <span class="s">http</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">http</span>
    <span class="na">port</span><span class="pi">:</span> <span class="m">80</span>
    <span class="na">protocol</span><span class="pi">:</span> <span class="s">TCP</span>
    <span class="na">targetPort</span><span class="pi">:</span> <span class="s">http</span>
  <span class="pi">-</span> <span class="na">appProtocol</span><span class="pi">:</span> <span class="s">https</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">https</span>
    <span class="na">port</span><span class="pi">:</span> <span class="m">443</span>
    <span class="na">protocol</span><span class="pi">:</span> <span class="s">TCP</span>
    <span class="na">targetPort</span><span class="pi">:</span> <span class="s">https</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">app.kubernetes.io/component</span><span class="pi">:</span> <span class="s">controller</span>
    <span class="na">app.kubernetes.io/instance</span><span class="pi">:</span> <span class="s">ingress-nginx</span>
    <span class="na">app.kubernetes.io/name</span><span class="pi">:</span> <span class="s">ingress-nginx</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">LoadBalancer</span>
</code></pre></div></div>

<p>Guess what, yes, it is still can rely on the in-tree controller. On the other hand, the problem we were mentioning can persist. It can be hard to expect which Pods will receive the traffic; however, the main issue is that an Ingress controller does not typically eliminate the need for an external load balancer, it simply adds an additional layer of routing and control behind the load balancer.</p>

<figure class="">
  <img src="/assets/images/posts/2022/04/eks-best-practice-load-balancing/nginx-ingress-architecture.png" alt="An architecture overview of using nginx Ingress controller" />
  
    <figcaption>
      Figure 1. An architecture overview of using nginx Ingress controller

    </figcaption>
  
</figure>

<p>So why to choose Nginx Ingress controller? It probably can be the reason why as mentioned in the post <sup id="fnref:nlb-nginx-ingress-aws-blog" role="doc-noteref"><a href="#fn:nlb-nginx-ingress-aws-blog" class="footnote" rel="footnote">4</a></sup> as mentioned on the AWS Blog:</p>

<ul>
  <li>By default, the NGINX Ingress controller will listen to all the ingress events from all the namespaces and add corresponding directives and rules into the NGINX configuration file. This makes it possible to use a centralized routing file which includes all the ingress rules, hosts, and paths.</li>
  <li>With the NGINX Ingress controller you can also have multiple ingress objects for multiple environments or namespaces with the same network load balancer.</li>
</ul>

<h3 id="aws-load-balancer-controller">AWS Load Balancer Controller</h3>

<p>AWS Load Balancer Controller is similar to the in-tree Kubernetes controller and use native AWS APIs to provision and manage Elastic Load Balancers. The controller was an open-source project originally named <strong>ALB Ingress Controller</strong> because it was only provides capability to manage Application Load Balancer at the intial stage, lately, it officially renamed as <strong>AWS Load Balancer Controller</strong> <sup id="fnref:intro-aws-load-balancer-controller" role="doc-noteref"><a href="#fn:intro-aws-load-balancer-controller" class="footnote" rel="footnote">5</a></sup>, which is maintaining by AWS product team and open-source community.</p>

<p>Unlike in-tree Kubernetes controller needs to wait the upstream code to be updated, which requires you to upgrade Kubernetes control plane version if the controller has any bug or any new ELB features need to be supported. Using AWS Load Balancer Controller, it can gracefully be replaced because it will be running as Kubernetes deployment instead of relying on Kubernetes upstream source code integration.</p>

<p>The controller directly maintain your Elastic Load Balancer resources with up-to-date annotations. For nginx ingress controller, it can provision and add an extra load balancing layer with the Network Load Balancer, in this case, the traffic generally will pass through the controller itself (nginx-ingress); instead, for AWS Load Balancer Controller, it doesn’t play as a gateway. The AWS Load Balancer Controller will directly control the Elastic Load Balancer resource, which can register your Pod (by using IP mode) so the request can directly forward to your backend application.</p>

<p>The AWS Load Balancer Controller also starts to support TargetGroupBinding <sup id="fnref:aws-lb-controller-targetgroupbinding" role="doc-noteref"><a href="#fn:aws-lb-controller-targetgroupbinding" class="footnote" rel="footnote">6</a></sup> and IngressGroup <sup id="fnref:aws-lb-controller-ingressgroup" role="doc-noteref"><a href="#fn:aws-lb-controller-ingressgroup" class="footnote" rel="footnote">7</a></sup> feature since v2.2. It enables you can group multiple Ingress resources together, which allows multiple service deployments can share the same Elastic Load Balancer resource.</p>

<h2 id="conclusion-what-controller-should-i-use">Conclusion: What controller should I use?</h2>

<p>After comparing different load balancer controllers, generally speaking, using <strong>AWS Load Balancer</strong> basically can have better feature supports as well as adopt with the performance optimization by configuring AWS Load Balancer attributes correctly. It is essential to enable <strong>IP mode</strong> when applying the Kubernetes service deployment with AWS Load Balancer Controller to reduce unnecessary hop that can be caused by Kubernetes networking itself, which is generally not totally suitable for AWS networking and elastic load balancing feature.</p>

<p>However, the disadvantage of using AWS Load Balancer can be all features require to be supported by Elastic Load Balancer itself because the controller doesn’t involve additional functions to extend the traffic control. Using other controller still can have its benefit and provide different features that Elastic Load Balancer doesn’t have, such as using nginx Ingress controller you may be able to define forward service to external FastCGI targets, using Regular Expression to perform path matching … etc.</p>

<p>By the end of this article, I hope the comparison and information can better help you understand how to select load balancer controller that will be running in Amazon EKS, and choose the right option for your environment.</p>

<p>Thanks for reading! If you have any feedback or opinions, please feel free to leave the comment below.</p>

<ul>
  <li><a href="/eks-best-practice-load-balancing-1-en">Best practice for load balancing - 1. Let’s start with an example from Kubernetes document</a></li>
  <li><a href="/eks-best-practice-load-balancing-2-en">Best practice for load balancing - 2. imbalanced problem</a></li>
  <li><a href="/eks-best-practice-load-balancing-3-en">Best practice for load balancing - 3. what controller should I use</a></li>
</ul>

<h2 id="references">References</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:eks-best-practice-load-balancing-1" role="doc-endnote">
      <p><a href="/eks-best-practice-load-balancing-1-en">[AWS][EKS] Best pratice load balancing - Let’s start with an example from Kubernetes document</a> <a href="#fnref:eks-best-practice-load-balancing-1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:eks-best-practice-load-balancing-2" role="doc-endnote">
      <p><a href="/eks-best-practice-load-balancing-2-en">[AWS][EKS] Best pratice load balancing - imbalanced problem</a> <a href="#fnref:eks-best-practice-load-balancing-2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:in-tree-aws-nlb-support" role="doc-endnote">
      <p>in-tree controller - <a href="https://kubernetes.io/docs/concepts/services-networking/service/#aws-nlb-support">Network Load Balancer support on AWS</a> <a href="#fnref:in-tree-aws-nlb-support" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:nlb-nginx-ingress-aws-blog" role="doc-endnote">
      <p><a href="https://aws.amazon.com/blogs/opensource/network-load-balancer-nginx-ingress-controller-eks/">Using a Network Load Balancer with the NGINX Ingress Controller on Amazon EKS</a> <a href="#fnref:nlb-nginx-ingress-aws-blog" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:intro-aws-load-balancer-controller" role="doc-endnote">
      <p><a href="https://aws.amazon.com/blogs/containers/introducing-aws-load-balancer-controller/">Introducing the AWS Load Balancer Controller</a> <a href="#fnref:intro-aws-load-balancer-controller" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:aws-lb-controller-targetgroupbinding" role="doc-endnote">
      <p>AWS Load Balancer controller v2.2 - <a href="https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.2/guide/targetgroupbinding/targetgroupbinding/">TargetGroupBinding</a> <a href="#fnref:aws-lb-controller-targetgroupbinding" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:aws-lb-controller-ingressgroup" role="doc-endnote">
      <p>AWS Load Balancer controller v2.2 - <a href="https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.2/guide/ingress/annotations/#group.name">IngressGroup</a> <a href="#fnref:aws-lb-controller-ingressgroup" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Yang-Xin Cao (Eason Cao)</name></author><category term="aws" /><category term="amazon web services" /><category term="EC2" /><category term="Elastic Compute Cloud" /><category term="amazon" /><category term="ELB" /><category term="ALB" /><category term="Load Balancer" /><category term="Elastic Load Balancer" /><category term="ALB Ingress Controller" /><category term="Kubernetes" /><category term="k8s" /><category term="EKS" /><category term="Elastic Kubernetes Service" /><category term="AWS Load Balancer Controller" /><summary type="html"><![CDATA[This article is sharing the best practice for doing load balancing on Amazon EKS, learn what is advantage and disadvantage of using different controller. We will discuss what controller should you use.]]></summary></entry><entry><title type="html">[AWS][EKS] Best practice for load balancing - 2. imbalanced problem</title><link href="https://easoncao.com/eks-best-practice-load-balancing-2-en/" rel="alternate" type="text/html" title="[AWS][EKS] Best practice for load balancing - 2. imbalanced problem" /><published>2022-04-08T00:00:00-05:00</published><updated>2022-04-08T00:00:00-05:00</updated><id>https://easoncao.com/eks-best-practice-load-balancing-2-en</id><content type="html" xml:base="https://easoncao.com/eks-best-practice-load-balancing-2-en/"><![CDATA[<p>This article is sharing the best practice for doing load balancing on Amazon EKS, learn what is advantage and disadvantage of using different controller. We will discuss more detail about the imbalanced problem after applying controller to deploy the Elastic Load Balancer.</p>

<h2 id="the-load-imbalanced-problem">The load imbalanced problem</h2>

<p>Follow the example as mentioned in the previous article, if you deployed a Kubernetes service and noticed the utilization on your backend application is not balanced; or, if you are using AWS Load Balancer controller, Traefik, nginx-ingress controller by finding the Elastic Load Balancer wasn’t correctly separate the loads (when using instance mode to register your Pods as targets), and you may find the imbalanced traffic, that’s the major topic in this article would like to talk about: discuss how to improve and optimize it.</p>

<h3 id="problem-description">Problem description</h3>

<p>Let’s say if I am deploying 4 Pods in my Kubernetes cluster, which is using the default deployment as mentioned below to expose my Kubernetes service:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl get pod <span class="nt">-o</span> wide
NAME                                READY   STATUS    RESTARTS   AGE   IP
nginx-deployment-594764c789-5s668   1/1     Running   0          30m   192.168.42.171
nginx-deployment-594764c789-9k949   1/1     Running   0          30m   192.168.39.194
nginx-deployment-594764c789-b292m   1/1     Running   0          33m   192.168.29.24 
nginx-deployment-594764c789-s226c   1/1     Running   0          30m   192.168.15.158
</code></pre></div></div>

<p>The Kubernetes service:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">nginx-svc</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">nginx</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">LoadBalancer</span>
  <span class="na">ports</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="m">80</span>
    <span class="na">protocol</span><span class="pi">:</span> <span class="s">TCP</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">nginx</span>
</code></pre></div></div>

<p>To better understand the problem I am describing in this post, the application I deployed will response Pod IP address to let us know which one received the request:</p>

<figure class="">
  <img src="/assets/images/posts/2022/04/eks-best-practice-load-balancing/imbalanced-test-result.png" alt="Testing the service and see the response from the backend." />
  
    <figcaption>
      Figure 1. Testing the service and see the response from the backend.

    </figcaption>
  
</figure>

<p>After running a loop and making at least <strong>79 HTTP requests</strong> in my test, I get the following response to know how the load has been distributed:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">192.168.42.171</code>: 12 times</li>
  <li><code class="language-plaintext highlighter-rouge">192.168.39.194</code>: 33 times</li>
  <li><code class="language-plaintext highlighter-rouge">192.168.29.24</code>: 23 times</li>
  <li><code class="language-plaintext highlighter-rouge">192.168.15.158</code>: 10 times</li>
</ul>

<p>According to the testing, we can see the load is not very evenly distributed.</p>

<h3 id="why-this-could-happen">Why this could happen?</h3>

<p>As mentioned in the <a href="/eks-best-practice-load-balancing-1-en">previous post</a>, whether you are defining <code class="language-plaintext highlighter-rouge">externalTrafficPolicy=Cluster</code> or <code class="language-plaintext highlighter-rouge">externalTrafficPolicy=Local</code>, the routing behavior is relying on <code class="language-plaintext highlighter-rouge">iptables</code>(or <code class="language-plaintext highlighter-rouge">ipvs</code>) can be unpredictable. Because it is doing second layer of load balancing, which is totally unnecessary for increasing a hop in AWS VPC.</p>

<p>Elastic Load Balancer in AWS already provides a straightforward solution to balance your loads, and its algorithm will try to distribute the requests to all backend servers as even as possible. Doing load balancing in Kubernetes network generally is increasing the complexity of your architecture, and make traffic can be hard to trace; or, even worse, cause the imbalanced issue as you can observe.</p>

<p>This also makes the load balancing became unpredictable. Although the traffic send to the registered EC2 instance can be evenly distributed; however, it doesn’t mean the load can be separated to Pods as well. You will never know which Pods will be routed due to this load balancing layer implemented by Kubernetes networking.</p>

<p>No matter choose Traefik, nginx-ingress, if you are still following the default load balancing pattern offered by upstream Kubernetes code, then you can expect the traffic can come with load imbalanced.</p>

<h2 id="how-to-optimize-the-load-balancing">How to optimize the load balancing?</h2>

<p>The major problem is the default load balancing behavior can involve the Kubernetes load balancing and add a hop for the traffic. So you may start to wondering how to better resolve this problem; however, there is no specific feature can be adjusted on Kubernetes to remove the default load balancing, but it still could be possible to skip the Kubernetes load balancing and forward the traffic to Pods directly.</p>

<p>If you are running Pods on Amazon EKS and using default AWS VPC CNI Plugin<sup id="fnref:aws-vpc-cni-plugin" role="doc-noteref"><a href="#fn:aws-vpc-cni-plugin" class="footnote" rel="footnote">1</a></sup>, you can expect your Pods should have dedicated secondary private IP address that can be communicated within your AWS VPC network; therefore, it also means that the IP address can be registered to your Elastic Load Balancer as backend target. The flow can be:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Client -&gt; NLB (forawrd request to IP target) -&gt; Pod IPs (Reach out to Pods directly)
</code></pre></div></div>

<p>For Application Load Balancer (ALB) and Network Load Balancer (NLB), both provide a feature that you can register backend targets with IP addresses (<a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-target-groups.html#target-type">NLB</a>, <a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#target-type">ALB</a>. Note: Classic Load Balancer doesn’t offer this option). We can simply to associate these Pod IP addresses as backend targets instead of using instances. As long as the Pod IP addresses are reachable, it can move the request be forwarded to the backend Pods by skipping the Kubernetes load balancing behavior.</p>

<h3 id="using-ip-mode">Using IP mode</h3>

<p>So how to register Pod IP addresses in Elastic Load Balancer? A seamlessly way is to deploy your Kubernetes service and use AWS Load Balancer Controller<sup id="fnref:aws-load-balancer-controller" role="doc-noteref"><a href="#fn:aws-load-balancer-controller" class="footnote" rel="footnote">2</a></sup> to enable this feature. Instead of using the default Kubernetes controller to deploy your Elastic Load Balancer, using AWS Load Balancer Controller helps you manage load balancer resource including all functionality features and different type of load balancer such as NLB, ALB, both are can be supported by the controller. After installing the AWS Load Balancer on your EKS cluster, you can enable the IP registration type for your Pods by simply adding annotations to the deployment manifests.</p>

<h4 id="network-load-balancer-nlb">Network Load Balancer (NLB)</h4>

<p>Here is a deployment sample that use IP targets with pods deployed to Amazon EC2 nodes. Your Kubernetes service must be created as type <code class="language-plaintext highlighter-rouge">LoadBalancer</code>:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">my-service</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="na">service.beta.kubernetes.io/aws-load-balancer-type</span><span class="pi">:</span> <span class="s2">"</span><span class="s">external"</span>
    <span class="na">service.beta.kubernetes.io/aws-load-balancer-nlb-target-type</span><span class="pi">:</span> <span class="s2">"</span><span class="s">ip"</span>
  <span class="s">...</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">LoadBalancer</span>
  <span class="s">...</span>
</code></pre></div></div>

<h4 id="application-load-balancer-alb">Application Load Balancer (ALB)</h4>

<p>To deploy application load balancer on Amazon EKS through the AWS Load Balancer Controller, you generally will create an Ingress object in your deployment. With the AWS Load Balancer Controller, it also provides supported annotation that can register pods as targets for the ALB. Traffic reaching the ALB is directly routed to pods for your service. Here is an example:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">networking.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Ingress</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">game-2048</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">ingress-2048</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="na">alb.ingress.kubernetes.io/scheme</span><span class="pi">:</span> <span class="s">internet-facing</span>
    <span class="na">alb.ingress.kubernetes.io/target-type</span><span class="pi">:</span> <span class="s">ip</span>
    <span class="s">...</span>
</code></pre></div></div>

<p>In the AWS EKS documentation, it also mentioned detailed guide regarding how to deploy these two load balancers and share an example by using IP target to register your Pods. If you are interested to learn more, please check out to the following documents to get more detail:</p>

<ul>
  <li><a href="https://docs.aws.amazon.com/eks/latest/userguide/network-load-balancing.html">Network load balancing on Amazon EKS</a></li>
  <li><a href="https://docs.aws.amazon.com/eks/latest/userguide/alb-ingress.html">Application load balancing on Amazon EKS</a></li>
</ul>

<p>By using IP mode, it totally removes the layer of load balancing manipulated by Kubernetes. This generally forward requests to the Pods without doing second forwarding:</p>

<figure class="">
  <img src="/assets/images/posts/2022/04/eks-best-practice-load-balancing/ip-target.png" alt="Register Pods with IP mode" />
  
    <figcaption>
      Figure 2. Register Pods with IP mode

    </figcaption>
  
</figure>

<h2 id="are-you-sure-it-is-balanced-lets-have-a-test">Are you sure it is balanced? Let’s have a test!</h2>

<p>This time I used the same testing strategy as mentioned in the first problem description section and ran four Pods associated with Network Load Balancer using IP mode, which is showing below:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl get pods <span class="nt">-o</span> wide
NAME                                READY   STATUS    RESTARTS   AGE     IP               NODE                                              NOMINATED NODE   READINESS GATES
nginx-deployment-75d48f6698-b5fm7   1/1     Running   0          35m     192.168.17.15    ip-192-168-5-38.ap-northeast-1.compute.internal   &lt;none&gt;           &lt;none&gt;
nginx-deployment-75d48f6698-l4gw5   1/1     Running   0          2m45s   192.168.27.143   ip-192-168-5-38.ap-northeast-1.compute.internal   &lt;none&gt;           &lt;none&gt;
nginx-deployment-75d48f6698-q2q57   1/1     Running   0          41m     192.168.22.126   ip-192-168-5-38.ap-northeast-1.compute.internal   &lt;none&gt;           &lt;none&gt;
nginx-deployment-75d48f6698-x5m25   1/1     Running   0          2m45s   192.168.14.48    ip-192-168-5-38.ap-northeast-1.compute.internal   &lt;none&gt;           &lt;none&gt;
</code></pre></div></div>

<p>After passing at least 50 requests, I can see the request distributions are showing below:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">192.168.17.15</code>: 10 times</li>
  <li><code class="language-plaintext highlighter-rouge">192.168.27.143</code>: 12 times</li>
  <li><code class="language-plaintext highlighter-rouge">192.168.22.126</code>: 14 times</li>
  <li><code class="language-plaintext highlighter-rouge">192.168.14.48</code>: 13 times</li>
</ul>

<p>For each target, it nearly have ~25% chances will be routed evenly by the Network Load Balancer. Because it skip the load balancing layer of the Kubernetes, it will follow the routing algorithm<sup id="fnref:routing-algorithm" role="doc-noteref"><a href="#fn:routing-algorithm" class="footnote" rel="footnote">3</a></sup> and separate load evenly as we expected.</p>

<h3 id="i-tried-to-use-ip-mode-but-the-traffic-still-get-imbalanced">I tried to use IP mode but the traffic still get imbalanced</h3>

<p>In my testing, I was running a couple of Pods with nginx image and provided simple web server in my backend. The scenario in this article mentioning generally is describing all targets were using stateless HTTP connections. However, in some cases, it could be possible ELB might unequally route traffic to your targets if:</p>

<ul>
  <li>Clients are routing requests to an incorrect IP address of a load balancer node with a DNS record that has an expired TTL.</li>
  <li>Sticky sessions (session affinity) are enabled for the load balancer. Sticky sessions use cookies to help the client maintain a connection to the same instance over a cookie’s lifetime, which can cause imbalances over time.</li>
  <li>Available healthy instances aren’t evenly distributed across Availability Zones.</li>
  <li>Instances of a specific capacity type aren’t equally distributed across Availability Zones.</li>
  <li>There are long-lived TCP connections between clients and instances.</li>
  <li>The connection uses a WebSocket.</li>
</ul>

<p>Generally speaking, if the client or any configuration can cause sticky session, it still have possibility can get the traffic imbalanced. The detail can refer to the following article on AWS knowledge center:</p>

<ul>
  <li><a href="https://aws.amazon.com/premiumsupport/knowledge-center/elb-fix-unequal-traffic-routing/">Why is Elastic Load Balancing unequally routing my load balancer traffic?</a></li>
</ul>

<p>But overall, using the IP mode to register our Pods, literally can resolve the problem as we described due to the design of Kubernetes service networking.</p>

<h2 id="a-summary-if-you-would-like-to-optimize-the-traffic-imbalanced-when-using-aws-load-balancer-controller">A summary if you would like to optimize the traffic imbalanced when using AWS Load Balancer Controller</h2>

<h3 id="using-ip-mode-as-register-target-to-prevent-kubernetes-additional-hop">Using IP mode as register target to prevent Kubernetes additional hop</h3>

<p>Although Elastic Load Balancer can offer an option to register your targets by instances, however, it generally would be suitable when you are running single service and expose it with a port on a dedicated EC2 instance. With Kubernetes service running on your EC2 instance but exposed as <code class="language-plaintext highlighter-rouge">NodePort</code> service, it can involve multiple Pods behind the service port offered on your instance due to the service load balancing. The packet can be replaced to other destination field of your Pod’s private IP address when the packet flood into the instance through Linux ipvs or iptables rules.</p>

<p>If the service work load is relying on Kubernetes deployment, it is recommended  such as <code class="language-plaintext highlighter-rouge">service.beta.kubernetes.io/aws-load-balancer-nlb-target-type</code> for NLB, <code class="language-plaintext highlighter-rouge">alb.ingress.kubernetes.io/target-type</code> for ALB.</p>

<h3 id="prevent-to-enable-sticky-session-on-elb">Prevent to enable sticky session on ELB</h3>

<p>It is also important to make sure the Elastic Load Balancer won’t stick your client session to specific target<sup id="fnref:aws-elb-sticky-clb" role="doc-noteref"><a href="#fn:aws-elb-sticky-clb" class="footnote" rel="footnote">4</a></sup> <sup id="fnref:aws-elb-sticky-alb" role="doc-noteref"><a href="#fn:aws-elb-sticky-alb" class="footnote" rel="footnote">5</a></sup>. Although Elastic Load Balancer provides cookie-based stickiness session to bind a user’s session to a specific target, which can be achieved by configuring the load balancer attribute and also supported by AWS Load Balancer Controller as below, but to optimize the traffic imbalanced, it is recommended to avoid use the sticky session as it can potentially cause the phenomena.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># ALB</span>
<span class="na">alb.ingress.kubernetes.io/target-group-attributes</span><span class="pi">:</span> <span class="s">stickiness.enabled=true,stickiness.lb_cookie.duration_seconds=60</span>

<span class="c1"># NLB</span>
<span class="na">service.beta.kubernetes.io/aws-load-balancer-target-group-attributes</span><span class="pi">:</span> <span class="s">stickiness.enabled=true,stickiness.type=source_ip</span>
</code></pre></div></div>

<h3 id="equally-distribute-pods-across-availability-zones">Equally distribute Pods across Availability Zones</h3>

<p>As ELB requires to strike the balance between your Availability Zones to ensure the service high availability. This helps your traffic can correctly be separated on all backend target.</p>

<h2 id="summary">Summary</h2>

<p>In this article, it explains the practice of optimizing the load balancing and mitigate the imbalanced traffic problem when deploying service with Kubernetes. This article also brings you an overview and learn what other scenarios that you can potentially find out ELB might unequally route traffic to your backend targets.</p>

<p>In the next article we will review a couple of Kubernetes load balancer controllers that can be deployed on Amazon EKS and see what option can be the best practice for your environment.</p>

<ul>
  <li><a href="/eks-best-practice-load-balancing-1-en">Best practice for load balancing - 1. Let’s start with an example from Kubernetes document</a></li>
  <li><a href="/eks-best-practice-load-balancing-2-en">Best practice for load balancing - 2. imbalanced problem</a></li>
  <li><a href="/eks-best-practice-load-balancing-3-en">Best practice for load balancing - 3. what controller should I use</a></li>
</ul>

<h2 id="references">References</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:aws-vpc-cni-plugin" role="doc-endnote">
      <p><a href="https://docs.aws.amazon.com/eks/latest/userguide/pod-networking.html">AWS VPC CNI Plugin</a> <a href="#fnref:aws-vpc-cni-plugin" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:aws-load-balancer-controller" role="doc-endnote">
      <p><a href="https://docs.aws.amazon.com/eks/latest/userguide/aws-load-balancer-controller.html">AWS Load Balancer Controller</a> <a href="#fnref:aws-load-balancer-controller" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:routing-algorithm" role="doc-endnote">
      <p>How Elastic Load Balancing works - <a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html#routing-algorithm">Routing algorithm</a> <a href="#fnref:routing-algorithm" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:aws-elb-sticky-clb" role="doc-endnote">
      <p><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-sticky-sessions.html">Configure sticky sessions for your Classic Load Balancer</a> <a href="#fnref:aws-elb-sticky-clb" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:aws-elb-sticky-alb" role="doc-endnote">
      <p><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/sticky-sessions.html">Sticky sessions for your Application Load Balancer</a> <a href="#fnref:aws-elb-sticky-alb" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Yang-Xin Cao (Eason Cao)</name></author><category term="aws" /><category term="amazon web services" /><category term="EC2" /><category term="Elastic Compute Cloud" /><category term="amazon" /><category term="ELB" /><category term="ALB" /><category term="Load Balancer" /><category term="Elastic Load Balancer" /><category term="ALB Ingress Controller" /><category term="Kubernetes" /><category term="k8s" /><category term="EKS" /><category term="Elastic Kubernetes Service" /><category term="AWS Load Balancer Controller" /><summary type="html"><![CDATA[This article is sharing the best practice for doing load balancing on Amazon EKS, learn what is advantage and disadvantage of using different controller. We will discuss more detail about the imbalanced problem after applying controller to deploy the Elastic Load Balancer.]]></summary></entry><entry><title type="html">[AWS][EKS] Best practice for load balancing - 1. Let’s start with an example from Kubernetes document</title><link href="https://easoncao.com/eks-best-practice-load-balancing-1-en/" rel="alternate" type="text/html" title="[AWS][EKS] Best practice for load balancing - 1. Let’s start with an example from Kubernetes document" /><published>2022-04-08T00:00:00-05:00</published><updated>2022-04-08T00:00:00-05:00</updated><id>https://easoncao.com/eks-best-practice-load-balancing-1-en</id><content type="html" xml:base="https://easoncao.com/eks-best-practice-load-balancing-1-en/"><![CDATA[<p>This article is sharing the best practice for doing load balancing on Amazon EKS, learn what is advantage and disadvantage of using different controller. We will discuss more detail about what is the problem of using default Kubernetes service deployment as mentioned on official document.</p>

<h2 id="understand-the-default-kubernetes-load-balancing">Understand the default Kubernetes load balancing</h2>

<h3 id="an-overview-of-externaltrafficpolicy">An overview of <code class="language-plaintext highlighter-rouge">externalTrafficPolicy</code></h3>

<p>I have many occurrences to see Kubernetes administrators are not very familiar with the Kubernetes network flow, and feel struggling about that when they need to diagnose networking issue, especially for users using managed Kubernetes cluster service. But I think that’s normal to see this gap because it reflects Kubernetes is doing encapsulation perfectly, causes you are unable to easily troubleshoot any real-world failures unless you had deeply understand its design.</p>

<p>Before walking through the detail about the load balancing, it is required to understand the fundamental knowledge of Kubernetes load balancing and its effect when defining your YAML files.</p>

<p>In the Kubernetes, it provides <a href="https://kubernetes.io/docs/concepts/services-networking/service/#external-traffic-policy">External traffic policy</a>, so you can set this field (<code class="language-plaintext highlighter-rouge">spec.externalTrafficPolicy</code>) in your Kubernetes service deployment to control the flow, and decide how to route the traffic from external. Kubernetes offers two options for this policy: <code class="language-plaintext highlighter-rouge">Cluster</code> and <code class="language-plaintext highlighter-rouge">Local</code>, let’s have a deep overview to see how it works:</p>

<figure class="">
  <img src="/assets/images/posts/2022/04/eks-best-practice-load-balancing/externaltrafficpolicy.png" alt="The overview of ExternalTrafficPolicy" />
  
    <figcaption>
      Figure 1: externalTrafficPolicy

    </figcaption>
  
</figure>

<p>By default, the <code class="language-plaintext highlighter-rouge">kube-proxy</code> is performing this layer of load balancing by using <code class="language-plaintext highlighter-rouge">iptables</code>. Based on the Pods you are running, it will create rules in  your iptables and uses random mode (<code class="language-plaintext highlighter-rouge">--mode random</code>) to perform the load balancing based on the probability. For example, if you have 3 Pods need to be distributed, <code class="language-plaintext highlighter-rouge">kube-proxy</code> will take the responsibility to add required iptables rules with defined probability, and try to balance the load:</p>

<figure class="">
  <img src="/assets/images/posts/2022/04/eks-best-practice-load-balancing/services-iptables-overview.svg" alt="Kubernetes service overview" />
  
    <figcaption>
      Figure 2: Kubernetes service overview (<a href="https://kubernetes.io/docs/concepts/services-networking/service/#proxy-mode-iptables">source</a>)

    </figcaption>
  
</figure>

<p>I am not going to drill down into too much detail as it can increase the complexity of this article, however, if you are interested to learn how this translation happens, you can review the iptables rules on your host to see what’s going on.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># An example of iptables rules
-A KUBE-SVC-XXXXX -m comment --comment "default/app" -m statistic --mode random --probability 0.20000000019 -j KUBE-SEP-AAAAAA
-A KUBE-SVC-XXXXX -m comment --comment "default/app" -m statistic --mode random --probability 0.25000000000 -j KUBE-SEP-BBBBBB
-A KUBE-SVC-XXXXX -m comment --comment "default/app" -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-CCCCCC
-A KUBE-SVC-XXXXX -m comment --comment "default/app" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-DDDDDD
-A KUBE-SVC-XXXXX -m comment --comment "default/app" -j KUBE-SEP-EEEEEE
</code></pre></div></div>

<p>As mentioned in <em>Figure 1</em>, when <code class="language-plaintext highlighter-rouge">externalTrafficPolicy=Cluster</code>, it can have a scenario will route the traffic to other Nodes if you deploy Pod(s) on them. By relying on the iptables rules, this policy can accomplish the load balancing by redirecting them to other Nodes. In theory, this can bring the traffic jump out of the original Node.</p>

<p>When <code class="language-plaintext highlighter-rouge">externalTrafficPolicy=Local</code>, it limits the traffic only can be redirected on the same Node; however, the behavior of doing load balancing through the iptables still happens. If you have multiple Pods running on the single Node, the traffic can be routed to one of them.</p>

<h3 id="deep-dive-into-the-load-balancing-behavior---an-example-from-k8s-document">Deep dive into the load balancing behavior - An example from K8s document</h3>

<p>Let’s see an example mentioned at official Kubernetes document<sup id="fnref:k8s-service-lb" role="doc-noteref"><a href="#fn:k8s-service-lb" class="footnote" rel="footnote">1</a></sup>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>apiVersion: v1
kind: Service
metadata:
  name: nginx-svc
  labels:
    app: nginx
spec:
  type: LoadBalancer
  ports:
  - port: 80
    protocol: TCP
  selector:
    app: nginx
</code></pre></div></div>

<p>If you use AWS as cloud provider and deploy the service, it generally will create an Elastic Load Balancer (Classic Load Balancer) and provide the traffic load balancing. The Elastic Load Balancer will be managed by the <strong>in-tree load balancer controller</strong><sup id="fnref:k8s-elb-source-code" role="doc-noteref"><a href="#fn:k8s-elb-source-code" class="footnote" rel="footnote">2</a></sup>, which is implemented in Kubernetes source code; hence, you can simply provision the Elastic Load Balancer on AWS seamlessly.</p>

<p>Looks familiar, right? The example above is quite common if you find tutorial on somewhere. Maybe that is exactly same configuration running in your production environment.</p>

<p>But here is the problem: by default, Kubernetes implements another layer of load balancing, which is backed with <code class="language-plaintext highlighter-rouge">kube-proxy</code>. Let’s say if you have two worker nodes (<code class="language-plaintext highlighter-rouge">Node-1</code> and <code class="language-plaintext highlighter-rouge">Node-2</code>), and each node have Pods running on it (<code class="language-plaintext highlighter-rouge">Pod-1</code>, <code class="language-plaintext highlighter-rouge">Pod-2</code> on <code class="language-plaintext highlighter-rouge">Node-1</code>; <code class="language-plaintext highlighter-rouge">Pod-3</code> on <code class="language-plaintext highlighter-rouge">Node-2</code>), using default option (<code class="language-plaintext highlighter-rouge">externalTrafficPolicy=Cluster</code>). On AWS, the traffic flow generally is representing as below:</p>

<h4 id="case-1">Case 1</h4>

<p>The default Kubernetes service will expose your application with a specific service port to provide external accessibility (<code class="language-plaintext highlighter-rouge">NodePort</code>), and establish relevant iptables rules to perform NAT traslation by replacing the IP address of the destination field.</p>

<p>With this design, this can be a happy case if <code class="language-plaintext highlighter-rouge">kube-proxy</code> doesn’t redirect the request to other host, which can be outlined as:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>client -&gt; Load Balancer -&gt; Node-1 (NodePort) -&gt; iptables rules -&gt; Pod-1 on Node-1
</code></pre></div></div>

<h4 id="case-2">Case 2</h4>

<p>However, what if the iptables forward the traffic to other Nodes?</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>client -&gt; Load Balancer -&gt; Node-1 (NodePort) -&gt; iptables rules -&gt; Pod-3 on Node-2 
</code></pre></div></div>

<p>On the other hand, if you deploy a Kubernetes service like this, the traffic flow can be routed as two particular phenomena:</p>

<figure class="">
  <img src="/assets/images/posts/2022/04/eks-best-practice-load-balancing/instance-target.png" alt="The traffic flow when working with externalTrafficPolicy" />
  
    <figcaption>
      Figure 3: The traffic flow when working with externalTrafficPolicy

    </figcaption>
  
</figure>

<p>As you can see, no matter what it is, the behavior seems like doesn’t provide a better route because it definitely increases the number of hops for the traffic flow.</p>

<p>What about <code class="language-plaintext highlighter-rouge">externalTrafficPolicy: Local</code>? Does it work better?</p>

<p>Follow the example as mentioned in the previous paragraph, let’s say if you have two Pods (<code class="language-plaintext highlighter-rouge">Pod-1</code> and <code class="language-plaintext highlighter-rouge">Pod-2</code>) running on the same Node (<code class="language-plaintext highlighter-rouge">Node-1</code>). The traffic flow of this policy generally can be breaking down as below:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>client -&gt; Load Balancer -&gt; Node-1 (NodePort) -&gt; iptables rules -&gt; Node-1 (Target Pod-1)
client -&gt; Load Balancer -&gt; Node-1 (NodePort) -&gt; iptables rules -&gt; Node-1 (Target Pod-2)
</code></pre></div></div>

<p>When load balancer move the request to the backend (<code class="language-plaintext highlighter-rouge">Node-1</code>), the probability to forward the request by iptables rules to the <code class="language-plaintext highlighter-rouge">Pod-1</code> and <code class="language-plaintext highlighter-rouge">Pod-2</code>, <strong>is 50% chances</strong>.</p>

<p>On the other hand, the traffic firstly pass through the Elastic Load Balancer, and do the routing again in the system level (iptables), which means the architecture will perform the load balancing <strong>twice</strong>.</p>

<p>With no doubt, it did not offer the best path for the traffic routing.</p>

<h3 id="the-reason-why-you-will-see-your-targets-are-failing-the-health-check-even-pods-are-running">The reason why you will see your targets are failing the health check even Pods are running</h3>

<p>If <code class="language-plaintext highlighter-rouge">externalTrafficPolicy=Local</code> and you have multiple Nodes running behind your Elastic Load Balancer, you probably will see some Nodes will fail the health check, <strong>which can be expected</strong>.</p>

<p>That’s because if some Node doesn’t run the service’s backend Pods so it cannot pass the health check.</p>

<figure class="">
  <img src="/assets/images/posts/2022/04/eks-best-practice-load-balancing/externalTrafficPolicy-local.png" alt="Some Node doesn't pass health check due to externalTrafficPolicy" />
  
    <figcaption>
      Figure 4: Some Node doesn’t pass health check due to externalTrafficPolicy

    </figcaption>
  
</figure>

<p>In general, it doesn’t impact anything because the ELB will ensure only healthy targets can be routed; however, in this case, it doesn’t perfectly distribute the load with Elastic Load Balancer and offer high availabilty when we have multiple Pods. If the Node down, it can impact all Pods running on it.</p>

<h3 id="whats-wrong-with-externaltrafficpolicy">What’s wrong with externalTrafficPolicy?</h3>

<blockquote>
  <p>So, looks like using <code class="language-plaintext highlighter-rouge">externalTrafficPolicy=Cluster</code> is a good option?</p>
</blockquote>

<p>Imagine you have a long running connection is jumping out of the first Node, unfortunately, the first Node is having issue such as hardware failure, intermittent connectivity problem … etc. In the end, it is going to be down. In this case, if any existing connections forwarded from other Nodes, the connections will be impacted and cannot response back to the origin correctly. In general, the Node down can cause the packet loss because the connection route is established in the middle:</p>

<p><em>(If you have established connection passed the Node, here is an example of the breaking route situation if the <code class="language-plaintext highlighter-rouge">Node-1</code> in the middle is down.)</em></p>
<blockquote>
  <p>client -&gt; Load Balancer -&gt; <del>Node-1 (NodePort)</del> -&gt; iptables rules -&gt; Target Pod-2 on Node-2</p>
</blockquote>

<p>If you reviewed the flow of <em>Figure 3</em>, connections can be routed to different paths and it can be hard to predict once you deployed many Pods. It also increase the complexity if you would like to trace the networking flow during the problem diagnostic.</p>

<p>When having a large scale scenario (e.g. deploy 100, 500 even 10,000 Pods), this also can potentially bring the system level issue, or, result in the packet loss, such as network latency increased due to kernel needs to compare several iptables rules when a new connection comes in; or, reach out to the kernel limits for the networking stack, because Linux kernel needs to track of them when working with iptables, and insert the rules on the system level. One common issue is to fill out the connection tracking table (conntrack) of the Linux kernel when the scale grows.</p>

<h2 id="summary">Summary</h2>

<p>In this article, it explains the behavior of load balancing on Kubernetes. This article also brings you an overview and learn what issue can occur if you follow the default Kubernetes example to deploy your Elastic Load Balancer.</p>

<p>Now we have deep-level understanding of the Kubernetes load balancing, let’s start with more discussion regarding the load imbalancing problem with the current architecture on Amazon EKS in the next article.</p>

<ul>
  <li><a href="/eks-best-practice-load-balancing-1-en">Best practice for load balancing - 1. Let’s start with an example from Kubernetes document</a></li>
  <li><a href="/eks-best-practice-load-balancing-2-en">Best practice for load balancing - 2. imbalanced problem</a></li>
  <li><a href="/eks-best-practice-load-balancing-3-en">Best practice for load balancing - 3. what controller should I use</a></li>
</ul>

<h2 id="references">References</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:k8s-service-lb" role="doc-endnote">
      <p>Kubernetes service - <a href="https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer">Type LoadBalancer</a> <a href="#fnref:k8s-service-lb" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:k8s-elb-source-code" role="doc-endnote">
      <p>Kubernetes source code - <a href="https://github.com/kubernetes/kubernetes/blob/64c5ed380441b4015301cf64a44a698400b03d8b/staging/src/k8s.io/legacy-cloud-providers/aws/aws_loadbalancer.go#L969">aws_loadbalancer.go</a> <a href="#fnref:k8s-elb-source-code" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Yang-Xin Cao (Eason Cao)</name></author><category term="aws" /><category term="amazon web services" /><category term="EC2" /><category term="Elastic Compute Cloud" /><category term="amazon" /><category term="ELB" /><category term="ALB" /><category term="Load Balancer" /><category term="Elastic Load Balancer" /><category term="ALB Ingress Controller" /><category term="Kubernetes" /><category term="k8s" /><category term="EKS" /><category term="Elastic Kubernetes Service" /><category term="AWS Load Balancer Controller" /><summary type="html"><![CDATA[This article is sharing the best practice for doing load balancing on Amazon EKS, learn what is advantage and disadvantage of using different controller. We will discuss more detail about what is the problem of using default Kubernetes service deployment as mentioned on official document.]]></summary></entry><entry><title type="html">NEX WORK - 獲得國際大公司的內推機會、串連世界各地菁英：如果流浪是為了找回家的路，我們有責任把回家的路變得更美好</title><link href="https://easoncao.com/nex-work/" rel="alternate" type="text/html" title="NEX WORK - 獲得國際大公司的內推機會、串連世界各地菁英：如果流浪是為了找回家的路，我們有責任把回家的路變得更美好" /><published>2022-03-12T00:00:00-06:00</published><updated>2022-03-12T00:00:00-06:00</updated><id>https://easoncao.com/nex-work</id><content type="html" xml:base="https://easoncao.com/nex-work/"><![CDATA[<p>我們知道獲得國際大公司的內推機會對於台灣人十分困難，為了串連海外台灣人才菁英並且開啟另類海外求職內推的機會，我將在這篇內容中與你分享你可能會感興趣的資源：<a href="https://work.nexf.org/">NEX WORK</a> — 一個非營利線上求職內推平台。</p>

<figure class="">
  <img src="/assets/images/posts/2022/03/nex-work/cover.jpeg" alt="NEX Foundation" />
  
    <figcaption>
      (<a href="https://www.nexf.org/get-involved/">source</a>)

    </figcaption>
  
</figure>

<p>NEX WORK 由一群 NEX Foundation (台灣未來基金會) 台灣熱血的工程師建置，目的在於打破對於海外求職的高門檻和增加被看見的機會，以串連在世界各地的海外菁英，建立永續的機制，並促進正向的人才循環。</p>

<h2 id="nex-foundation-台灣未來基金會是什麼">NEX Foundation 台灣未來基金會是什麼？</h2>

<p><img src="/assets/images/posts/2022/03/nex-work/NEX-logo-color.png" alt="" /></p>

<p>NEX Foundation 台灣未來基金會成立於 2018 年，為美國聯邦政府核准的 501(c)(3) <strong>非營利慈善新創機構</strong>。NEX 以美國西雅圖及台灣台北為據點，希望透過研發和經營線上的資源平台，協助海外人才在國際舞台上的職涯發展。並更進一步扮演橋樑的角色，連結活躍於世界各地的台灣人才，共同推動企業媒合、職涯諮詢、媒體實驗、社群聚會等計畫，期盼建立具有延續性的全球台灣人才互助圈。</p>

<blockquote>
  <p>NEX 的成立背景，來自團隊成員們的海外故事。一腳踏出熟悉的家鄉來到文化迥然的異地，學習在高度的競爭環境中生存，在面臨新的挑戰時保持堅強。然而，對於缺乏資源和當地人脈的海外遊子來說，眼前往往有許多的不容易和不安全感。</p>
</blockquote>

<p>為此，我們希望透過 NEX 的運作，以互助的力量來提拔下一位追夢者，為需要幫助的人才們，創造更多的機會和協助更多夢想的實現。</p>

<p>基金會最初是由陳浩維 (HW. Chen) 和一群在美國工作的熱血朋友們，開始投入「NEX Foundation 台灣未來基金會」的籌備工作。(<a href="https://www.nexf.org/team/">認識在世界各地的努力貢獻的團隊專家及志工</a>)</p>

<p>並且，基金會於 2018 年 12 月獲得美國國稅局（IRS）批准，正式成為美國聯邦層級的非營利教育慈善機構。</p>

<figure class="">
  <img src="/assets/images/posts/2022/03/nex-work/nex-mission.png" alt="NEX Foundation - Why NEX" />
  
    <figcaption>
      NEX Foundation - Why NEX (<a href="https://www.nexf.org/get-involved/">source</a>)

    </figcaption>
  
</figure>

<p>NEX 的首要任務為建立一個信任平台來連結全球的台灣人才，系統性整合現有資源，推動相關協助計畫（如：公司內推整合、人才輔 導計畫、獎學金計畫等），期待去翻轉台灣「人才外移」的負面印象，並希望作為一個正向動力去團結大家的力量和資源，去幫助更多夢想和成就的實現。</p>

<p>目前 NEX Work 仍在持續投入許多非營利專案、包含定期舉辦社群活動和分享，為更多台灣人開啟海外求職的契機。你也可以關注在以下連結獲得更多資訊：</p>

<ul>
  <li>Website: <a href="https://nexf.org/">https://nexf.org/</a></li>
  <li>LinkedIn: <a href="https://www.linkedin.com/company/nexfoundation/">https://www.linkedin.com/company/nexfoundation/</a></li>
  <li>Facebook: <a href="https://www.facebook.com/NEXFoundation/">https://www.facebook.com/NEXFoundation/</a></li>
  <li>NEX Community: <a href="https://www.facebook.com/groups/nexfoundation">https://www.facebook.com/groups/nexfoundation</a>
    <ul>
      <li>(Facebook 社團 - 不定期發布社群講座和活動資訊)</li>
    </ul>
  </li>
  <li>NEX Media Lab: <a href="https://media.nexf.org/">https://media.nexf.org/</a></li>
</ul>

<h2 id="什麼是-nex-work為什麼會有-nex-work">什麼是 NEX WORK？為什麼會有 NEX WORK？</h2>

<p><img src="/assets/images/posts/2022/03/nex-work/nex-work-logo.png" alt="" /></p>

<p><a href="https://work.nexf.org/">NEX WORK</a> 是一個非營利線上求職內推平台，目前仍在 Beta 階段，目的在於連結世界各地菁英，創造團結互助的力量。</p>

<p>尤其在日趨競爭的就業環境、對外國人不友善的簽證流程、其它族裔互助合作壯大自我（甚至遊走法律邊緣）等等現實情況下，我們認為幫助自己人為理所當然，且勢在必行。<sup id="fnref:oversea-ptt" role="doc-noteref"><a href="#fn:oversea-ptt" class="footnote" rel="footnote">1</a></sup></p>

<p>因此，作為一個推進的力量，支持和協助台灣人才的職涯發展，團隊成員在工作之餘努力的推動 NEX WORK 專案，且不斷的仍在持續收集用戶回饋和優化。</p>

<p><strong>透過 NEX 設計的內推系統，讓台灣人陪著台灣人在國際職涯路上打開第一扇窗或衝刺最後一哩路。NEX WORK 快速整合求職供需鏈，讓你在同公司內找到那一把手，爭取時間就是爭取機會。</strong></p>

<h3 id="誰在使用-nex-work">誰在使用 NEX WORK?</h3>

<p>NEX WORK 作為初始試驗平台，目前已經累積許多在各個世界知名公司的台灣人自發性的在上面提供內推的管道 (包含我自己)。</p>

<p>目前平台上除了有知名的科技公司的台灣海外菁英自發性提供內推渠道 (例如：<strong>Facebook(Meta), Amazon, Apple, Google, Dropbox, Cisco</strong>)，也包含其他知名會計、加密貨幣交易所等。如果你有興趣提供相關的機會，也可以參考以下資訊透過註冊系統開啟這項渠道。</p>

<h2 id="如何使用-nex-work">如何使用 NEX WORK</h2>

<p>第一步可以透過以下連結訪問 NEX WORK 平台：</p>

<ul>
  <li>NEX WORK: <a href="https://work.nexf.org/">https://work.nexf.org/</a></li>
</ul>

<p>界面十分直覺 (如果覺得很難用請不吝透過右側的 Feedback 表單與我們分享使用者反饋)，下捲即可以看到目前有哪些公司，以及該公司存在的推薦人數。</p>

<figure class="">
  <img src="/assets/images/posts/2022/03/nex-work/nex-work-ui.png" alt="NEX WORK 的介面" />
  
    <figcaption>
      NEX WORK 的介面

    </figcaption>
  
</figure>

<p>為了快速幫助你了解平台的相關特性，以下分為兩種使用情境分別提供更多細節：</p>

<h3 id="a-若你要尋求內推機會">A. 若你要尋求內推機會</h3>

<p>你可以透過右上角的註冊按鈕 (<a href="https://work.nexf.org/signup">或是點擊這裡註冊</a>) 填寫基本資料註冊帳戶並且完成信箱驗證。</p>

<p>完成註冊後，你可以選擇要尋求內推的公司，以下以 Amazon 為例：</p>

<figure class="">
  <img src="/assets/images/posts/2022/03/nex-work/nex-work-usage-1.png" alt="選擇尋求內推機會的公司" />
  
    <figcaption>
      選擇尋求內推機會的公司

    </figcaption>
  
</figure>

<p>點擊後你可以查看有關提供內推渠道的推薦人以及查看更多資訊。</p>

<p>為了避免尋找不是活躍的推薦人讓推薦請求石沉大海，你也可以透過系統的幾項關鍵指標，例如：</p>

<ul>
  <li>推薦了幾名申請人</li>
  <li>最近內推時間</li>
</ul>

<figure class="">
  <img src="/assets/images/posts/2022/03/nex-work/nex-work-usage-2.png" alt="選擇內推的推薦人" />
  
    <figcaption>
      選擇內推的推薦人

    </figcaption>
  
</figure>

<p>點擊「幫我內推」即可以填寫必要基本資料及上傳履歷資訊：</p>

<figure class="">
  <img src="/assets/images/posts/2022/03/nex-work/nex-work-usage-3.png" alt="填寫必要基本資料和上傳履歷、職位細節" />
  
    <figcaption>
      填寫必要基本資料和上傳履歷、職位細節

    </figcaption>
  
</figure>

<p>完成申請後，點擊 「檢視我的內推紀錄」即可查看你的內推資訊：</p>

<figure class="">
  <img src="/assets/images/posts/2022/03/nex-work/nex-work-usage-4.png" alt="檢視自己的內推紀錄" />
  
    <figcaption>
      檢視自己的內推紀錄

    </figcaption>
  
</figure>

<p><strong>請注意：NEX WORK 並不保證推薦人一定會幫助你內推，內推的推薦人仍然會針對你的經歷及你提供的各種資料來決定是否內推，並非來者不拒的盲目內推。</strong></p>

<h4 id="給申請者的建議---履歷撰寫">給申請者的建議 - 履歷撰寫</h4>

<p>請提供詳實的自我經驗總結介紹及必要資訊，幫忙的朋友看了你提供的資料後將會依情況決定是否要花時間內推。</p>

<p>若履歷上經驗不足、或是你沒有提供備註註明所需的資訊，為了維持內推的品質而被 HR 列為黑名單，仍然有機會拒絕你的請求。</p>

<p>若你對於履歷格式不確定如何開始，可以參考以下撰寫範例：</p>

<ul>
  <li><a href="https://careercup.com/resume">This Is What A GOOD Resume Should Look Like</a></li>
</ul>

<p>請注意上述履歷通常比較適用美國企業 (例如：不需要特別放 profile photo、個人簡介)，不過可能會隨著國家有所區別，網路上有許多資源，請依照自行狀況斟酌。</p>

<h4 id="給申請者的建議---內推就一定會有面試機會嗎">給申請者的建議 - 內推就一定會有面試機會嗎？</h4>

<p>通常內推是無法保證一定有職缺面試、錄取的機會 (我自己協助內推的經驗仍然是有被錄取的候選人寥寥無幾)。</p>

<p>但各公司的內推機制通常提供了比直接從網路上海投更容易被招聘團隊看見的機會，甚至可能縮短你在前期等待回應的時間。</p>

<p>內推通常為推薦人主動提供這樣的機會，並且很常會需要花費額外的時間。甚至必須要花費一陣努力 (私下了解你的背景、跟 HR 追問進度)。</p>

<div class="notice--warning">
<p><strong>在付出這麼多額外的時間後，仍在招聘公司許多考量下，沒有被錄取也十分常見，因此，請記得保持禮貌及感謝每一位幫助的海外台灣人。</strong></p>
</div>

<h3 id="b-註冊成為推薦人-我是某公司員工我願意為串連海外工作機會盡一份心力">B. 註冊成為推薦人 (我是某公司員工，我願意為串連海外工作機會盡一份心力)</h3>

<p><a href="https://work.nexf.org/signup/referrer"></a></p>

<p>你可以透過右上角的註冊按鈕，並且點擊「註冊成為推薦人」(<a href="https://work.nexf.org/signup/referrer">或是點擊這裡註冊成為推薦人</a>) 填寫基本資料註冊帳戶：</p>

<figure class="">
  <img src="/assets/images/posts/2022/03/nex-work/nex-work-referral-1.png" alt="註冊成為推薦人" />
  
    <figcaption>
      註冊成為推薦人

    </figcaption>
  
</figure>

<p>並且可以於「公司名稱」一欄選擇或是新增你目前能夠協助推薦的公司完成註冊：</p>

<figure class="">
  <img src="/assets/images/posts/2022/03/nex-work/nex-work-referral-2.png" alt="選擇或是新增協助推薦的公司" />
  
    <figcaption>
      選擇或是新增協助推薦的公司

    </figcaption>
  
</figure>

<p>你可以進一步編輯相關的個人資料以利尋求內推機會的人更加了解你。如此一來就完成必要資料的填寫，尋求推薦的候選人便可以到首頁檢視並且透過您所提供的管道提交必要材料。</p>

<p>一旦有新的推薦請求，你可以在「檢視內推申請」查看待處理的推薦請求：</p>

<figure class="">
  <img src="/assets/images/posts/2022/03/nex-work/nex-work-referral-3.png" alt="檢視待處理的內推申請" />
  
    <figcaption>
      檢視待處理的內推申請

    </figcaption>
  
</figure>

<h2 id="常見問題">常見問題</h2>

<p>註：若你是推薦人，目前 NEX WORK 工程團隊已經收到「內推紀錄」和「內推申請」的相關使用者反饋，一個是尋求內推機會的紀錄、一個是協助內推的紀錄，請記得不要搞混哦</p>

<figure class="">
  <img src="/assets/images/posts/2022/03/nex-work/nex-work-usage-5.png" alt="檢視內推紀錄" />
  
    <figcaption>
      檢視內推紀錄

    </figcaption>
  
</figure>

<h2 id="總結">總結</h2>

<p>希望這篇內容能夠對你有所幫助並且更加了解 NEX WORK 平台。若有任何關於 NEX WORK 的任何建議，也歡迎透過右側的 Feedback 表單或是以下聯繫方式，讓我們一起把 NEX WORK 變得更好：</p>

<ul>
  <li><a href="https://www.facebook.com/NEXFoundation">Facebook 專頁私訊 - NEX Foundation 台灣未來基金會</a></li>
  <li>Email: <a href="mailto:contact@nexf.org">contact@nexf.org</a></li>
</ul>

<p>此外，在 NEX Foundation 我們相信「今日的路人是明日的引路人」，延續 Give and Take 的精神，如果你願意一同攜手支持或是加入全球志工團隊的一份子讓我們啟動正向迴圈<sup id="fnref:nex-hiring-ptt" role="doc-noteref"><a href="#fn:nex-hiring-ptt" class="footnote" rel="footnote">2</a></sup>，幫助更多台灣人走向世界，讓回家的路變得更好。你可以透過以下連結了解更多資訊：</p>

<ul>
  <li><a href="https://www.nexf.org/get-involved/">NEX Foundation - Get involved</a></li>
  <li><a href="https://www.nexf.org/donation/">NEX Foundation - SUPPORT TO MAKE THE CHANGE</a></li>
</ul>

<p>我同時也在 NEX Foundation 為串連台灣人才於國際舞台上的職涯發展貢獻己力，你可以透過 NEX WORK 附上 CV 提交內推申請 (需註冊登入) 以引薦更多像你這樣的優秀人才，或是透過我的 LinkedIn 與我聯繫。</p>

<ul>
  <li><a href="https://work.nexf.org/companies/Amazon/referrers/tdG7zR9knf8XnpHzrpTw">NEX WORK：申請內推 (需登入)</a></li>
</ul>

<p>如果你覺得這樣的內容有幫助，可以在底下按個 Like / 留言讓我知道。</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:oversea-ptt" role="doc-endnote">
      <p>ptt, <a href="https://www.ptt.cc/bbs/Oversea_Job/M.1535586886.A.182.html">Amazon各職位內推 讓台灣人把亞麻填滿滿</a> <a href="#fnref:oversea-ptt" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:nex-hiring-ptt" role="doc-endnote">
      <p>ptt, <a href="https://www.ptt.cc/bbs/Oversea_Job/M.1544246238.A.790.html">NEX Foundation台灣未來基金會號召全球熱血鄉民</a> <a href="#fnref:nex-hiring-ptt" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Yang-Xin Cao (Eason Cao)</name></author><category term="nex work" /><category term="nex foundation" /><category term="work" /><category term="volunteer" /><category term="taiwanzonian" /><summary type="html"><![CDATA[另類海外求職的管道，獲得國際大公司的內推機會，串連海外菁英的 NEX WORK - 如果流浪是為了找回家的路，我們有責任把回家的路變得更美好]]></summary></entry><entry><title type="html">AWS Firecracker 論文導讀：一個小孩才做選擇，我兩個全部都要的 VMM</title><link href="https://easoncao.com/firecracker-paper-reading/" rel="alternate" type="text/html" title="AWS Firecracker 論文導讀：一個小孩才做選擇，我兩個全部都要的 VMM" /><published>2022-02-20T00:00:00-06:00</published><updated>2022-02-20T00:00:00-06:00</updated><id>https://easoncao.com/firecracker-paper-reading</id><content type="html" xml:base="https://easoncao.com/firecracker-paper-reading/"><![CDATA[<p>AWS 公開 Firecracker 專案已經至少 1-2 年的時間，也許你多少都聽過這項技術。然而，究竟 Firecracker 是什麼，我想可能你也仍然一知半解。有鑒於我認為學術類型的內容有時不容易讓人明白，因此，我希望可以透過以下的篇幅，分享我自己對於閱讀 Firecracker 設計論文的一些理解。</p>

<p>由於我個人沒有受過正式的學術訓練，因此，如果有專家願意提供任何見解，也請不吝給予指正及建議。</p>

<figure class="">
  <img src="/assets/images/posts/2022/02/firecracker-paper-reading/cover.png" alt="Firecracker 概覽" />
  
    <figcaption>
      Firecracker 概覽 (<a href="https://firecracker-microvm.github.io/">source</a>)

    </figcaption>
  
</figure>

<h2 id="概覽">概覽</h2>

<p>在閱讀完 Firecracker 的相關論文後，我對於這個專案的結論，<strong>簡直可以說是一個小孩才做選擇，我兩個全部都要的設計</strong>。</p>

<p>在論文中提到，設計 AWS Firecracker 由於在權衡 Hypervisor-based 和 Linux container 虛擬化技術之間產生的相容性、安全性的優缺點，兩者取其一似乎都無法滿足 AWS 基礎建設所需滿足的工程目標。因此，Firecracker 決定打破這樣的抉擇，擔任起 VMM (Virtual Machine Monitor) 的角色，並且引入相關的各種現有功能機制的優點，以滿足運算虛擬化的設計需要。</p>

<blockquote>
  <p>Implementors of serverless and container services can choose between hypervisor-based virtualization (and the potentially unacceptable overhead related to it), and Linux containers (and the related compatibility vs. security tradeoffs). <strong>We built Firecracker because we didn’t want to choose.</strong></p>
</blockquote>

<p>目前 AWS 已經將 Firecracker 導入至兩個公開的無伺服器 (Serverless) 服務：<a href="https://aws.amazon.com/lambda/">AWS Lambda</a> 及 <a href="https://aws.amazon.com/fargate/">AWS Fargate</a>，並且支援數百萬的用戶和單月萬億級別 (trillions) 的請求，以下將具體描述更多 Firecracker 相關的細節。</p>

<p>(相關的 Paper 原文和我自己畫的重點請參考<sup id="fnref:firecracker-paper-note" role="doc-noteref"><a href="#fn:firecracker-paper-note" class="footnote" rel="footnote">1</a></sup>)</p>

<h2 id="基本名詞釋義-terminology">基本名詞釋義 (Terminology)</h2>

<p>由於 Firecracker 屬於一種作業系統虛擬化技術的延伸，其中涉及 Linux 作業系統虛擬化的諸多細節。因此，首先在閱讀這篇內容之前，必須先了解基本的一些概念和名詞釋義：</p>

<h3 id="hypervisor">Hypervisor</h3>

<p>Hypervisor 可以視為用於管理虛擬機器 (Virtual Machine) 的軟體、系統或是韌體。使用虛擬化技術允許我們在單一個電腦上運行多個不同的系統、甚至是可能不同的作業系統 Kernel，並且將其放置於一個虛擬的運行環境中 (Virtual Machine)。因此，Hypervisor 的目的就是用來管理這些虛擬機器，通常，用來執行一個或多個虛擬機器的電腦稱為宿主機 (Host)，這些虛擬機器則稱為客戶機 (Guest)。</p>

<p>Gerald J. Popek 及 Robert P. Goldberg 在 1974 提出了兩種類型的 Hypervisor <sup id="fnref:virtualization-architectures" role="doc-noteref"><a href="#fn:virtualization-architectures" class="footnote" rel="footnote">2</a></sup>，分別為 Type 1 和 Type 2：</p>

<figure class="">
  <img src="/assets/images/posts/2022/02/firecracker-paper-reading/hypervisor-types.png" alt="Hypervisor 類型" />
  
    <figcaption>
      Hypervisor 類型 (<a href="https://en.wikipedia.org/wiki/Hypervisor">source</a>)

    </figcaption>
  
</figure>

<p>因為在虛擬機器中，安裝了一個 Guest OS 並不意味著就能直接使用 Host OS 的所有資源 (例如：磁碟寫入、CPU 時間、I/O 等操作)。通常，Hypervisor 會實作「模擬」這些裝置讓 Guest OS 以為能夠使用，但實際上仍交由虛擬化技術實際將這些操作轉譯、排程交給 Host OS 處理。</p>

<ul>
  <li><strong>Type 1</strong> 的 Hypervisor 通常需要硬體和 Kernel 支援，因為通常能充分交付使用硬體操作，而無需透過 Hypervisor 為 Guest OS 執行作業系統各種操作的 (syscall) 轉譯。這也通常意味著，執行的效能也比較高。常見的實作如：Xen 和 Linux KVM。</li>
  <li><strong>Type 2</strong> 軟體會運行於主要的 (Host OS) 並且可能會以一般的軟體應用形式運行，通常執行效率比 Type 1 來得低。常見作業系統層級的軟體類似於 VMWare、Vritual Box Hypervisor 軟體。</li>
</ul>

<h3 id="vmm-virtual-machine-monitor">VMM (Virtual Machine Monitor)</h3>

<p>基本上與 Hypervisor 相同，如同他的名字一樣 (Virtual Machine Monitor)，VMM 設計的目的就是用於建立、監控、管理和捕捉跑在虛擬機器中的 I/O 操作 (磁碟寫入、網路吞吐等)。</p>

<h3 id="qemu">QEMU</h3>

<p><a href="https://www.qemu.org/">QEMU</a> 是一個開源的 VMM，由於 QEMU 的架構由純軟體實現，並且處於 Guest machine 與 Host machine 擔任中間者角色，以處理 Guest machine 的相關硬體請求，並由其轉譯給真正的硬體，使得其存在一些效能問題。</p>

<h3 id="kvm">KVM</h3>

<p>KVM (Kernel-based Virtual Machine) 是一種 Linux Kernel 支持的虛擬化技術，可以將 Linux Kernel 轉換成一個可用的 VMM 並將系統轉換為 Type 1 (bare-metal) 類型的 Hypervisor，使得你可以在 Linux 系統上運行多個隔離的虛擬環境 (VM)。KVM 一直是 Linux Kernel 設計的一部分，並且存在於主流的 Linux Kernel 版本中。因此，由於屬於 Linux Kernel 支持功能的一部分，通常可以使用接近原生系統的相應執行效能處理對應的 I/O 操作。</p>

<h3 id="crosvm">crosvm</h3>

<p><a href="https://github.com/google/crosvm">crosvm</a> 為 Google 的一項開源專案 (Chrome OS Virtual Machine Monitor)，用於 Chrome OS 執行虛擬化機制的操作，基於 Linux KVM Hypervisor 實現虛擬化技術，並且用於 Android、Chrome OS 為基礎的系統中。與 QEMU 相比，它並不直接模擬實際的硬體裝置，反之，它採用了 Linux 支持的半虛擬化的裝置標準 (<a href="https://developer.ibm.com/articles/l-virtio/">virtio</a>) 來模擬虛擬機中相關的裝置。Firecracker paper 中具體提到了實作中採用了以 crosvm 作為基礎核心背景修改。</p>

<h3 id="cgroup-control-group">Cgroup (Control Group)</h3>

<p>cgroup 是 Linux Kernel 一項支持的功能，主要可以用來限制運行在容器執行環境中的資源使用 (例如：CPU、Memory 和磁碟讀取寫入等)。cgroup 同時也被大量運用在 Linux container 的技術中，例如：Kubernetes、Docker 等。</p>

<p>在 Firecracker 中，提及了基於不信任 Guest OS 對於資源控制的行為。這是由於 Guest OS 屬於客戶控制的一部分，並無法預期其是否能依照合理的使用行為運行，因此，Firecracker 也採用了 Linux 本身支持的功能及 cgroup 等機制，限制了 VMM 和各個虛擬機器總體可用的資源。</p>

<h3 id="seccomp">Seccomp</h3>

<p>seccomp 是 Linux Kernel 支持的一項功能，用來限制在容器中運行的 process 可以呼叫的系統方法 (syscall)。可以想像就像是允許使用特定 Linux function 的白名單，在 process 的直接階段僅允許特定系統呼叫操作。</p>

<p>同樣的機制也被實踐在一些容器虛擬化技術中，例如 Docker 定義的預設 <a href="https://github.com/moby/moby/blob/b0806bdb03f0843267b34f50142d5f52ddd68757/profiles/seccomp/default_linux.go#L51-L390">seccomp profile</a>。</p>

<h2 id="aws-firecracker-是什麼">AWS Firecracker 是什麼</h2>

<p>在過去，AWS 主流的 Serverless 服務提供了 AWS 客戶另一項託管運行應用的選擇 - 用戶不需要再自行管理底層運作機器和安全性修補的工作。</p>

<p>最著代表性的 AWS 服務便是 AWS Lambda，如果你不知道 AWS Lambda 是什麼，<a href="https://aws.amazon.com/lambda/">AWS Lambda</a> 是一種無伺服器 (Severless) 運算服務，使用者可以直接上傳你的程式碼並且選擇對應的規格運行，你無須在煩惱需要使用什麼樣的硬體規格以及為維護工作煩惱。</p>

<p>同時，也提供在大規模的應用情境中隨著用量可以動態擴展的優勢。然而，在 AWS Lambda 服務剛釋出時，其採用了 Linux Container 用以隔離不同客戶的執行環境 (類似於 Docker 相應的技術)，然而，這樣的機制除了可能在客戶使用運行執行環境存在限制 (需使用依賴 Host OS 支持的 Kernel 版本指令集)，也因為同時共用相同的 Kernel，也可能存在部分安全性風險。</p>

<blockquote>
  <p>When we first built AWS Lambda, we chose to use Linux containers to isolate functions, and virtualization to isolate between customer accounts.</p>
</blockquote>

<figure class="">
  <img src="/assets/images/posts/2022/02/firecracker-paper-reading/model-compare.png" alt="虛擬化架構" />
  
    <figcaption>
      AWS Lambda 虛擬化架構 (左) 為之前的設計 (右) 為採用 KVM &amp; VMM 技術的設計

    </figcaption>
  
</figure>

<p>因此，在這篇 Paper 中，主要提到了 firecracker 評估使用虛擬化技術設計時存在六項重點考量：</p>

<ul>
  <li><strong>Isolation</strong>: 需要具備安全的隔離環境使用戶在運行應用時，能避免資料洩漏、非法提權等安全性問題</li>
  <li><strong>Overhead and Density</strong>: 為了盡可能使單一機器的硬體資源應用最大化，該技術需要能夠提供運行至少數千個 microVM 執行環境 (Lambda function)</li>
  <li><strong>Performance</strong>: 執行效能要能貼近使用原生實體機器 (Bare-metal) 的執行效能，換句話說能降低因為硬體指令轉譯的執行時間</li>
  <li><strong>Compatibility</strong>: 需要能夠讓用戶執行應用時運行 Linux 支持的函式庫和執行檔，以客戶無需進行程式碼修改或是重新編譯</li>
  <li><strong>Fast Switching</strong>: 能夠盡可能快速的啟動執行環境 (microVM) 和清除執行環境</li>
  <li><strong>Soft Allocation</strong>: 存在資源動態調整機制，能夠允許虛擬執行環境分配額外的 CPU、Memory 等資源。讓每個應用僅可使用消耗他所需要的資源，而不是有權使用的系統資源</li>
</ul>

<p>在這樣的條件下，論文 2.1 中的細節便是在具體討論和評估數項現有的虛擬化技術，包含：</p>

<ul>
  <li>Linux container: Linux Kernel 本身支持的容器化技術，使 Linux process 存在於獨立的執行環境 (namespaces)，並達到 process-level 的隔離，包含 user IDs (uids), process IDs (pids) 及 network interface，並且能夠利用 chroot 機制隔離執行的檔案系統。同時，利用 seccomp-bpf 更可以達到 process 執行系統呼叫的限制 (syscall)。在相關的研究中，一般啟動 Ubuntu Linux (15.04) 版本的安裝需要 224 syscalls 及 52 個獨立的 ioctl 操作。</li>
  <li>Language-Specific Isolation：例如 JVM 透過劃分 Heap size 和虛擬執行環境，於記憶體空間分配支持的虛擬執行環境</li>
  <li>硬體和 Kernel 支持的主流虛擬化技術：Intel VT-x、KVM、QEMU。在論文中提到常見的 KVM 和 QEMU 組合通常增加了執行虛擬化上的複雜度，由於 QEMU 專案本身包含了大於 140 萬 (1.4 million) 的程式碼，並且至少需要 270 個系統呼叫操作 (syscall)，若使用這項基礎再疊加使用 KVM，則會再另外增加了 120,000 行程式碼。</li>
</ul>

<p>因此，在評估和主流虛擬化技術比較這樣的背景下，AWS Firecracker 借鑒了許多解決方案而在眾多項目中選擇一個適當的平和。同時，基於 AWS 內部許多團隊，維運基礎架構都採用 Linux 系統，促使 Firecracker 在設計的哲學上的這項決定。更重要的是，Firecracker 更之所以遵循沿用 Linux Kernel 本身就支持的技術，而不是重新實作替代它，正是因為這些功能行之有年，並且具備高質量、成熟的設計 (例如：scheduler、TUN/TAP network interface)，也能讓 AWS 原本的團隊使用熟悉的 Linux 工具和維運流程執行除錯。例如：採用 <code class="language-plaintext highlighter-rouge">ps</code> 即可列舉機器上運行的 microVM，其他 Linux 本身支持的工具 (<code class="language-plaintext highlighter-rouge">top</code>、<code class="language-plaintext highlighter-rouge">vmstat</code> 甚至是 <code class="language-plaintext highlighter-rouge">kill</code>) 均可以在預期的操作下管理 Firecracker。</p>

<p>基於這項原因，Firecracker 使用了 KVM 作為主要的虛擬化執行基礎，並且實作 VMM (Virtual Machine Monitor) 元件以滿足管理 KVM 執行環境的需要。</p>

<blockquote>
  <p>Our other philosophy in implementing Firecracker was to rely on components built into Linux rather than re-implementing our own, where the Linux components offer the right features, performance, and design</p>
</blockquote>

<h3 id="kvm-在這樣的基礎下做了哪些事">KVM 在這樣的基礎下做了哪些事？</h3>

<p>執行硬體層級的虛擬化 (HVM) 及資源分配，例如：CPU、處理記憶體管理、分頁 (Paging) 等。</p>

<h2 id="aws-firecracker-實作">AWS Firecracker 實作</h2>

<p>在 Firecracker 的實作中，以 Google crosvm 作為基礎，移除了大量不必要的裝置，例如：USB、GPU 以及 9p 檔案系統協議 (Plan 9 Filesystem Protocol)。在這樣的基礎下，Firecracker 以 Rust 語言為主增加了額外約 2 萬行的程式碼；同時修改了約 3 萬行的程式碼並且開源公開。</p>

<figure class="">
  <img src="/assets/images/posts/2022/02/firecracker-paper-reading/firecracker-layers.png" alt="Firecracker 架構" />
  
    <figcaption>
      Firecracker 架構

    </figcaption>
  
</figure>

<p>Firecracker 同時模擬了有限的一些 I/O 裝置，例如：網路卡、磁碟、序列端口 (serial ports)、i8042 支持 (PS/2 鍵盤的控制器)；與 QEMU 相比，QEMU 相對複雜許多，其支持多餘 40 種不同的裝置，包含 USB、影像和音訊裝置。</p>

<p>更細部的設計架構如下：</p>

<figure class="">
  <img src="/assets/images/posts/2022/02/firecracker-paper-reading/firecracker-design.png" alt="Firecracker 細部設計架構" />
  
    <figcaption>
      Firecracker 細部設計架構 (<a href="https://github.com/firecracker-microvm/firecracker/blob/main/docs/design.md">source</a>)

    </figcaption>
  
</figure>

<p>在 Firecracker 中採用了 virtio 作為網路和磁碟裝置的模擬，其中大約佔 Firecracker 1, 400 行 Rust 程式碼。同時 Firecracker 也提供了 REST API 設計，使其能夠使用 HTTP 的用戶端直接與其互動 (例如：<code class="language-plaintext highlighter-rouge">curl</code>)。</p>

<p>總結來說，Firecracker 旨在提供以下機制 <sup id="fnref:how-firecracker-work" role="doc-noteref"><a href="#fn:how-firecracker-work" class="footnote" rel="footnote">3</a></sup> <sup id="fnref:firecracker-open-source-innovation" role="doc-noteref"><a href="#fn:firecracker-open-source-innovation" class="footnote" rel="footnote">4</a></sup>:</p>

<ul>
  <li>設定 KVM</li>
  <li>提供裝置模擬，包含模擬 SSD、NIC (網卡) 等，即使沒有那麼多裝置在實際的硬體 (AWS Lambda Host) 上，使用 virtio 使得其可以虛擬化這些裝置</li>
  <li>執行環境效能的隔離 (採用 cgroup)</li>
  <li>為 Serverless 提供優化的效能 (5-8 Mb 的 Linux 啟動時間可以縮短為 100ms 以內、更微小的 Kernel 更可以縮短為 5ms)</li>
</ul>

<h3 id="資源限制-rate-limiter">資源限制 (Rate Limiter)</h3>

<p>在 Firecracker 中的硬體裝置涵括了限制配額的機制，包含可以限制 Disk IOPS (I/O Per Second)、PPS (Packets Per Second for network)。在 Firecracker 提供了使用 API 設定 microVM 可用的資源請求，包含 CPU、磁碟 I/O、網路吞吐等。</p>

<p>其資源限制機制採用 <code class="language-plaintext highlighter-rouge">virtio</code> 本身支持的資源限制功能，以網路裝置來說，可以是以下的配置機制 (<a href="https://github.com/firecracker-microvm/firecracker/blob/f35f324b84ce5c78dcf706cd97117bca41485b34/src/vmm/src/vmm_config/net.rs#L30"><code class="language-plaintext highlighter-rouge">rx_rate_limiter</code></a>)：</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>PATCH /network-interfaces/iface_1 HTTP/1.1
Host: localhost
Content-Type: application/json
Accept: application/json

<span class="o">{</span>
    <span class="s2">"iface_id"</span>: <span class="s2">"iface_1"</span>,
    <span class="s2">"rx_rate_limiter"</span>: <span class="o">{</span>
        <span class="s2">"bandwidth"</span>: <span class="o">{</span>
            <span class="s2">"size"</span>: 1048576,
            <span class="s2">"refill_time"</span>: 1000
        <span class="o">}</span>,
        <span class="s2">"ops"</span>: <span class="o">{</span>
            <span class="s2">"size"</span>: 2000,
            <span class="s2">"refill_time"</span>: 1000
        <span class="o">}</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div>

<h3 id="安全性-security">安全性 (Security)</h3>

<p>為了實踐安全性的最佳化，Firecracker 在部署階段需要充分避免一些因為 Linux kernel 或虛擬化技術可能帶來潛在的安全性問題，例如：Intel Meltdown、Spectre、Zombieload 等安全性漏洞。因此，在生產環境中，為了解決這項顧慮，Firecracker 實踐了幾項部署重點：</p>

<ul>
  <li>關閉 Symmetric MultiThreading (SMT, aka HyperThreading)</li>
  <li>Kernel Page-Table Isolation, Indirect Branch Prediction Barriers, Indirect Branch Restricted Speculation and cache flush mitigations against L1 Terminal Fault</li>
  <li>啟動部分 Kernel 參數包含 Speculative Store Bypass mitigations</li>
  <li>關閉 swap 和 samepage merging</li>
  <li>避免共享檔案 (解決 timing attacks like Flush+Reload and Prime+Probe)</li>
  <li>以及使用建議的硬體設備以解決 RowHammer 攻擊技術</li>
</ul>

<p>相關的 Firecracker 生產環境部署建議同時列舉於以下文件中：</p>

<ul>
  <li><a href="https://github.com/firecracker-microvm/firecracker/blob/cd29c64b35f694a5a442a7778b7c1463551bc1e2/docs/prod-host-setup.md">Production Host Setup Recommendations</a></li>
</ul>

<p>同時為了避免 Firecracker VMM 執行操作的過程出現任何非預期行為 (例如：安全性漏洞允許植入惡意代碼)，在 Firecracker 中實現了使用另一層沙箱 (Sandbox) 提供額外隔離的保護。在 Firecracker 的設計稱之為 <strong>Jailer</strong>。</p>

<p>雖然是這樣說，但在 Paper 中提到的具體實作，仍為使用 Linux container 提供的技術執行，包含：</p>

<ul>
  <li>以 chroot 機制隔離執行的檔案系統</li>
  <li>以 namepsace 隔離執行環境、pid、network namespaces</li>
  <li>移除 System privilege (基於 Linux Capabilities 功能 <sup id="fnref:linux-capabilities" role="doc-noteref"><a href="#fn:linux-capabilities" class="footnote" rel="footnote">5</a></sup>)</li>
  <li>以 seccomp-bpf profile 設定允許呼叫的 syscall</li>
</ul>

<figure class="">
  <img src="/assets/images/posts/2022/02/firecracker-paper-reading/firecracker-jailer.png" alt="Firecracker Jailer" />
  
    <figcaption>
      Firecracker Jailer

    </figcaption>
  
</figure>

<p>在 jailer sandbox 配置的 chroot 目錄中，裡面僅包含 Firecracker 編譯的執行檔、<code class="language-plaintext highlighter-rouge">/dev/net/tun</code>、cgroup 控制檔案和 microVM 所需的資源。並且，預設情況下 seccomp-bpf profile 設定了 24 個系統呼叫操作 (syscalls) 和 30 個 ioctls 操作為白名單。</p>

<p>不過就我的研究，如果我理解正確，似乎 Firecracker 在 seccomp filter 上面在最近的版本多了不少 syscalls 支援:</p>

<ul>
  <li><a href="https://github.com/firecracker-microvm/firecracker/blob/main/resources/seccomp/x86_64-unknown-linux-musl.json">Firecracker default seccomp filters (x86_64)</a></li>
  <li><a href="https://github.com/firecracker-microvm/firecracker/blob/77fdc6ef390b5e04ae97a4c1b4bf0da541afc7bc/tests/integration_tests/security/test_seccomp.py">Firecracker seccomp integration test case</a></li>
</ul>

<h2 id="firecracker-與-aws-lambda-架構之間的關係-high-level-architecture">Firecracker 與 AWS Lambda 架構之間的關係 (High-level architecture)</h2>

<p>在 Firecracker 設計出來後，AWS 便逐漸於 AWS Lambda 的底層架構中導入使用。使用 AWS Firecracker 允許 Lambda 的架構在每個執行的節點 (Lambda worker) 運行數千個 microVM。</p>

<h3 id="lambda-high-level-to-low-level-architecture">Lambda High-level to low-level architecture</h3>

<p>AWS Lambda 從上層到下層的架構可以由遠至近如下：</p>

<p>(1) 用戶透過事件經由 Frondend service 觸發 Lambda function (可以是 API Gateway, 其他來源等)，會由 Worker Manager 定義配置部署可用的執行機器 (Lambda Worker)</p>

<figure class="">
  <img src="/assets/images/posts/2022/02/firecracker-paper-reading/lambda-invoke-flow.png" alt="AWS Lambda 觸發的架構流程" />
  
    <figcaption>
      AWS Lambda 觸發的架構流程

    </figcaption>
  
</figure>

<p>(2) 一旦觸發後，Frondend service 交付由 Worker Manager 會遵循調度演算法 (sticky routing) 盡可能將觸發對象黏著在特定的 Lambda Worker 機器上，並且建議觸發的對象 (invoke service) 直接將請求的內容 (payload) 直接轉送到目標的 Lambda worker 機器上，減少觸發上的延遲和來回交互請求 (round-trip)。</p>

<figure class="">
  <img src="/assets/images/posts/2022/02/firecracker-paper-reading/lambda-event-path.png" alt="AWS Lambda 事件觸發的流程" />
  
    <figcaption>
      AWS Lambda 事件觸發的流程

    </figcaption>
  
</figure>

<p>(3) 在每個 Lambda worker 提供了 <em>slot</em> 這個抽象物件，該抽象物件即客戶預先載入的 Lambda function 應用程式碼 (Lambda function code)，並且在後面每次觸發的行為上盡可能的重複使用這個執行環境 (slot)</p>

<p>重點在於 Firecracker 於 Lambda Worker 中部署的機制，每個 Lambda Worker 可以視為一個 Bare-metal 的機器，上面運行著 Firecracker VMM 用於管理多個 MicroVM (Lambda function, slot)；每個 microVM 包含了客戶的執行環境 (sandbox) 和客戶的應用程式碼，以及一個 shim control process 用於採用 TCP/IP socket 和 Micro Manager 互動的元件。</p>

<p>(MicroManager 可以視為 Lambda data plane 和 control plane 互動的元件)</p>

<blockquote>
  <p>MicroManager provides slot management and locking APIs to placement, and an event invoke API to the Frontend</p>
</blockquote>

<figure class="">
  <img src="/assets/images/posts/2022/02/firecracker-paper-reading/lambda-worker-architecture.png" alt="Lambda Worker 中的細部架構" />
  
    <figcaption>
      Lambda Worker 中的細部架構

    </figcaption>
  
</figure>

<p>同時 MicroManager 部分也確保存在小量預先啟動的 MicroVMs，以確保有放置請求的即時需要。這是因為即使 Firecracker 能縮短在 <strong>125ms</strong> 內啟動，這樣的啟動時間可能仍不足以滿足 AWS Lambda 客戶快速啟動擴展的需要，並且可能會部分阻塞用戶的執行請求，因此在實務中，存在類似這樣 pre-warm 的機制。</p>

<h3 id="firecracker-io-path">Firecracker I/O Path</h3>

<p>當 AWS Lambda 中的應用執行寫入操作時 (假設 Guest OS 中的應用希望寫入檔案到磁碟)，此時會交付由 <code class="language-plaintext highlighter-rouge">virtio</code> driver 處理該操作，並且由 <code class="language-plaintext highlighter-rouge">virtio</code> driver 將其放置到共享記憶體 (shared memory) 中，並且於系統 Ring Buffer 進行緩衝。然後，Firecracker 將被喚醒執行 I/O 操作，並且將該寫入操作真實的寫進實體磁碟當中。<sup id="fnref:deep-dive-into-aws-lambda-security" role="doc-noteref"><a href="#fn:deep-dive-into-aws-lambda-security" class="footnote" rel="footnote">6</a></sup></p>

<h3 id="實際遷移到-aws-firecracker-的執行">實際遷移到 AWS Firecracker 的執行</h3>

<p>論文中提到 AWS 從 2018 年開始將 AWS Lambda 的客戶從 EC2 運行容器 (per function per container) 的基礎平台轉移到 Firecracker。在遷移過程中，並無可用性中斷、延遲或其他指標層面問題。</p>

<p>不過，在 AWS 內部團隊在遷移過程中，一些小問題也因為這樣的遷移暴露出來，例如前面提到為了安全性考量關閉了 Symmetric MultiThreading (SMT) 機制 (過去的部署中是開啟的)，使得使用 Apache HttpClient 應用執行的行為因為一些執行緒 (Thread) 相關的 bug 也因此暴露，並且存在於過去的 AWS SDK 版本中，需透過修補依賴函式庫解決這項問題。</p>

<p>但在 AWS 內部團隊完成遷移後，便開始實際將外部客戶的相關基礎建設逐步遷移至 AWS Firecracker 為基礎的設施，並且獲得巨大的成功。</p>

<p>同時，有鑒於涉及未來安全性補丁的修復和系統更新，由於傳統使用 <code class="language-plaintext highlighter-rouge">rpm</code>、<code class="language-plaintext highlighter-rouge">yum</code> 等套件管理工具進行管理的變因太多，可能導致軟體一致性問題產生，AWS 團隊採用了 immutable infrastructure 的策略來完成這項工作，即透過使用新版本的 AMI (Amazon Machine Image, 用於 EC2 的啟動鏡像) 直接啟動新的 EC2 instances，並且替換舊的 EC2 instances 來完成這項工作。</p>

<h2 id="evaluation-性能評估">Evaluation (性能評估)</h2>

<p>在該篇論文中，Firecracker 提供了數項不同測試數據的表現，同時，在 NSDI 2020 會議上也公開了對應的測試數據<sup id="fnref:nsdi2020-data" role="doc-noteref"><a href="#fn:nsdi2020-data" class="footnote" rel="footnote">7</a></sup>。</p>

<p>下列的測試採用 EC2 <code class="language-plaintext highlighter-rouge">m5d.metal</code> instance type，其擁有 <code class="language-plaintext highlighter-rouge">Intel Xeon Platinum 8175M</code> 處理器 (48 cores, hyper-threading disabled)、<strong>384GB RAM</strong> 和 4 個 <strong>840GB 的 NVMe 磁碟</strong>。</p>

<p>在這項測試中 Host OS 使用 <code class="language-plaintext highlighter-rouge">Ubuntu 18.04.2</code> 以及 Linux kernel <code class="language-plaintext highlighter-rouge">4.15.0-1044-aws</code> 版本。</p>

<p>這項測試與幾個主要的虛擬化技術執行比較，包含：<code class="language-plaintext highlighter-rouge">Firecracker v0.20.0</code>、<code class="language-plaintext highlighter-rouge">Pre-configured Firecracker</code>、<code class="language-plaintext highlighter-rouge">Intel Cloud Hypervisor</code>、<code class="language-plaintext highlighter-rouge">QEMU v4.2.0</code></p>

<figure class="">
  <img src="/assets/images/posts/2022/02/firecracker-paper-reading/firecracker-evaluation-boot-time.png" alt="Firecracker 啟動時間表現" />
  
    <figcaption>
      Firecracker 啟動時間表現

    </figcaption>
  
</figure>

<p>在啟動時間的定義中，啟動時間為 VMM process 執行建立 process 操作 (fork) 並且 Guest Kernel 發起第一個 <code class="language-plaintext highlighter-rouge">init</code> process 的時間。</p>

<p>從數據顯示預先配置好 IO Port 的 Firecracker 和 Intel Cloud Hypervisor，<strong>兩者環境啟動時間皆優於 QEMU</strong>。然而，要注意的是，上述的測試結果中不包含設置網路裝置，一旦加入網路裝置的設置，Firecracker 和 Cloud Hypervisor 皆會在啟動時間中增加約 <strong>20ms</strong>，然而，QEMU 則是 <strong>35ms</strong>。</p>

<figure class="">
  <img src="/assets/images/posts/2022/02/firecracker-paper-reading/firecracker-memory-overhead.png" alt="Firecracker 記憶體消耗使用表現" />
  
    <figcaption>
      Firecracker 記憶體消耗使用表現

    </figcaption>
  
</figure>

<p>在記憶體的消耗表現上 (Figure 7)，可以觀察到 QEMU 本身需要 128MB 的記憶體、Cloud Hypervisor 則約為 13 MB，然而，Firecracker 僅需約為 3MB 的記憶體消耗。</p>

<figure class="">
  <img src="/assets/images/posts/2022/02/firecracker-paper-reading/firecracker-io-performance.png" alt="Firecracker 磁碟 I/O 操作效能評估" />
  
    <figcaption>
      Firecracker 磁碟 I/O 操作效能評估 (使用 fio)

    </figcaption>
  
</figure>

<p>值得一提的是，在檔案 I/O 操作的表現上 (Figure 8 &amp; Figure 9)，該研究使用 <code class="language-plaintext highlighter-rouge">fio</code> 執行測試，<strong>明顯可以關注到在硬體資源能夠負荷超過 340,000 read IOPS (1GB/s at 4kB) 的情況下，Firecracker 以及 Cloud Hypervisor  僅可以被限縮使用約 13,000 IOPS (52MB/s at 4kB) 的吞吐效能</strong>。</p>

<figure class="">
  <img src="/assets/images/posts/2022/02/firecracker-paper-reading/firecracker-iperf.png" alt="Firecracker 網路吞吐效能評估" />
  
    <figcaption>
      Firecracker 網路吞吐效能評估 (使用 iperf3)

    </figcaption>
  
</figure>

<p>該研究同時也採用了 <code class="language-plaintext highlighter-rouge">iperf3</code> 執行網路效能的測試 (針對虛擬的 tap 網路介面, MTU 為 <strong>1500 byte</strong>)。<strong>在機器能夠達到單一網路流 44Gb/s 及 46Gb/s (10 個並行傳輸) 的狀況下，Firecracker 僅可達到約 15Gb/s 的吞吐</strong>。然而，QEMU 獲得接近於 Cloud Hypervisor 的測試結果，均能擁有較好的網路吞吐性能，這裡部分歸納結論是由於 <code class="language-plaintext highlighter-rouge">virtio</code> 的裝置設計實作而產生的限制。</p>

<h2 id="研究改進和結論">研究改進和結論</h2>

<p>如同前面性能評估所提及的，基於 <code class="language-plaintext highlighter-rouge">virtio</code> 的實作因素，這使得 Firecracker 並無法取得以直接存取 PCI 裝置以接近實體機器的 I/O 吞吐性能。使得網路和磁碟 I/O 的效能存在部分限制。</p>

<p>然而，就論該研究的總結，與前面提及的六項主要問題呼應，AWS Firecracker 的技術著實達到其工程上的設計目標，包含：</p>

<ul>
  <li><strong>Isolation</strong>: 以 Rust 為基礎的 VMM 允許多個用戶 (multi-tenant) 於單一機器上運行並達到執行環境隔離</li>
  <li><strong>Overhead and Density</strong>: Firecracker 允許單一硬體資源運行數千個 MicroVMs，以達到節省硬體資源的目的，同時帶來更低的 CPU 和 Memory 資源消耗</li>
  <li><strong>Performance</strong>: Block IO 以及網路吞吐效能著實存在改善空間，但著實已經滿足 AWS Lambda 及 AWS Fargate 兩者產品所需</li>
  <li><strong>Compatibility</strong>: Firecracker MicroVMs 運行為修改的 Linux kernel，允許客戶運行相關的應用程式碼，目前尚未發現不允許於 MicroVM 中運行的應用程式</li>
  <li><strong>Fast Switching</strong>: Firecracker MicroVMs 有較短的啟動時間 (150ms)，並且在多個 MicroVMs 並行啟動的狀況下保持一致的效能</li>
  <li><strong>Soft Allocation</strong>: Firecracker 測試允許超出 20 倍的資源配置比例，在 AWS 生環境中為允許超出實際 CPU 和 Memory 10 倍的配置比例並未存在問題</li>
</ul>

<h2 id="總結">總結</h2>

<p>Firecracker 除了於部分開源專案為虛擬化提供解決方案外 (例如：Kata container)，目前 AWS Firecracker 更是已經導入使用於 AWS 的基礎產品建設中，包含 <a href="https://aws.amazon.com/fargate/">AWS Fargate</a> 和 <a href="https://aws.amazon.com/lambda/">AWS Lambda</a>。</p>

<p>這樣的基礎設施改進同時也為客戶帶來更大的優勢，借助 Firecracker 的設計，使得 AWS Fargate 將運算定價折扣甚至達到 50% 的成本優化 (<a href="https://aws.amazon.com/blogs/compute/aws-fargate-price-reduction-up-to-50">AWS Fargate Price Reduction – Up to 50%</a>)。</p>

<p>基於對於這樣的技術感興趣，我著實閱讀了有關 Firecracker 整篇論文和參考部分 Firecracker 專案，歸納出上述的內容，並且花了一些時間整理這項導讀，更多訊息可以參考：</p>

<ul>
  <li><a href="https://github.com/0xlen/paper-reading/blob/master/firecracker-lightweight-virtualization-for-serverless-applications.pdf">Firecracker: Lightweight virtualization for serverless applications</a></li>
  <li><a href="https://firecracker-microvm.github.io/">Firecracker project page</a></li>
  <li><a href="https://github.com/firecracker-microvm/firecracker">Firecracker source code</a></li>
</ul>

<p>希望透過這樣的導讀，能夠有助於你更加了解 AWS Firecracker 這項技術。</p>

<p>如果你覺得這樣的內容有幫助，可以在底下按個 Like / 留言讓我知道。</p>

<h2 id="延伸資源">延伸資源</h2>

<ul>
  <li><a href="https://www.youtube.com/watch?v=QdzV04T_kec">AWS re:Invent 2018: REPEAT 1 A Serverless Journey: AWS Lambda Under the Hood (SRV409-R1)</a></li>
  <li><a href="https://www.youtube.com/watch?v=xmacMfbrG28">AWS re:Invent 2019: REPEAT 1 A serverless journey: AWS Lambda under the hood (SVS405-R1)</a></li>
  <li><a href="https://www.youtube.com/watch?v=PAEMGa-i2lU">Firecracker: A Secure and Fast microVM for Serverless Computing (Orelly)</a></li>
  <li><a href="https://www.youtube.com/watch?v=ADOfX2LiEns">Firecracker: Lightweight Virtualization - Opportunities and Challenges (2020)</a></li>
  <li><a href="https://www.youtube.com/watch?v=0wEiizErKZw">Deep Dive into firecracker-containerd (2019)</a></li>
  <li><a href="https://www.youtube.com/watch?v=9avPJL9Zqso">Extending containerd - Samuel Karp &amp; Maksym Pavlenko, Amazon (2019)</a></li>
  <li><a href="https://www.youtube.com/watch?v=2rwYZdVPN4g">Deep Dive into firecracker-containerd - Mitch Beaumont (AWS)</a></li>
</ul>

<h2 id="references">References</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:firecracker-paper-note" role="doc-endnote">
      <p>Alexandru Agache, Marc Brooker, Andreea Florescu, Alexandra Iordache, Anthony Liguori, Rolf Neugebauer, Phil Piwonka, Diana-Maria Popa. (2020). <a href="https://github.com/0xlen/paper-reading/blob/master/firecracker-lightweight-virtualization-for-serverless-applications.pdf">Firecracker: Lightweight virtualization for serverless applications</a> <a href="#fnref:firecracker-paper-note" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:virtualization-architectures" role="doc-endnote">
      <p>Gerald J. Popek, Robert P. Goldberg. (1974). <a href="https://dl.acm.org/doi/10.1145/361011.361073">Formal requirements for virtualizable third generation architectures</a> <a href="#fnref:virtualization-architectures" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:how-firecracker-work" role="doc-endnote">
      <p><a href="https://www.youtube.com/watch?v=BIRv2FnHJAg">How AWS’s Firecracker virtual machines work</a> <a href="#fnref:how-firecracker-work" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:firecracker-open-source-innovation" role="doc-endnote">
      <p><a href="https://www.youtube.com/watch?v=yDplzXEdBTI">AWS re:Invent 2019: Firecracker open-source innovation (OPN402)</a> <a href="#fnref:firecracker-open-source-innovation" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:linux-capabilities" role="doc-endnote">
      <p><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_atomic_host/7/html/container_security_guide/linux_capabilities_and_seccomp">Linux Capabilities and Seccomp</a> <a href="#fnref:linux-capabilities" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:deep-dive-into-aws-lambda-security" role="doc-endnote">
      <p><a href="https://www.youtube.com/watch?v=FTwsMYXWGB0">AWS re:Invent 2020: Deep dive into AWS Lambda security: Function isolation</a> <a href="#fnref:deep-dive-into-aws-lambda-security" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:nsdi2020-data" role="doc-endnote">
      <p><a href="https://github.com/firecracker-microvm/nsdi2020-data">Firecracker benchmarking code and data</a> <a href="#fnref:nsdi2020-data" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Yang-Xin Cao (Eason Cao)</name></author><category term="amazon" /><category term="aws" /><category term="amazon web services" /><category term="firecracker" /><category term="virtualization" /><category term="Lambda" /><category term="severless" /><category term="AWS Fargate" /><summary type="html"><![CDATA[AWS Firecracker 是一款由 AWS 開源的輕量級虛擬化運行環境。這項技術使得運行的執行環境具備傳統虛擬機的安全性，提供優化的執行效能和資源利用。在這篇內容中，我將與你分享在閱讀完 Firecracker 論文後具體的導讀細節。]]></summary></entry></feed>